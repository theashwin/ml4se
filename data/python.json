[{"code": "def _get_fill(arr: ABCSparseArray) -> np.ndarray:\n    \"\"\"\n    Create a 0-dim ndarray containing the fill value\n\n    Parameters\n    ----------\n    arr : SparseArray\n\n    Returns\n    -------\n    fill_value : ndarray\n        0-dim ndarray with just the fill value.\n\n    Notes\n    -----\n    coerce fill_value to arr dtype if possible\n    int64 SparseArray can have NaN as fill_value if there is no missing\n    \"\"\"\n    try:\n        return np.asarray(arr.fill_value, dtype=arr.dtype.subtype)\n    except ValueError:\n        return np.asarray(arr.fill_value)", "code_tokens": ["def", "_get_fill", "(", "arr", ":", "ABCSparseArray", ")", "->", "np", ".", "ndarray", ":", "try", ":", "return", "np", ".", "asarray", "(", "arr", ".", "fill_value", ",", "dtype", "=", "arr", ".", "dtype", ".", "subtype", ")", "except", "ValueError", ":", "return", "np", ".", "asarray", "(", "arr", ".", "fill_value", ")"], "original_string": "def _get_fill(arr: ABCSparseArray) -> np.ndarray:\n    \"\"\"\n    Create a 0-dim ndarray containing the fill value\n\n    Parameters\n    ----------\n    arr : SparseArray\n\n    Returns\n    -------\n    fill_value : ndarray\n        0-dim ndarray with just the fill value.\n\n    Notes\n    -----\n    coerce fill_value to arr dtype if possible\n    int64 SparseArray can have NaN as fill_value if there is no missing\n    \"\"\"\n    try:\n        return np.asarray(arr.fill_value, dtype=arr.dtype.subtype)\n    except ValueError:\n        return np.asarray(arr.fill_value)"}, {"code": "def new_vertex(self):\n    \"\"\"Creates and returns a new vertex.\n\n    Returns:\n      A new Vertex instance with a unique index.\n    \"\"\"\n    vertex = Vertex(len(self.vertices))\n    self.vertices.append(vertex)\n    return vertex", "code_tokens": ["def", "new_vertex", "(", "self", ")", ":", "vertex", "=", "Vertex", "(", "len", "(", "self", ".", "vertices", ")", ")", "self", ".", "vertices", ".", "append", "(", "vertex", ")", "return", "vertex"], "original_string": "def new_vertex(self):\n    \"\"\"Creates and returns a new vertex.\n\n    Returns:\n      A new Vertex instance with a unique index.\n    \"\"\"\n    vertex = Vertex(len(self.vertices))\n    self.vertices.append(vertex)\n    return vertex"}, {"code": "def set_dhcp_linklocal_all(interface):\n    '''\n    Configure specified adapter to use DHCP with linklocal fallback\n\n    Change adapter mode to TCP/IP. If previous adapter mode was EtherCAT, the target will need reboot.\n\n    :param str interface: interface label\n    :return: True if the settings were applied, otherwise an exception will be thrown.\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.set_dhcp_linklocal_all interface-label\n    '''\n    if __grains__['lsb_distrib_id'] == 'nilrt':\n        initial_mode = _get_adapter_mode_info(interface)\n        _save_config(interface, 'Mode', 'TCPIP')\n        _save_config(interface, 'dhcpenabled', '1')\n        _save_config(interface, 'linklocalenabled', '1')\n        if initial_mode == 'ethercat':\n            __salt__['system.set_reboot_required_witnessed']()\n        else:\n            _restart(interface)\n        return True\n    service = _interface_to_service(interface)\n    if not service:\n        raise salt.exceptions.CommandExecutionError('Invalid interface name: {0}'.format(interface))\n    service = pyconnman.ConnService(os.path.join(SERVICE_PATH, service))\n    ipv4 = service.get_property('IPv4.Configuration')\n    ipv4['Method'] = dbus.String('dhcp', variant_level=1)\n    ipv4['Address'] = dbus.String('', variant_level=1)\n    ipv4['Netmask'] = dbus.String('', variant_level=1)\n    ipv4['Gateway'] = dbus.String('', variant_level=1)\n    try:\n        service.set_property('IPv4.Configuration', ipv4)\n        service.set_property('Nameservers.Configuration', [''])  # reset nameservers list\n    except Exception as exc:\n        exc_msg = 'Couldn\\'t set dhcp linklocal for service: {0}\\nError: {1}\\n'.format(service, exc)\n        raise salt.exceptions.CommandExecutionError(exc_msg)\n    return True", "code_tokens": ["def", "set_dhcp_linklocal_all", "(", "interface", ")", ":", "if", "__grains__", "[", "'lsb_distrib_id'", "]", "==", "'nilrt'", ":", "initial_mode", "=", "_get_adapter_mode_info", "(", "interface", ")", "_save_config", "(", "interface", ",", "'Mode'", ",", "'TCPIP'", ")", "_save_config", "(", "interface", ",", "'dhcpenabled'", ",", "'1'", ")", "_save_config", "(", "interface", ",", "'linklocalenabled'", ",", "'1'", ")", "if", "initial_mode", "==", "'ethercat'", ":", "__salt__", "[", "'system.set_reboot_required_witnessed'", "]", "(", ")", "else", ":", "_restart", "(", "interface", ")", "return", "True", "service", "=", "_interface_to_service", "(", "interface", ")", "if", "not", "service", ":", "raise", "salt", ".", "exceptions", ".", "CommandExecutionError", "(", "'Invalid interface name: {0}'", ".", "format", "(", "interface", ")", ")", "service", "=", "pyconnman", ".", "ConnService", "(", "os", ".", "path", ".", "join", "(", "SERVICE_PATH", ",", "service", ")", ")", "ipv4", "=", "service", ".", "get_property", "(", "'IPv4.Configuration'", ")", "ipv4", "[", "'Method'", "]", "=", "dbus", ".", "String", "(", "'dhcp'", ",", "variant_level", "=", "1", ")", "ipv4", "[", "'Address'", "]", "=", "dbus", ".", "String", "(", "''", ",", "variant_level", "=", "1", ")", "ipv4", "[", "'Netmask'", "]", "=", "dbus", ".", "String", "(", "''", ",", "variant_level", "=", "1", ")", "ipv4", "[", "'Gateway'", "]", "=", "dbus", ".", "String", "(", "''", ",", "variant_level", "=", "1", ")", "try", ":", "service", ".", "set_property", "(", "'IPv4.Configuration'", ",", "ipv4", ")", "service", ".", "set_property", "(", "'Nameservers.Configuration'", ",", "[", "''", "]", ")", "# reset nameservers list", "except", "Exception", "as", "exc", ":", "exc_msg", "=", "'Couldn\\'t set dhcp linklocal for service: {0}\\nError: {1}\\n'", ".", "format", "(", "service", ",", "exc", ")", "raise", "salt", ".", "exceptions", ".", "CommandExecutionError", "(", "exc_msg", ")", "return", "True"], "original_string": "def set_dhcp_linklocal_all(interface):\n    '''\n    Configure specified adapter to use DHCP with linklocal fallback\n\n    Change adapter mode to TCP/IP. If previous adapter mode was EtherCAT, the target will need reboot.\n\n    :param str interface: interface label\n    :return: True if the settings were applied, otherwise an exception will be thrown.\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.set_dhcp_linklocal_all interface-label\n    '''\n    if __grains__['lsb_distrib_id'] == 'nilrt':\n        initial_mode = _get_adapter_mode_info(interface)\n        _save_config(interface, 'Mode', 'TCPIP')\n        _save_config(interface, 'dhcpenabled', '1')\n        _save_config(interface, 'linklocalenabled', '1')\n        if initial_mode == 'ethercat':\n            __salt__['system.set_reboot_required_witnessed']()\n        else:\n            _restart(interface)\n        return True\n    service = _interface_to_service(interface)\n    if not service:\n        raise salt.exceptions.CommandExecutionError('Invalid interface name: {0}'.format(interface))\n    service = pyconnman.ConnService(os.path.join(SERVICE_PATH, service))\n    ipv4 = service.get_property('IPv4.Configuration')\n    ipv4['Method'] = dbus.String('dhcp', variant_level=1)\n    ipv4['Address'] = dbus.String('', variant_level=1)\n    ipv4['Netmask'] = dbus.String('', variant_level=1)\n    ipv4['Gateway'] = dbus.String('', variant_level=1)\n    try:\n        service.set_property('IPv4.Configuration', ipv4)\n        service.set_property('Nameservers.Configuration', [''])  # reset nameservers list\n    except Exception as exc:\n        exc_msg = 'Couldn\\'t set dhcp linklocal for service: {0}\\nError: {1}\\n'.format(service, exc)\n        raise salt.exceptions.CommandExecutionError(exc_msg)\n    return True"}, {"code": "def _wait_for_completion(conn, promise, wait_timeout, msg):\n    '''\n    Poll request status until resource is provisioned.\n    '''\n    if not promise:\n        return\n    wait_timeout = time.time() + wait_timeout\n    while wait_timeout > time.time():\n        time.sleep(5)\n        operation_result = conn.get_request(\n            request_id=promise['requestId'],\n            status=True)\n\n        if operation_result['metadata']['status'] == \"DONE\":\n            return\n        elif operation_result['metadata']['status'] == \"FAILED\":\n            raise Exception(\n                \"Request: {0}, requestId: {1} failed to complete:\\n{2}\".format(\n                    msg, six.text_type(promise['requestId']),\n                    operation_result['metadata']['message']\n                )\n            )\n\n    raise Exception(\n        'Timed out waiting for asynchronous operation {0} \"{1}\" to complete.'.format(\n            msg, six.text_type(promise['requestId'])\n        )\n    )", "code_tokens": ["def", "_wait_for_completion", "(", "conn", ",", "promise", ",", "wait_timeout", ",", "msg", ")", ":", "if", "not", "promise", ":", "return", "wait_timeout", "=", "time", ".", "time", "(", ")", "+", "wait_timeout", "while", "wait_timeout", ">", "time", ".", "time", "(", ")", ":", "time", ".", "sleep", "(", "5", ")", "operation_result", "=", "conn", ".", "get_request", "(", "request_id", "=", "promise", "[", "'requestId'", "]", ",", "status", "=", "True", ")", "if", "operation_result", "[", "'metadata'", "]", "[", "'status'", "]", "==", "\"DONE\"", ":", "return", "elif", "operation_result", "[", "'metadata'", "]", "[", "'status'", "]", "==", "\"FAILED\"", ":", "raise", "Exception", "(", "\"Request: {0}, requestId: {1} failed to complete:\\n{2}\"", ".", "format", "(", "msg", ",", "six", ".", "text_type", "(", "promise", "[", "'requestId'", "]", ")", ",", "operation_result", "[", "'metadata'", "]", "[", "'message'", "]", ")", ")", "raise", "Exception", "(", "'Timed out waiting for asynchronous operation {0} \"{1}\" to complete.'", ".", "format", "(", "msg", ",", "six", ".", "text_type", "(", "promise", "[", "'requestId'", "]", ")", ")", ")"], "original_string": "def _wait_for_completion(conn, promise, wait_timeout, msg):\n    '''\n    Poll request status until resource is provisioned.\n    '''\n    if not promise:\n        return\n    wait_timeout = time.time() + wait_timeout\n    while wait_timeout > time.time():\n        time.sleep(5)\n        operation_result = conn.get_request(\n            request_id=promise['requestId'],\n            status=True)\n\n        if operation_result['metadata']['status'] == \"DONE\":\n            return\n        elif operation_result['metadata']['status'] == \"FAILED\":\n            raise Exception(\n                \"Request: {0}, requestId: {1} failed to complete:\\n{2}\".format(\n                    msg, six.text_type(promise['requestId']),\n                    operation_result['metadata']['message']\n                )\n            )\n\n    raise Exception(\n        'Timed out waiting for asynchronous operation {0} \"{1}\" to complete.'.format(\n            msg, six.text_type(promise['requestId'])\n        )\n    )"}, {"code": "def _concat_compat(to_concat, axis=0):\n    \"\"\"\n    provide concatenation of an array of arrays each of which is a single\n    'normalized' dtypes (in that for example, if it's object, then it is a\n    non-datetimelike and provide a combined dtype for the resulting array that\n    preserves the overall dtype if possible)\n\n    Parameters\n    ----------\n    to_concat : array of arrays\n    axis : axis to provide concatenation\n\n    Returns\n    -------\n    a single array, preserving the combined dtypes\n    \"\"\"\n\n    # filter empty arrays\n    # 1-d dtypes always are included here\n    def is_nonempty(x):\n        try:\n            return x.shape[axis] > 0\n        except Exception:\n            return True\n\n    # If all arrays are empty, there's nothing to convert, just short-cut to\n    # the concatenation, #3121.\n    #\n    # Creating an empty array directly is tempting, but the winnings would be\n    # marginal given that it would still require shape & dtype calculation and\n    # np.concatenate which has them both implemented is compiled.\n\n    typs = get_dtype_kinds(to_concat)\n    _contains_datetime = any(typ.startswith('datetime') for typ in typs)\n    _contains_period = any(typ.startswith('period') for typ in typs)\n\n    if 'category' in typs:\n        # this must be priort to _concat_datetime,\n        # to support Categorical + datetime-like\n        return _concat_categorical(to_concat, axis=axis)\n\n    elif _contains_datetime or 'timedelta' in typs or _contains_period:\n        return _concat_datetime(to_concat, axis=axis, typs=typs)\n\n    # these are mandated to handle empties as well\n    elif 'sparse' in typs:\n        return _concat_sparse(to_concat, axis=axis, typs=typs)\n\n    all_empty = all(not is_nonempty(x) for x in to_concat)\n    if any(is_extension_array_dtype(x) for x in to_concat) and axis == 1:\n        to_concat = [np.atleast_2d(x.astype('object')) for x in to_concat]\n\n    if all_empty:\n        # we have all empties, but may need to coerce the result dtype to\n        # object if we have non-numeric type operands (numpy would otherwise\n        # cast this to float)\n        typs = get_dtype_kinds(to_concat)\n        if len(typs) != 1:\n\n            if (not len(typs - {'i', 'u', 'f'}) or\n                    not len(typs - {'bool', 'i', 'u'})):\n                # let numpy coerce\n                pass\n            else:\n                # coerce to object\n                to_concat = [x.astype('object') for x in to_concat]\n\n    return np.concatenate(to_concat, axis=axis)", "code_tokens": ["def", "_concat_compat", "(", "to_concat", ",", "axis", "=", "0", ")", ":", "# filter empty arrays", "# 1-d dtypes always are included here", "def", "is_nonempty", "(", "x", ")", ":", "try", ":", "return", "x", ".", "shape", "[", "axis", "]", ">", "0", "except", "Exception", ":", "return", "True", "# If all arrays are empty, there's nothing to convert, just short-cut to", "# the concatenation, #3121.", "#", "# Creating an empty array directly is tempting, but the winnings would be", "# marginal given that it would still require shape & dtype calculation and", "# np.concatenate which has them both implemented is compiled.", "typs", "=", "get_dtype_kinds", "(", "to_concat", ")", "_contains_datetime", "=", "any", "(", "typ", ".", "startswith", "(", "'datetime'", ")", "for", "typ", "in", "typs", ")", "_contains_period", "=", "any", "(", "typ", ".", "startswith", "(", "'period'", ")", "for", "typ", "in", "typs", ")", "if", "'category'", "in", "typs", ":", "# this must be priort to _concat_datetime,", "# to support Categorical + datetime-like", "return", "_concat_categorical", "(", "to_concat", ",", "axis", "=", "axis", ")", "elif", "_contains_datetime", "or", "'timedelta'", "in", "typs", "or", "_contains_period", ":", "return", "_concat_datetime", "(", "to_concat", ",", "axis", "=", "axis", ",", "typs", "=", "typs", ")", "# these are mandated to handle empties as well", "elif", "'sparse'", "in", "typs", ":", "return", "_concat_sparse", "(", "to_concat", ",", "axis", "=", "axis", ",", "typs", "=", "typs", ")", "all_empty", "=", "all", "(", "not", "is_nonempty", "(", "x", ")", "for", "x", "in", "to_concat", ")", "if", "any", "(", "is_extension_array_dtype", "(", "x", ")", "for", "x", "in", "to_concat", ")", "and", "axis", "==", "1", ":", "to_concat", "=", "[", "np", ".", "atleast_2d", "(", "x", ".", "astype", "(", "'object'", ")", ")", "for", "x", "in", "to_concat", "]", "if", "all_empty", ":", "# we have all empties, but may need to coerce the result dtype to", "# object if we have non-numeric type operands (numpy would otherwise", "# cast this to float)", "typs", "=", "get_dtype_kinds", "(", "to_concat", ")", "if", "len", "(", "typs", ")", "!=", "1", ":", "if", "(", "not", "len", "(", "typs", "-", "{", "'i'", ",", "'u'", ",", "'f'", "}", ")", "or", "not", "len", "(", "typs", "-", "{", "'bool'", ",", "'i'", ",", "'u'", "}", ")", ")", ":", "# let numpy coerce", "pass", "else", ":", "# coerce to object", "to_concat", "=", "[", "x", ".", "astype", "(", "'object'", ")", "for", "x", "in", "to_concat", "]", "return", "np", ".", "concatenate", "(", "to_concat", ",", "axis", "=", "axis", ")"], "original_string": "def _concat_compat(to_concat, axis=0):\n    \"\"\"\n    provide concatenation of an array of arrays each of which is a single\n    'normalized' dtypes (in that for example, if it's object, then it is a\n    non-datetimelike and provide a combined dtype for the resulting array that\n    preserves the overall dtype if possible)\n\n    Parameters\n    ----------\n    to_concat : array of arrays\n    axis : axis to provide concatenation\n\n    Returns\n    -------\n    a single array, preserving the combined dtypes\n    \"\"\"\n\n    # filter empty arrays\n    # 1-d dtypes always are included here\n    def is_nonempty(x):\n        try:\n            return x.shape[axis] > 0\n        except Exception:\n            return True\n\n    # If all arrays are empty, there's nothing to convert, just short-cut to\n    # the concatenation, #3121.\n    #\n    # Creating an empty array directly is tempting, but the winnings would be\n    # marginal given that it would still require shape & dtype calculation and\n    # np.concatenate which has them both implemented is compiled.\n\n    typs = get_dtype_kinds(to_concat)\n    _contains_datetime = any(typ.startswith('datetime') for typ in typs)\n    _contains_period = any(typ.startswith('period') for typ in typs)\n\n    if 'category' in typs:\n        # this must be priort to _concat_datetime,\n        # to support Categorical + datetime-like\n        return _concat_categorical(to_concat, axis=axis)\n\n    elif _contains_datetime or 'timedelta' in typs or _contains_period:\n        return _concat_datetime(to_concat, axis=axis, typs=typs)\n\n    # these are mandated to handle empties as well\n    elif 'sparse' in typs:\n        return _concat_sparse(to_concat, axis=axis, typs=typs)\n\n    all_empty = all(not is_nonempty(x) for x in to_concat)\n    if any(is_extension_array_dtype(x) for x in to_concat) and axis == 1:\n        to_concat = [np.atleast_2d(x.astype('object')) for x in to_concat]\n\n    if all_empty:\n        # we have all empties, but may need to coerce the result dtype to\n        # object if we have non-numeric type operands (numpy would otherwise\n        # cast this to float)\n        typs = get_dtype_kinds(to_concat)\n        if len(typs) != 1:\n\n            if (not len(typs - {'i', 'u', 'f'}) or\n                    not len(typs - {'bool', 'i', 'u'})):\n                # let numpy coerce\n                pass\n            else:\n                # coerce to object\n                to_concat = [x.astype('object') for x in to_concat]\n\n    return np.concatenate(to_concat, axis=axis)"}, {"code": "def get_trace(self):\n        '''This returns an abbreviated stack trace with lines that only concern\n        the caller. In other words, the stack trace inside the Pexpect module\n        is not included. '''\n\n        tblist = traceback.extract_tb(sys.exc_info()[2])\n        tblist = [item for item in tblist if ('pexpect/__init__' not in item[0])\n                                           and ('pexpect/expect' not in item[0])]\n        tblist = traceback.format_list(tblist)\n        return ''.join(tblist)", "code_tokens": ["def", "get_trace", "(", "self", ")", ":", "tblist", "=", "traceback", ".", "extract_tb", "(", "sys", ".", "exc_info", "(", ")", "[", "2", "]", ")", "tblist", "=", "[", "item", "for", "item", "in", "tblist", "if", "(", "'pexpect/__init__'", "not", "in", "item", "[", "0", "]", ")", "and", "(", "'pexpect/expect'", "not", "in", "item", "[", "0", "]", ")", "]", "tblist", "=", "traceback", ".", "format_list", "(", "tblist", ")", "return", "''", ".", "join", "(", "tblist", ")"], "original_string": "def get_trace(self):\n        '''This returns an abbreviated stack trace with lines that only concern\n        the caller. In other words, the stack trace inside the Pexpect module\n        is not included. '''\n\n        tblist = traceback.extract_tb(sys.exc_info()[2])\n        tblist = [item for item in tblist if ('pexpect/__init__' not in item[0])\n                                           and ('pexpect/expect' not in item[0])]\n        tblist = traceback.format_list(tblist)\n        return ''.join(tblist)"}, {"code": "def create_namespaced_pod_preset(self, namespace, body, **kwargs):\n        \"\"\"\n        create a PodPreset\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_pod_preset(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1alpha1PodPreset body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1alpha1PodPreset\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_pod_preset_with_http_info(namespace, body, **kwargs)\n        else:\n            (data) = self.create_namespaced_pod_preset_with_http_info(namespace, body, **kwargs)\n            return data", "code_tokens": ["def", "create_namespaced_pod_preset", "(", "self", ",", "namespace", ",", "body", ",", "*", "*", "kwargs", ")", ":", "kwargs", "[", "'_return_http_data_only'", "]", "=", "True", "if", "kwargs", ".", "get", "(", "'async_req'", ")", ":", "return", "self", ".", "create_namespaced_pod_preset_with_http_info", "(", "namespace", ",", "body", ",", "*", "*", "kwargs", ")", "else", ":", "(", "data", ")", "=", "self", ".", "create_namespaced_pod_preset_with_http_info", "(", "namespace", ",", "body", ",", "*", "*", "kwargs", ")", "return", "data"], "original_string": "def create_namespaced_pod_preset(self, namespace, body, **kwargs):\n        \"\"\"\n        create a PodPreset\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_pod_preset(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1alpha1PodPreset body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1alpha1PodPreset\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_pod_preset_with_http_info(namespace, body, **kwargs)\n        else:\n            (data) = self.create_namespaced_pod_preset_with_http_info(namespace, body, **kwargs)\n            return data"}, {"code": "def factorial(n, mod=None):\n    \"\"\"Calculates factorial iteratively.\n    If mod is not None, then return (n! % mod)\n    Time Complexity - O(n)\"\"\"\n    if not (isinstance(n, int) and n >= 0):\n        raise ValueError(\"'n' must be a non-negative integer.\")\n    if mod is not None and not (isinstance(mod, int) and mod > 0):\n        raise ValueError(\"'mod' must be a positive integer\")\n    result = 1\n    if n == 0:\n        return 1\n    for i in range(2, n+1):\n        result *= i\n        if mod:\n            result %= mod\n    return result", "code_tokens": ["def", "factorial", "(", "n", ",", "mod", "=", "None", ")", ":", "if", "not", "(", "isinstance", "(", "n", ",", "int", ")", "and", "n", ">=", "0", ")", ":", "raise", "ValueError", "(", "\"'n' must be a non-negative integer.\"", ")", "if", "mod", "is", "not", "None", "and", "not", "(", "isinstance", "(", "mod", ",", "int", ")", "and", "mod", ">", "0", ")", ":", "raise", "ValueError", "(", "\"'mod' must be a positive integer\"", ")", "result", "=", "1", "if", "n", "==", "0", ":", "return", "1", "for", "i", "in", "range", "(", "2", ",", "n", "+", "1", ")", ":", "result", "*=", "i", "if", "mod", ":", "result", "%=", "mod", "return", "result"], "original_string": "def factorial(n, mod=None):\n    \"\"\"Calculates factorial iteratively.\n    If mod is not None, then return (n! % mod)\n    Time Complexity - O(n)\"\"\"\n    if not (isinstance(n, int) and n >= 0):\n        raise ValueError(\"'n' must be a non-negative integer.\")\n    if mod is not None and not (isinstance(mod, int) and mod > 0):\n        raise ValueError(\"'mod' must be a positive integer\")\n    result = 1\n    if n == 0:\n        return 1\n    for i in range(2, n+1):\n        result *= i\n        if mod:\n            result %= mod\n    return result"}, {"code": "def cancel_entrust(self, entrust_no):\n        \"\"\"\n        \u5bf9\u672a\u6210\u4ea4\u7684\u8c03\u4ed3\u8fdb\u884c\u4f2a\u64a4\u5355\n        :param entrust_no:\n        :return:\n        \"\"\"\n        xq_entrust_list = self._get_xq_history()\n        is_have = False\n        for xq_entrusts in xq_entrust_list:\n            status = xq_entrusts[\"status\"]  # \u8c03\u4ed3\u72b6\u6001\n            for entrust in xq_entrusts[\"rebalancing_histories\"]:\n                if entrust[\"id\"] == entrust_no and status == \"pending\":\n                    is_have = True\n                    buy_or_sell = (\n                        \"buy\"\n                        if entrust[\"target_weight\"] < entrust[\"weight\"]\n                        else \"sell\"\n                    )\n                    if (\n                        entrust[\"target_weight\"] == 0\n                        and entrust[\"weight\"] == 0\n                    ):\n                        raise exceptions.TradeError(u\"\u79fb\u9664\u7684\u80a1\u7968\u64cd\u4f5c\u65e0\u6cd5\u64a4\u9500,\u5efa\u8bae\u91cd\u65b0\u4e70\u5165\")\n                    balance = self.get_balance()[0]\n                    volume = (\n                        abs(entrust[\"target_weight\"] - entrust[\"weight\"])\n                        * balance[\"asset_balance\"]\n                        / 100\n                    )\n                    r = self._trade(\n                        security=entrust[\"stock_symbol\"],\n                        volume=volume,\n                        entrust_bs=buy_or_sell,\n                    )\n                    if len(r) > 0 and \"error_info\" in r[0]:\n                        raise exceptions.TradeError(\n                            u\"\u64a4\u9500\u5931\u8d25!%s\" % (\"error_info\" in r[0])\n                        )\n        if not is_have:\n            raise exceptions.TradeError(u\"\u64a4\u9500\u5bf9\u8c61\u5df2\u5931\u6548\")\n        return True", "code_tokens": ["def", "cancel_entrust", "(", "self", ",", "entrust_no", ")", ":", "xq_entrust_list", "=", "self", ".", "_get_xq_history", "(", ")", "is_have", "=", "False", "for", "xq_entrusts", "in", "xq_entrust_list", ":", "status", "=", "xq_entrusts", "[", "\"status\"", "]", "# \u8c03\u4ed3\u72b6\u6001", "for", "entrust", "in", "xq_entrusts", "[", "\"rebalancing_histories\"", "]", ":", "if", "entrust", "[", "\"id\"", "]", "==", "entrust_no", "and", "status", "==", "\"pending\"", ":", "is_have", "=", "True", "buy_or_sell", "=", "(", "\"buy\"", "if", "entrust", "[", "\"target_weight\"", "]", "<", "entrust", "[", "\"weight\"", "]", "else", "\"sell\"", ")", "if", "(", "entrust", "[", "\"target_weight\"", "]", "==", "0", "and", "entrust", "[", "\"weight\"", "]", "==", "0", ")", ":", "raise", "exceptions", ".", "TradeError", "(", "u\"\u79fb\u9664\u7684\u80a1\u7968\u64cd\u4f5c\u65e0\u6cd5\u64a4\u9500,\u5efa\u8bae\u91cd\u65b0\u4e70\u5165\")", "", "balance", "=", "self", ".", "get_balance", "(", ")", "[", "0", "]", "volume", "=", "(", "abs", "(", "entrust", "[", "\"target_weight\"", "]", "-", "entrust", "[", "\"weight\"", "]", ")", "*", "balance", "[", "\"asset_balance\"", "]", "/", "100", ")", "r", "=", "self", ".", "_trade", "(", "security", "=", "entrust", "[", "\"stock_symbol\"", "]", ",", "volume", "=", "volume", ",", "entrust_bs", "=", "buy_or_sell", ",", ")", "if", "len", "(", "r", ")", ">", "0", "and", "\"error_info\"", "in", "r", "[", "0", "]", ":", "raise", "exceptions", ".", "TradeError", "(", "u\"\u64a4\u9500\u5931\u8d25!%s\" % (\"err", "r", "i", "nfo\" in r[0]", "", "", "", "", "", "", ")", "if", "not", "is_have", ":", "raise", "exceptions", ".", "TradeError", "(", "u\"\u64a4\u9500\u5bf9\u8c61\u5df2\u5931\u6548\")", "", "return", "True"], "original_string": "def cancel_entrust(self, entrust_no):\n        \"\"\"\n        \u5bf9\u672a\u6210\u4ea4\u7684\u8c03\u4ed3\u8fdb\u884c\u4f2a\u64a4\u5355\n        :param entrust_no:\n        :return:\n        \"\"\"\n        xq_entrust_list = self._get_xq_history()\n        is_have = False\n        for xq_entrusts in xq_entrust_list:\n            status = xq_entrusts[\"status\"]  # \u8c03\u4ed3\u72b6\u6001\n            for entrust in xq_entrusts[\"rebalancing_histories\"]:\n                if entrust[\"id\"] == entrust_no and status == \"pending\":\n                    is_have = True\n                    buy_or_sell = (\n                        \"buy\"\n                        if entrust[\"target_weight\"] < entrust[\"weight\"]\n                        else \"sell\"\n                    )\n                    if (\n                        entrust[\"target_weight\"] == 0\n                        and entrust[\"weight\"] == 0\n                    ):\n                        raise exceptions.TradeError(u\"\u79fb\u9664\u7684\u80a1\u7968\u64cd\u4f5c\u65e0\u6cd5\u64a4\u9500,\u5efa\u8bae\u91cd\u65b0\u4e70\u5165\")\n                    balance = self.get_balance()[0]\n                    volume = (\n                        abs(entrust[\"target_weight\"] - entrust[\"weight\"])\n                        * balance[\"asset_balance\"]\n                        / 100\n                    )\n                    r = self._trade(\n                        security=entrust[\"stock_symbol\"],\n                        volume=volume,\n                        entrust_bs=buy_or_sell,\n                    )\n                    if len(r) > 0 and \"error_info\" in r[0]:\n                        raise exceptions.TradeError(\n                            u\"\u64a4\u9500\u5931\u8d25!%s\" % (\"error_info\" in r[0])\n                        )\n        if not is_have:\n            raise exceptions.TradeError(u\"\u64a4\u9500\u5bf9\u8c61\u5df2\u5931\u6548\")\n        return True"}, {"code": "def do_one(myStats, destIP, hostname, timeout, mySeqNumber, packet_size, quiet=False):\n    \"\"\"\n    Returns either the delay (in ms) or None on timeout.\n    \"\"\"\n    delay = None\n\n    try:  # One could use UDP here, but it's obscure\n        mySocket = socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.getprotobyname(\"icmp\"))\n    except socket.error as e:\n        print(\"failed. (socket error: '%s')\" % e.args[1])\n        raise  # raise the original error\n\n    my_ID = os.getpid() & 0xFFFF\n\n    sentTime = send_one_ping(mySocket, destIP, my_ID, mySeqNumber, packet_size)\n    if sentTime == None:\n        mySocket.close()\n        return delay\n\n    myStats.pktsSent += 1\n\n    recvTime, dataSize, iphSrcIP, icmpSeqNumber, iphTTL = receive_one_ping(mySocket, my_ID, timeout)\n\n    mySocket.close()\n\n    if recvTime:\n        delay = (recvTime - sentTime) * 1000\n        if not quiet:\n            print(\"%d bytes from %s: icmp_seq=%d ttl=%d time=%d ms\" % (\n                dataSize, socket.inet_ntoa(struct.pack(\"!I\", iphSrcIP)), icmpSeqNumber, iphTTL, delay)\n                  )\n        myStats.pktsRcvd += 1\n        myStats.totTime += delay\n        if myStats.minTime > delay:\n            myStats.minTime = delay\n        if myStats.maxTime < delay:\n            myStats.maxTime = delay\n    else:\n        delay = None\n        print(\"Request timed out.\")\n\n    return delay", "code_tokens": ["def", "do_one", "(", "myStats", ",", "destIP", ",", "hostname", ",", "timeout", ",", "mySeqNumber", ",", "packet_size", ",", "quiet", "=", "False", ")", ":", "delay", "=", "None", "try", ":", "# One could use UDP here, but it's obscure", "mySocket", "=", "socket", ".", "socket", "(", "socket", ".", "AF_INET", ",", "socket", ".", "SOCK_RAW", ",", "socket", ".", "getprotobyname", "(", "\"icmp\"", ")", ")", "except", "socket", ".", "error", "as", "e", ":", "print", "(", "\"failed. (socket error: '%s')\"", "%", "e", ".", "args", "[", "1", "]", ")", "raise", "# raise the original error", "my_ID", "=", "os", ".", "getpid", "(", ")", "&", "0xFFFF", "sentTime", "=", "send_one_ping", "(", "mySocket", ",", "destIP", ",", "my_ID", ",", "mySeqNumber", ",", "packet_size", ")", "if", "sentTime", "==", "None", ":", "mySocket", ".", "close", "(", ")", "return", "delay", "myStats", ".", "pktsSent", "+=", "1", "recvTime", ",", "dataSize", ",", "iphSrcIP", ",", "icmpSeqNumber", ",", "iphTTL", "=", "receive_one_ping", "(", "mySocket", ",", "my_ID", ",", "timeout", ")", "mySocket", ".", "close", "(", ")", "if", "recvTime", ":", "delay", "=", "(", "recvTime", "-", "sentTime", ")", "*", "1000", "if", "not", "quiet", ":", "print", "(", "\"%d bytes from %s: icmp_seq=%d ttl=%d time=%d ms\"", "%", "(", "dataSize", ",", "socket", ".", "inet_ntoa", "(", "struct", ".", "pack", "(", "\"!I\"", ",", "iphSrcIP", ")", ")", ",", "icmpSeqNumber", ",", "iphTTL", ",", "delay", ")", ")", "myStats", ".", "pktsRcvd", "+=", "1", "myStats", ".", "totTime", "+=", "delay", "if", "myStats", ".", "minTime", ">", "delay", ":", "myStats", ".", "minTime", "=", "delay", "if", "myStats", ".", "maxTime", "<", "delay", ":", "myStats", ".", "maxTime", "=", "delay", "else", ":", "delay", "=", "None", "print", "(", "\"Request timed out.\"", ")", "return", "delay"], "original_string": "def do_one(myStats, destIP, hostname, timeout, mySeqNumber, packet_size, quiet=False):\n    \"\"\"\n    Returns either the delay (in ms) or None on timeout.\n    \"\"\"\n    delay = None\n\n    try:  # One could use UDP here, but it's obscure\n        mySocket = socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.getprotobyname(\"icmp\"))\n    except socket.error as e:\n        print(\"failed. (socket error: '%s')\" % e.args[1])\n        raise  # raise the original error\n\n    my_ID = os.getpid() & 0xFFFF\n\n    sentTime = send_one_ping(mySocket, destIP, my_ID, mySeqNumber, packet_size)\n    if sentTime == None:\n        mySocket.close()\n        return delay\n\n    myStats.pktsSent += 1\n\n    recvTime, dataSize, iphSrcIP, icmpSeqNumber, iphTTL = receive_one_ping(mySocket, my_ID, timeout)\n\n    mySocket.close()\n\n    if recvTime:\n        delay = (recvTime - sentTime) * 1000\n        if not quiet:\n            print(\"%d bytes from %s: icmp_seq=%d ttl=%d time=%d ms\" % (\n                dataSize, socket.inet_ntoa(struct.pack(\"!I\", iphSrcIP)), icmpSeqNumber, iphTTL, delay)\n                  )\n        myStats.pktsRcvd += 1\n        myStats.totTime += delay\n        if myStats.minTime > delay:\n            myStats.minTime = delay\n        if myStats.maxTime < delay:\n            myStats.maxTime = delay\n    else:\n        delay = None\n        print(\"Request timed out.\")\n\n    return delay"}, {"code": "def help(module=None, *args):\n    '''\n    Display help on Ansible standard module.\n\n    :param module:\n    :return:\n    '''\n    if not module:\n        raise CommandExecutionError('Please tell me what module you want to have helped with. '\n                                    'Or call \"ansible.list\" to know what is available.')\n    try:\n        module = _resolver.load_module(module)\n    except (ImportError, LoaderError) as err:\n        raise CommandExecutionError('Module \"{0}\" is currently not functional on your system.'.format(module))\n\n    doc = {}\n    ret = {}\n    for docset in module.DOCUMENTATION.split('---'):\n        try:\n            docset = salt.utils.yaml.safe_load(docset)\n            if docset:\n                doc.update(docset)\n        except Exception as err:\n            log.error(\"Error parsing doc section: %s\", err)\n    if not args:\n        if 'description' in doc:\n            description = doc.get('description') or ''\n            del doc['description']\n            ret['Description'] = description\n        ret['Available sections on module \"{}\"'.format(module.__name__.replace('ansible.modules.', ''))] = doc.keys()\n    else:\n        for arg in args:\n            info = doc.get(arg)\n            if info is not None:\n                ret[arg] = info\n\n    return ret", "code_tokens": ["def", "help", "(", "module", "=", "None", ",", "*", "args", ")", ":", "if", "not", "module", ":", "raise", "CommandExecutionError", "(", "'Please tell me what module you want to have helped with. '", "'Or call \"ansible.list\" to know what is available.'", ")", "try", ":", "module", "=", "_resolver", ".", "load_module", "(", "module", ")", "except", "(", "ImportError", ",", "LoaderError", ")", "as", "err", ":", "raise", "CommandExecutionError", "(", "'Module \"{0}\" is currently not functional on your system.'", ".", "format", "(", "module", ")", ")", "doc", "=", "{", "}", "ret", "=", "{", "}", "for", "docset", "in", "module", ".", "DOCUMENTATION", ".", "split", "(", "'---'", ")", ":", "try", ":", "docset", "=", "salt", ".", "utils", ".", "yaml", ".", "safe_load", "(", "docset", ")", "if", "docset", ":", "doc", ".", "update", "(", "docset", ")", "except", "Exception", "as", "err", ":", "log", ".", "error", "(", "\"Error parsing doc section: %s\"", ",", "err", ")", "if", "not", "args", ":", "if", "'description'", "in", "doc", ":", "description", "=", "doc", ".", "get", "(", "'description'", ")", "or", "''", "del", "doc", "[", "'description'", "]", "ret", "[", "'Description'", "]", "=", "description", "ret", "[", "'Available sections on module \"{}\"'", ".", "format", "(", "module", ".", "__name__", ".", "replace", "(", "'ansible.modules.'", ",", "''", ")", ")", "]", "=", "doc", ".", "keys", "(", ")", "else", ":", "for", "arg", "in", "args", ":", "info", "=", "doc", ".", "get", "(", "arg", ")", "if", "info", "is", "not", "None", ":", "ret", "[", "arg", "]", "=", "info", "return", "ret"], "original_string": "def help(module=None, *args):\n    '''\n    Display help on Ansible standard module.\n\n    :param module:\n    :return:\n    '''\n    if not module:\n        raise CommandExecutionError('Please tell me what module you want to have helped with. '\n                                    'Or call \"ansible.list\" to know what is available.')\n    try:\n        module = _resolver.load_module(module)\n    except (ImportError, LoaderError) as err:\n        raise CommandExecutionError('Module \"{0}\" is currently not functional on your system.'.format(module))\n\n    doc = {}\n    ret = {}\n    for docset in module.DOCUMENTATION.split('---'):\n        try:\n            docset = salt.utils.yaml.safe_load(docset)\n            if docset:\n                doc.update(docset)\n        except Exception as err:\n            log.error(\"Error parsing doc section: %s\", err)\n    if not args:\n        if 'description' in doc:\n            description = doc.get('description') or ''\n            del doc['description']\n            ret['Description'] = description\n        ret['Available sections on module \"{}\"'.format(module.__name__.replace('ansible.modules.', ''))] = doc.keys()\n    else:\n        for arg in args:\n            info = doc.get(arg)\n            if info is not None:\n                ret[arg] = info\n\n    return ret"}, {"code": "def xpathNewValueTree(self):\n        \"\"\"Create a new xmlXPathObjectPtr of type Value Tree (XSLT)\n           and initialize it with the tree root @val \"\"\"\n        ret = libxml2mod.xmlXPathNewValueTree(self._o)\n        if ret is None:raise xpathError('xmlXPathNewValueTree() failed')\n        return xpathObjectRet(ret)", "code_tokens": ["def", "xpathNewValueTree", "(", "self", ")", ":", "ret", "=", "libxml2mod", ".", "xmlXPathNewValueTree", "(", "self", ".", "_o", ")", "if", "ret", "is", "None", ":", "raise", "xpathError", "(", "'xmlXPathNewValueTree() failed'", ")", "return", "xpathObjectRet", "(", "ret", ")"], "original_string": "def xpathNewValueTree(self):\n        \"\"\"Create a new xmlXPathObjectPtr of type Value Tree (XSLT)\n           and initialize it with the tree root @val \"\"\"\n        ret = libxml2mod.xmlXPathNewValueTree(self._o)\n        if ret is None:raise xpathError('xmlXPathNewValueTree() failed')\n        return xpathObjectRet(ret)"}, {"code": "def get_yml_content(file_path):\n    '''Load yaml file content'''\n    try:\n        with open(file_path, 'r') as file:\n            return yaml.load(file, Loader=yaml.Loader)\n    except yaml.scanner.ScannerError as err:\n        print_error('yaml file format error!')\n        exit(1)\n    except Exception as exception:\n        print_error(exception)\n        exit(1)", "code_tokens": ["def", "get_yml_content", "(", "file_path", ")", ":", "try", ":", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "file", ":", "return", "yaml", ".", "load", "(", "file", ",", "Loader", "=", "yaml", ".", "Loader", ")", "except", "yaml", ".", "scanner", ".", "ScannerError", "as", "err", ":", "print_error", "(", "'yaml file format error!'", ")", "exit", "(", "1", ")", "except", "Exception", "as", "exception", ":", "print_error", "(", "exception", ")", "exit", "(", "1", ")"], "original_string": "def get_yml_content(file_path):\n    '''Load yaml file content'''\n    try:\n        with open(file_path, 'r') as file:\n            return yaml.load(file, Loader=yaml.Loader)\n    except yaml.scanner.ScannerError as err:\n        print_error('yaml file format error!')\n        exit(1)\n    except Exception as exception:\n        print_error(exception)\n        exit(1)"}, {"code": "def create_alias(FunctionName, Name, FunctionVersion, Description=\"\",\n                 region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a valid config, create an alias to a function.\n\n    Returns {created: true} if the alias was created and returns\n    {created: False} if the alias was not created.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_lamba.create_alias my_function my_alias $LATEST \"An alias\"\n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        alias = conn.create_alias(FunctionName=FunctionName, Name=Name,\n                                  FunctionVersion=FunctionVersion, Description=Description)\n        if alias:\n            log.info('The newly created alias name is %s', alias['Name'])\n\n            return {'created': True, 'name': alias['Name']}\n        else:\n            log.warning('Alias was not created')\n            return {'created': False}\n    except ClientError as e:\n        return {'created': False, 'error': __utils__['boto3.get_error'](e)}", "code_tokens": ["def", "create_alias", "(", "FunctionName", ",", "Name", ",", "FunctionVersion", ",", "Description", "=", "\"\"", ",", "region", "=", "None", ",", "key", "=", "None", ",", "keyid", "=", "None", ",", "profile", "=", "None", ")", ":", "try", ":", "conn", "=", "_get_conn", "(", "region", "=", "region", ",", "key", "=", "key", ",", "keyid", "=", "keyid", ",", "profile", "=", "profile", ")", "alias", "=", "conn", ".", "create_alias", "(", "FunctionName", "=", "FunctionName", ",", "Name", "=", "Name", ",", "FunctionVersion", "=", "FunctionVersion", ",", "Description", "=", "Description", ")", "if", "alias", ":", "log", ".", "info", "(", "'The newly created alias name is %s'", ",", "alias", "[", "'Name'", "]", ")", "return", "{", "'created'", ":", "True", ",", "'name'", ":", "alias", "[", "'Name'", "]", "}", "else", ":", "log", ".", "warning", "(", "'Alias was not created'", ")", "return", "{", "'created'", ":", "False", "}", "except", "ClientError", "as", "e", ":", "return", "{", "'created'", ":", "False", ",", "'error'", ":", "__utils__", "[", "'boto3.get_error'", "]", "(", "e", ")", "}"], "original_string": "def create_alias(FunctionName, Name, FunctionVersion, Description=\"\",\n                 region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a valid config, create an alias to a function.\n\n    Returns {created: true} if the alias was created and returns\n    {created: False} if the alias was not created.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_lamba.create_alias my_function my_alias $LATEST \"An alias\"\n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        alias = conn.create_alias(FunctionName=FunctionName, Name=Name,\n                                  FunctionVersion=FunctionVersion, Description=Description)\n        if alias:\n            log.info('The newly created alias name is %s', alias['Name'])\n\n            return {'created': True, 'name': alias['Name']}\n        else:\n            log.warning('Alias was not created')\n            return {'created': False}\n    except ClientError as e:\n        return {'created': False, 'error': __utils__['boto3.get_error'](e)}"}, {"code": "def attach_pipeline(self, pipeline, name, chunks=None, eager=True):\n        \"\"\"Register a pipeline to be computed at the start of each day.\n\n        Parameters\n        ----------\n        pipeline : Pipeline\n            The pipeline to have computed.\n        name : str\n            The name of the pipeline.\n        chunks : int or iterator, optional\n            The number of days to compute pipeline results for. Increasing\n            this number will make it longer to get the first results but\n            may improve the total runtime of the simulation. If an iterator\n            is passed, we will run in chunks based on values of the iterator.\n            Default is True.\n        eager : bool, optional\n            Whether or not to compute this pipeline prior to\n            before_trading_start.\n\n        Returns\n        -------\n        pipeline : Pipeline\n            Returns the pipeline that was attached unchanged.\n\n        See Also\n        --------\n        :func:`zipline.api.pipeline_output`\n        \"\"\"\n        if chunks is None:\n            # Make the first chunk smaller to get more immediate results:\n            # (one week, then every half year)\n            chunks = chain([5], repeat(126))\n        elif isinstance(chunks, int):\n            chunks = repeat(chunks)\n\n        if name in self._pipelines:\n            raise DuplicatePipelineName(name=name)\n\n        self._pipelines[name] = AttachedPipeline(pipeline, iter(chunks), eager)\n\n        # Return the pipeline to allow expressions like\n        # p = attach_pipeline(Pipeline(), 'name')\n        return pipeline", "code_tokens": ["def", "attach_pipeline", "(", "self", ",", "pipeline", ",", "name", ",", "chunks", "=", "None", ",", "eager", "=", "True", ")", ":", "if", "chunks", "is", "None", ":", "# Make the first chunk smaller to get more immediate results:", "# (one week, then every half year)", "chunks", "=", "chain", "(", "[", "5", "]", ",", "repeat", "(", "126", ")", ")", "elif", "isinstance", "(", "chunks", ",", "int", ")", ":", "chunks", "=", "repeat", "(", "chunks", ")", "if", "name", "in", "self", ".", "_pipelines", ":", "raise", "DuplicatePipelineName", "(", "name", "=", "name", ")", "self", ".", "_pipelines", "[", "name", "]", "=", "AttachedPipeline", "(", "pipeline", ",", "iter", "(", "chunks", ")", ",", "eager", ")", "# Return the pipeline to allow expressions like", "# p = attach_pipeline(Pipeline(), 'name')", "return", "pipeline"], "original_string": "def attach_pipeline(self, pipeline, name, chunks=None, eager=True):\n        \"\"\"Register a pipeline to be computed at the start of each day.\n\n        Parameters\n        ----------\n        pipeline : Pipeline\n            The pipeline to have computed.\n        name : str\n            The name of the pipeline.\n        chunks : int or iterator, optional\n            The number of days to compute pipeline results for. Increasing\n            this number will make it longer to get the first results but\n            may improve the total runtime of the simulation. If an iterator\n            is passed, we will run in chunks based on values of the iterator.\n            Default is True.\n        eager : bool, optional\n            Whether or not to compute this pipeline prior to\n            before_trading_start.\n\n        Returns\n        -------\n        pipeline : Pipeline\n            Returns the pipeline that was attached unchanged.\n\n        See Also\n        --------\n        :func:`zipline.api.pipeline_output`\n        \"\"\"\n        if chunks is None:\n            # Make the first chunk smaller to get more immediate results:\n            # (one week, then every half year)\n            chunks = chain([5], repeat(126))\n        elif isinstance(chunks, int):\n            chunks = repeat(chunks)\n\n        if name in self._pipelines:\n            raise DuplicatePipelineName(name=name)\n\n        self._pipelines[name] = AttachedPipeline(pipeline, iter(chunks), eager)\n\n        # Return the pipeline to allow expressions like\n        # p = attach_pipeline(Pipeline(), 'name')\n        return pipeline"}, {"code": "def QA_fetch_stock_day_full_adv(date):\n    '''\n    '\u8fd4\u56de\u5168\u5e02\u573a\u67d0\u4e00\u5929\u7684\u6570\u636e'\n    :param date:\n    :return: QA_DataStruct_Stock_day\u7c7b \u578b\u6570\u636e\n    '''\n    # \ud83d\udee0 todo \u68c0\u67e5\u65e5\u671fdata\u53c2\u6570\n    res = QA_fetch_stock_full(date, 'pd')\n    if res is None:\n        print(\"QA Error QA_fetch_stock_day_full_adv parameter date=%s call QA_fetch_stock_full return None\" % (date))\n        return None\n    else:\n        res_set_index = res.set_index(['date', 'code'])\n        # if res_set_index is None:\n        #     print(\"QA Error QA_fetch_stock_day_full set index 'date, code' return None\")\n        return QA_DataStruct_Stock_day(res_set_index)", "code_tokens": ["def", "QA_fetch_stock_day_full_adv", "(", "date", ")", ":", "# \ud83d\udee0 todo \u68c0\u67e5\u65e5\u671fdata\u53c2\u6570", "res", "=", "QA_fetch_stock_full", "(", "date", ",", "'pd'", ")", "if", "res", "is", "None", ":", "print", "(", "\"QA Error QA_fetch_stock_day_full_adv parameter date=%s call QA_fetch_stock_full return None\"", "%", "(", "date", ")", ")", "return", "None", "else", ":", "res_set_index", "=", "res", ".", "set_index", "(", "[", "'date'", ",", "'code'", "]", ")", "# if res_set_index is None:", "#     print(\"QA Error QA_fetch_stock_day_full set index 'date, code' return None\")", "return", "QA_DataStruct_Stock_day", "(", "res_set_index", ")"], "original_string": "def QA_fetch_stock_day_full_adv(date):\n    '''\n    '\u8fd4\u56de\u5168\u5e02\u573a\u67d0\u4e00\u5929\u7684\u6570\u636e'\n    :param date:\n    :return: QA_DataStruct_Stock_day\u7c7b \u578b\u6570\u636e\n    '''\n    # \ud83d\udee0 todo \u68c0\u67e5\u65e5\u671fdata\u53c2\u6570\n    res = QA_fetch_stock_full(date, 'pd')\n    if res is None:\n        print(\"QA Error QA_fetch_stock_day_full_adv parameter date=%s call QA_fetch_stock_full return None\" % (date))\n        return None\n    else:\n        res_set_index = res.set_index(['date', 'code'])\n        # if res_set_index is None:\n        #     print(\"QA Error QA_fetch_stock_day_full set index 'date, code' return None\")\n        return QA_DataStruct_Stock_day(res_set_index)"}, {"code": "def avail(search=None, verbose=False):\n    '''\n    Return a list of available images\n\n    search : string\n        search keyword\n    verbose : boolean (False)\n        toggle verbose output\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' imgadm.avail [percona]\n        salt '*' imgadm.avail verbose=True\n    '''\n    ret = {}\n    cmd = 'imgadm avail -j'\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = _exit_status(retcode)\n        return ret\n\n    for image in salt.utils.json.loads(res['stdout']):\n        if image['manifest']['disabled'] or not image['manifest']['public']:\n            continue\n        if search and search not in image['manifest']['name']:\n            # we skip if we are searching but don't have a match\n            continue\n        uuid = image['manifest']['uuid']\n        data = _parse_image_meta(image, verbose)\n        if data:\n            ret[uuid] = data\n\n    return ret", "code_tokens": ["def", "avail", "(", "search", "=", "None", ",", "verbose", "=", "False", ")", ":", "ret", "=", "{", "}", "cmd", "=", "'imgadm avail -j'", "res", "=", "__salt__", "[", "'cmd.run_all'", "]", "(", "cmd", ")", "retcode", "=", "res", "[", "'retcode'", "]", "if", "retcode", "!=", "0", ":", "ret", "[", "'Error'", "]", "=", "_exit_status", "(", "retcode", ")", "return", "ret", "for", "image", "in", "salt", ".", "utils", ".", "json", ".", "loads", "(", "res", "[", "'stdout'", "]", ")", ":", "if", "image", "[", "'manifest'", "]", "[", "'disabled'", "]", "or", "not", "image", "[", "'manifest'", "]", "[", "'public'", "]", ":", "continue", "if", "search", "and", "search", "not", "in", "image", "[", "'manifest'", "]", "[", "'name'", "]", ":", "# we skip if we are searching but don't have a match", "continue", "uuid", "=", "image", "[", "'manifest'", "]", "[", "'uuid'", "]", "data", "=", "_parse_image_meta", "(", "image", ",", "verbose", ")", "if", "data", ":", "ret", "[", "uuid", "]", "=", "data", "return", "ret"], "original_string": "def avail(search=None, verbose=False):\n    '''\n    Return a list of available images\n\n    search : string\n        search keyword\n    verbose : boolean (False)\n        toggle verbose output\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' imgadm.avail [percona]\n        salt '*' imgadm.avail verbose=True\n    '''\n    ret = {}\n    cmd = 'imgadm avail -j'\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = _exit_status(retcode)\n        return ret\n\n    for image in salt.utils.json.loads(res['stdout']):\n        if image['manifest']['disabled'] or not image['manifest']['public']:\n            continue\n        if search and search not in image['manifest']['name']:\n            # we skip if we are searching but don't have a match\n            continue\n        uuid = image['manifest']['uuid']\n        data = _parse_image_meta(image, verbose)\n        if data:\n            ret[uuid] = data\n\n    return ret"}, {"code": "def make_dataloader(data_train, data_val, data_test, args,\n                    use_average_length=False, num_shards=0, num_workers=8):\n    \"\"\"Create data loaders for training/validation/test.\"\"\"\n    data_train_lengths = get_data_lengths(data_train)\n    data_val_lengths = get_data_lengths(data_val)\n    data_test_lengths = get_data_lengths(data_test)\n    train_batchify_fn = btf.Tuple(btf.Pad(), btf.Pad(),\n                                  btf.Stack(dtype='float32'), btf.Stack(dtype='float32'))\n    test_batchify_fn = btf.Tuple(btf.Pad(), btf.Pad(),\n                                 btf.Stack(dtype='float32'), btf.Stack(dtype='float32'),\n                                 btf.Stack())\n    target_val_lengths = list(map(lambda x: x[-1], data_val_lengths))\n    target_test_lengths = list(map(lambda x: x[-1], data_test_lengths))\n    if args.bucket_scheme == 'constant':\n        bucket_scheme = nlp.data.ConstWidthBucket()\n    elif args.bucket_scheme == 'linear':\n        bucket_scheme = nlp.data.LinearWidthBucket()\n    elif args.bucket_scheme == 'exp':\n        bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n    else:\n        raise NotImplementedError\n    train_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_train_lengths,\n                                                      batch_size=args.batch_size,\n                                                      num_buckets=args.num_buckets,\n                                                      ratio=args.bucket_ratio,\n                                                      shuffle=True,\n                                                      use_average_length=use_average_length,\n                                                      num_shards=num_shards,\n                                                      bucket_scheme=bucket_scheme)\n    logging.info('Train Batch Sampler:\\n%s', train_batch_sampler.stats())\n    train_data_loader = nlp.data.ShardedDataLoader(data_train,\n                                                   batch_sampler=train_batch_sampler,\n                                                   batchify_fn=train_batchify_fn,\n                                                   num_workers=num_workers)\n\n    val_batch_sampler = nlp.data.FixedBucketSampler(lengths=target_val_lengths,\n                                                    batch_size=args.test_batch_size,\n                                                    num_buckets=args.num_buckets,\n                                                    ratio=args.bucket_ratio,\n                                                    shuffle=False,\n                                                    use_average_length=use_average_length,\n                                                    bucket_scheme=bucket_scheme)\n    logging.info('Valid Batch Sampler:\\n%s', val_batch_sampler.stats())\n    val_data_loader = gluon.data.DataLoader(data_val,\n                                            batch_sampler=val_batch_sampler,\n                                            batchify_fn=test_batchify_fn,\n                                            num_workers=num_workers)\n    test_batch_sampler = nlp.data.FixedBucketSampler(lengths=target_test_lengths,\n                                                     batch_size=args.test_batch_size,\n                                                     num_buckets=args.num_buckets,\n                                                     ratio=args.bucket_ratio,\n                                                     shuffle=False,\n                                                     use_average_length=use_average_length,\n                                                     bucket_scheme=bucket_scheme)\n    logging.info('Test Batch Sampler:\\n%s', test_batch_sampler.stats())\n    test_data_loader = gluon.data.DataLoader(data_test,\n                                             batch_sampler=test_batch_sampler,\n                                             batchify_fn=test_batchify_fn,\n                                             num_workers=num_workers)\n    return train_data_loader, val_data_loader, test_data_loader", "code_tokens": ["def", "make_dataloader", "(", "data_train", ",", "data_val", ",", "data_test", ",", "args", ",", "use_average_length", "=", "False", ",", "num_shards", "=", "0", ",", "num_workers", "=", "8", ")", ":", "data_train_lengths", "=", "get_data_lengths", "(", "data_train", ")", "data_val_lengths", "=", "get_data_lengths", "(", "data_val", ")", "data_test_lengths", "=", "get_data_lengths", "(", "data_test", ")", "train_batchify_fn", "=", "btf", ".", "Tuple", "(", "btf", ".", "Pad", "(", ")", ",", "btf", ".", "Pad", "(", ")", ",", "btf", ".", "Stack", "(", "dtype", "=", "'float32'", ")", ",", "btf", ".", "Stack", "(", "dtype", "=", "'float32'", ")", ")", "test_batchify_fn", "=", "btf", ".", "Tuple", "(", "btf", ".", "Pad", "(", ")", ",", "btf", ".", "Pad", "(", ")", ",", "btf", ".", "Stack", "(", "dtype", "=", "'float32'", ")", ",", "btf", ".", "Stack", "(", "dtype", "=", "'float32'", ")", ",", "btf", ".", "Stack", "(", ")", ")", "target_val_lengths", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "-", "1", "]", ",", "data_val_lengths", ")", ")", "target_test_lengths", "=", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "-", "1", "]", ",", "data_test_lengths", ")", ")", "if", "args", ".", "bucket_scheme", "==", "'constant'", ":", "bucket_scheme", "=", "nlp", ".", "data", ".", "ConstWidthBucket", "(", ")", "elif", "args", ".", "bucket_scheme", "==", "'linear'", ":", "bucket_scheme", "=", "nlp", ".", "data", ".", "LinearWidthBucket", "(", ")", "elif", "args", ".", "bucket_scheme", "==", "'exp'", ":", "bucket_scheme", "=", "nlp", ".", "data", ".", "ExpWidthBucket", "(", "bucket_len_step", "=", "1.2", ")", "else", ":", "raise", "NotImplementedError", "train_batch_sampler", "=", "nlp", ".", "data", ".", "FixedBucketSampler", "(", "lengths", "=", "data_train_lengths", ",", "batch_size", "=", "args", ".", "batch_size", ",", "num_buckets", "=", "args", ".", "num_buckets", ",", "ratio", "=", "args", ".", "bucket_ratio", ",", "shuffle", "=", "True", ",", "use_average_length", "=", "use_average_length", ",", "num_shards", "=", "num_shards", ",", "bucket_scheme", "=", "bucket_scheme", ")", "logging", ".", "info", "(", "'Train Batch Sampler:\\n%s'", ",", "train_batch_sampler", ".", "stats", "(", ")", ")", "train_data_loader", "=", "nlp", ".", "data", ".", "ShardedDataLoader", "(", "data_train", ",", "batch_sampler", "=", "train_batch_sampler", ",", "batchify_fn", "=", "train_batchify_fn", ",", "num_workers", "=", "num_workers", ")", "val_batch_sampler", "=", "nlp", ".", "data", ".", "FixedBucketSampler", "(", "lengths", "=", "target_val_lengths", ",", "batch_size", "=", "args", ".", "test_batch_size", ",", "num_buckets", "=", "args", ".", "num_buckets", ",", "ratio", "=", "args", ".", "bucket_ratio", ",", "shuffle", "=", "False", ",", "use_average_length", "=", "use_average_length", ",", "bucket_scheme", "=", "bucket_scheme", ")", "logging", ".", "info", "(", "'Valid Batch Sampler:\\n%s'", ",", "val_batch_sampler", ".", "stats", "(", ")", ")", "val_data_loader", "=", "gluon", ".", "data", ".", "DataLoader", "(", "data_val", ",", "batch_sampler", "=", "val_batch_sampler", ",", "batchify_fn", "=", "test_batchify_fn", ",", "num_workers", "=", "num_workers", ")", "test_batch_sampler", "=", "nlp", ".", "data", ".", "FixedBucketSampler", "(", "lengths", "=", "target_test_lengths", ",", "batch_size", "=", "args", ".", "test_batch_size", ",", "num_buckets", "=", "args", ".", "num_buckets", ",", "ratio", "=", "args", ".", "bucket_ratio", ",", "shuffle", "=", "False", ",", "use_average_length", "=", "use_average_length", ",", "bucket_scheme", "=", "bucket_scheme", ")", "logging", ".", "info", "(", "'Test Batch Sampler:\\n%s'", ",", "test_batch_sampler", ".", "stats", "(", ")", ")", "test_data_loader", "=", "gluon", ".", "data", ".", "DataLoader", "(", "data_test", ",", "batch_sampler", "=", "test_batch_sampler", ",", "batchify_fn", "=", "test_batchify_fn", ",", "num_workers", "=", "num_workers", ")", "return", "train_data_loader", ",", "val_data_loader", ",", "test_data_loader"], "original_string": "def make_dataloader(data_train, data_val, data_test, args,\n                    use_average_length=False, num_shards=0, num_workers=8):\n    \"\"\"Create data loaders for training/validation/test.\"\"\"\n    data_train_lengths = get_data_lengths(data_train)\n    data_val_lengths = get_data_lengths(data_val)\n    data_test_lengths = get_data_lengths(data_test)\n    train_batchify_fn = btf.Tuple(btf.Pad(), btf.Pad(),\n                                  btf.Stack(dtype='float32'), btf.Stack(dtype='float32'))\n    test_batchify_fn = btf.Tuple(btf.Pad(), btf.Pad(),\n                                 btf.Stack(dtype='float32'), btf.Stack(dtype='float32'),\n                                 btf.Stack())\n    target_val_lengths = list(map(lambda x: x[-1], data_val_lengths))\n    target_test_lengths = list(map(lambda x: x[-1], data_test_lengths))\n    if args.bucket_scheme == 'constant':\n        bucket_scheme = nlp.data.ConstWidthBucket()\n    elif args.bucket_scheme == 'linear':\n        bucket_scheme = nlp.data.LinearWidthBucket()\n    elif args.bucket_scheme == 'exp':\n        bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n    else:\n        raise NotImplementedError\n    train_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_train_lengths,\n                                                      batch_size=args.batch_size,\n                                                      num_buckets=args.num_buckets,\n                                                      ratio=args.bucket_ratio,\n                                                      shuffle=True,\n                                                      use_average_length=use_average_length,\n                                                      num_shards=num_shards,\n                                                      bucket_scheme=bucket_scheme)\n    logging.info('Train Batch Sampler:\\n%s', train_batch_sampler.stats())\n    train_data_loader = nlp.data.ShardedDataLoader(data_train,\n                                                   batch_sampler=train_batch_sampler,\n                                                   batchify_fn=train_batchify_fn,\n                                                   num_workers=num_workers)\n\n    val_batch_sampler = nlp.data.FixedBucketSampler(lengths=target_val_lengths,\n                                                    batch_size=args.test_batch_size,\n                                                    num_buckets=args.num_buckets,\n                                                    ratio=args.bucket_ratio,\n                                                    shuffle=False,\n                                                    use_average_length=use_average_length,\n                                                    bucket_scheme=bucket_scheme)\n    logging.info('Valid Batch Sampler:\\n%s', val_batch_sampler.stats())\n    val_data_loader = gluon.data.DataLoader(data_val,\n                                            batch_sampler=val_batch_sampler,\n                                            batchify_fn=test_batchify_fn,\n                                            num_workers=num_workers)\n    test_batch_sampler = nlp.data.FixedBucketSampler(lengths=target_test_lengths,\n                                                     batch_size=args.test_batch_size,\n                                                     num_buckets=args.num_buckets,\n                                                     ratio=args.bucket_ratio,\n                                                     shuffle=False,\n                                                     use_average_length=use_average_length,\n                                                     bucket_scheme=bucket_scheme)\n    logging.info('Test Batch Sampler:\\n%s', test_batch_sampler.stats())\n    test_data_loader = gluon.data.DataLoader(data_test,\n                                             batch_sampler=test_batch_sampler,\n                                             batchify_fn=test_batchify_fn,\n                                             num_workers=num_workers)\n    return train_data_loader, val_data_loader, test_data_loader"}, {"code": "def check(set=None, entry=None, family='ipv4'):\n    '''\n    Check that an entry exists in the specified set.\n\n    set\n        The ipset name\n\n    entry\n        An entry in the ipset.  This parameter can be a single IP address, a\n        range of IP addresses, or a subnet block.  Example:\n\n        .. code-block:: cfg\n\n            192.168.0.1\n            192.168.0.2-192.168.0.19\n            192.168.0.0/25\n\n    family\n        IP protocol version: ipv4 or ipv6\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ipset.check setname '192.168.0.1 comment \"Hello\"'\n\n    '''\n    if not set:\n        return 'Error: Set needs to be specified'\n    if not entry:\n        return 'Error: Entry needs to be specified'\n\n    settype = _find_set_type(set)\n    if not settype:\n        return 'Error: Set {0} does not exist'.format(set)\n\n    current_members = _parse_members(settype, _find_set_members(set))\n\n    if not current_members:\n        return False\n\n    if isinstance(entry, list):\n        entries = _parse_members(settype, entry)\n    else:\n        entries = [_parse_member(settype, entry)]\n\n    for current_member in current_members:\n        for entry in entries:\n            if _member_contains(current_member, entry):\n                return True\n\n    return False", "code_tokens": ["def", "check", "(", "set", "=", "None", ",", "entry", "=", "None", ",", "family", "=", "'ipv4'", ")", ":", "if", "not", "set", ":", "return", "'Error: Set needs to be specified'", "if", "not", "entry", ":", "return", "'Error: Entry needs to be specified'", "settype", "=", "_find_set_type", "(", "set", ")", "if", "not", "settype", ":", "return", "'Error: Set {0} does not exist'", ".", "format", "(", "set", ")", "current_members", "=", "_parse_members", "(", "settype", ",", "_find_set_members", "(", "set", ")", ")", "if", "not", "current_members", ":", "return", "False", "if", "isinstance", "(", "entry", ",", "list", ")", ":", "entries", "=", "_parse_members", "(", "settype", ",", "entry", ")", "else", ":", "entries", "=", "[", "_parse_member", "(", "settype", ",", "entry", ")", "]", "for", "current_member", "in", "current_members", ":", "for", "entry", "in", "entries", ":", "if", "_member_contains", "(", "current_member", ",", "entry", ")", ":", "return", "True", "return", "False"], "original_string": "def check(set=None, entry=None, family='ipv4'):\n    '''\n    Check that an entry exists in the specified set.\n\n    set\n        The ipset name\n\n    entry\n        An entry in the ipset.  This parameter can be a single IP address, a\n        range of IP addresses, or a subnet block.  Example:\n\n        .. code-block:: cfg\n\n            192.168.0.1\n            192.168.0.2-192.168.0.19\n            192.168.0.0/25\n\n    family\n        IP protocol version: ipv4 or ipv6\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ipset.check setname '192.168.0.1 comment \"Hello\"'\n\n    '''\n    if not set:\n        return 'Error: Set needs to be specified'\n    if not entry:\n        return 'Error: Entry needs to be specified'\n\n    settype = _find_set_type(set)\n    if not settype:\n        return 'Error: Set {0} does not exist'.format(set)\n\n    current_members = _parse_members(settype, _find_set_members(set))\n\n    if not current_members:\n        return False\n\n    if isinstance(entry, list):\n        entries = _parse_members(settype, entry)\n    else:\n        entries = [_parse_member(settype, entry)]\n\n    for current_member in current_members:\n        for entry in entries:\n            if _member_contains(current_member, entry):\n                return True\n\n    return False"}, {"code": "def shape4d(a, data_format='NHWC'):\n    \"\"\"\n    Ensuer a 4D shape, to use with 4D symbolic functions.\n\n    Args:\n        a: a int or tuple/list of length 2\n\n    Returns:\n        list: of length 4. if ``a`` is a int, return ``[1, a, a, 1]``\n            or ``[1, 1, a, a]`` depending on data_format.\n    \"\"\"\n    s2d = shape2d(a)\n    if get_data_format(data_format, False) == 'NHWC':\n        return [1] + s2d + [1]\n    else:\n        return [1, 1] + s2d", "code_tokens": ["def", "shape4d", "(", "a", ",", "data_format", "=", "'NHWC'", ")", ":", "s2d", "=", "shape2d", "(", "a", ")", "if", "get_data_format", "(", "data_format", ",", "False", ")", "==", "'NHWC'", ":", "return", "[", "1", "]", "+", "s2d", "+", "[", "1", "]", "else", ":", "return", "[", "1", ",", "1", "]", "+", "s2d"], "original_string": "def shape4d(a, data_format='NHWC'):\n    \"\"\"\n    Ensuer a 4D shape, to use with 4D symbolic functions.\n\n    Args:\n        a: a int or tuple/list of length 2\n\n    Returns:\n        list: of length 4. if ``a`` is a int, return ``[1, a, a, 1]``\n            or ``[1, 1, a, a]`` depending on data_format.\n    \"\"\"\n    s2d = shape2d(a)\n    if get_data_format(data_format, False) == 'NHWC':\n        return [1] + s2d + [1]\n    else:\n        return [1, 1] + s2d"}, {"code": "def loadACatalog(filename):\n    \"\"\"Load the catalog and build the associated data structures.\n      This can be either an XML Catalog or an SGML Catalog It\n      will recurse in SGML CATALOG entries. On the other hand XML\n       Catalogs are not handled recursively. \"\"\"\n    ret = libxml2mod.xmlLoadACatalog(filename)\n    if ret is None:raise treeError('xmlLoadACatalog() failed')\n    return catalog(_obj=ret)", "code_tokens": ["def", "loadACatalog", "(", "filename", ")", ":", "ret", "=", "libxml2mod", ".", "xmlLoadACatalog", "(", "filename", ")", "if", "ret", "is", "None", ":", "raise", "treeError", "(", "'xmlLoadACatalog() failed'", ")", "return", "catalog", "(", "_obj", "=", "ret", ")"], "original_string": "def loadACatalog(filename):\n    \"\"\"Load the catalog and build the associated data structures.\n      This can be either an XML Catalog or an SGML Catalog It\n      will recurse in SGML CATALOG entries. On the other hand XML\n       Catalogs are not handled recursively. \"\"\"\n    ret = libxml2mod.xmlLoadACatalog(filename)\n    if ret is None:raise treeError('xmlLoadACatalog() failed')\n    return catalog(_obj=ret)"}, {"code": "def transform(self, fn, lazy=True):\n        \"\"\"Returns a new dataset with each sample transformed by the\n        transformer function `fn`.\n\n        Parameters\n        ----------\n        fn : callable\n            A transformer function that takes a sample as input and\n            returns the transformed sample.\n        lazy : bool, default True\n            If False, transforms all samples at once. Otherwise,\n            transforms each sample on demand. Note that if `fn`\n            is stochastic, you must set lazy to True or you will\n            get the same result on all epochs.\n\n        Returns\n        -------\n        Dataset\n            The transformed dataset.\n        \"\"\"\n        trans = _LazyTransformDataset(self, fn)\n        if lazy:\n            return trans\n        return SimpleDataset([i for i in trans])", "code_tokens": ["def", "transform", "(", "self", ",", "fn", ",", "lazy", "=", "True", ")", ":", "trans", "=", "_LazyTransformDataset", "(", "self", ",", "fn", ")", "if", "lazy", ":", "return", "trans", "return", "SimpleDataset", "(", "[", "i", "for", "i", "in", "trans", "]", ")"], "original_string": "def transform(self, fn, lazy=True):\n        \"\"\"Returns a new dataset with each sample transformed by the\n        transformer function `fn`.\n\n        Parameters\n        ----------\n        fn : callable\n            A transformer function that takes a sample as input and\n            returns the transformed sample.\n        lazy : bool, default True\n            If False, transforms all samples at once. Otherwise,\n            transforms each sample on demand. Note that if `fn`\n            is stochastic, you must set lazy to True or you will\n            get the same result on all epochs.\n\n        Returns\n        -------\n        Dataset\n            The transformed dataset.\n        \"\"\"\n        trans = _LazyTransformDataset(self, fn)\n        if lazy:\n            return trans\n        return SimpleDataset([i for i in trans])"}, {"code": "def compile_format(self, log_format: str) -> Tuple[str, List[KeyMethod]]:\n        \"\"\"Translate log_format into form usable by modulo formatting\n\n        All known atoms will be replaced with %s\n        Also methods for formatting of those atoms will be added to\n        _methods in appropriate order\n\n        For example we have log_format = \"%a %t\"\n        This format will be translated to \"%s %s\"\n        Also contents of _methods will be\n        [self._format_a, self._format_t]\n        These method will be called and results will be passed\n        to translated string format.\n\n        Each _format_* method receive 'args' which is list of arguments\n        given to self.log\n\n        Exceptions are _format_e, _format_i and _format_o methods which\n        also receive key name (by functools.partial)\n\n        \"\"\"\n        # list of (key, method) tuples, we don't use an OrderedDict as users\n        # can repeat the same key more than once\n        methods = list()\n\n        for atom in self.FORMAT_RE.findall(log_format):\n            if atom[1] == '':\n                format_key1 = self.LOG_FORMAT_MAP[atom[0]]\n                m = getattr(AccessLogger, '_format_%s' % atom[0])\n                key_method = KeyMethod(format_key1, m)\n            else:\n                format_key2 = (self.LOG_FORMAT_MAP[atom[2]], atom[1])\n                m = getattr(AccessLogger, '_format_%s' % atom[2])\n                key_method = KeyMethod(format_key2,\n                                       functools.partial(m, atom[1]))\n\n            methods.append(key_method)\n\n        log_format = self.FORMAT_RE.sub(r'%s', log_format)\n        log_format = self.CLEANUP_RE.sub(r'%\\1', log_format)\n        return log_format, methods", "code_tokens": ["def", "compile_format", "(", "self", ",", "log_format", ":", "str", ")", "->", "Tuple", "[", "str", ",", "List", "[", "KeyMethod", "]", "]", ":", "# list of (key, method) tuples, we don't use an OrderedDict as users", "# can repeat the same key more than once", "methods", "=", "list", "(", ")", "for", "atom", "in", "self", ".", "FORMAT_RE", ".", "findall", "(", "log_format", ")", ":", "if", "atom", "[", "1", "]", "==", "''", ":", "format_key1", "=", "self", ".", "LOG_FORMAT_MAP", "[", "atom", "[", "0", "]", "]", "m", "=", "getattr", "(", "AccessLogger", ",", "'_format_%s'", "%", "atom", "[", "0", "]", ")", "key_method", "=", "KeyMethod", "(", "format_key1", ",", "m", ")", "else", ":", "format_key2", "=", "(", "self", ".", "LOG_FORMAT_MAP", "[", "atom", "[", "2", "]", "]", ",", "atom", "[", "1", "]", ")", "m", "=", "getattr", "(", "AccessLogger", ",", "'_format_%s'", "%", "atom", "[", "2", "]", ")", "key_method", "=", "KeyMethod", "(", "format_key2", ",", "functools", ".", "partial", "(", "m", ",", "atom", "[", "1", "]", ")", ")", "methods", ".", "append", "(", "key_method", ")", "log_format", "=", "self", ".", "FORMAT_RE", ".", "sub", "(", "r'%s'", ",", "log_format", ")", "log_format", "=", "self", ".", "CLEANUP_RE", ".", "sub", "(", "r'%\\1'", ",", "log_format", ")", "return", "log_format", ",", "methods"], "original_string": "def compile_format(self, log_format: str) -> Tuple[str, List[KeyMethod]]:\n        \"\"\"Translate log_format into form usable by modulo formatting\n\n        All known atoms will be replaced with %s\n        Also methods for formatting of those atoms will be added to\n        _methods in appropriate order\n\n        For example we have log_format = \"%a %t\"\n        This format will be translated to \"%s %s\"\n        Also contents of _methods will be\n        [self._format_a, self._format_t]\n        These method will be called and results will be passed\n        to translated string format.\n\n        Each _format_* method receive 'args' which is list of arguments\n        given to self.log\n\n        Exceptions are _format_e, _format_i and _format_o methods which\n        also receive key name (by functools.partial)\n\n        \"\"\"\n        # list of (key, method) tuples, we don't use an OrderedDict as users\n        # can repeat the same key more than once\n        methods = list()\n\n        for atom in self.FORMAT_RE.findall(log_format):\n            if atom[1] == '':\n                format_key1 = self.LOG_FORMAT_MAP[atom[0]]\n                m = getattr(AccessLogger, '_format_%s' % atom[0])\n                key_method = KeyMethod(format_key1, m)\n            else:\n                format_key2 = (self.LOG_FORMAT_MAP[atom[2]], atom[1])\n                m = getattr(AccessLogger, '_format_%s' % atom[2])\n                key_method = KeyMethod(format_key2,\n                                       functools.partial(m, atom[1]))\n\n            methods.append(key_method)\n\n        log_format = self.FORMAT_RE.sub(r'%s', log_format)\n        log_format = self.CLEANUP_RE.sub(r'%\\1', log_format)\n        return log_format, methods"}, {"code": "def _find_set_members(set):\n    '''\n    Return list of members for a set\n    '''\n\n    cmd = '{0} list {1}'.format(_ipset_cmd(), set)\n    out = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    if out['retcode'] > 0:\n        # Set doesn't exist return false\n        return False\n\n    _tmp = out['stdout'].split('\\n')\n    members = []\n    startMembers = False\n    for i in _tmp:\n        if startMembers:\n            members.append(i)\n        if 'Members:' in i:\n            startMembers = True\n    return members", "code_tokens": ["def", "_find_set_members", "(", "set", ")", ":", "cmd", "=", "'{0} list {1}'", ".", "format", "(", "_ipset_cmd", "(", ")", ",", "set", ")", "out", "=", "__salt__", "[", "'cmd.run_all'", "]", "(", "cmd", ",", "python_shell", "=", "False", ")", "if", "out", "[", "'retcode'", "]", ">", "0", ":", "# Set doesn't exist return false", "return", "False", "_tmp", "=", "out", "[", "'stdout'", "]", ".", "split", "(", "'\\n'", ")", "members", "=", "[", "]", "startMembers", "=", "False", "for", "i", "in", "_tmp", ":", "if", "startMembers", ":", "members", ".", "append", "(", "i", ")", "if", "'Members:'", "in", "i", ":", "startMembers", "=", "True", "return", "members"], "original_string": "def _find_set_members(set):\n    '''\n    Return list of members for a set\n    '''\n\n    cmd = '{0} list {1}'.format(_ipset_cmd(), set)\n    out = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    if out['retcode'] > 0:\n        # Set doesn't exist return false\n        return False\n\n    _tmp = out['stdout'].split('\\n')\n    members = []\n    startMembers = False\n    for i in _tmp:\n        if startMembers:\n            members.append(i)\n        if 'Members:' in i:\n            startMembers = True\n    return members"}, {"code": "def fill_price_worse_than_limit_price(fill_price, order):\n    \"\"\"\n    Checks whether the fill price is worse than the order's limit price.\n\n    Parameters\n    ----------\n    fill_price: float\n        The price to check.\n\n    order: zipline.finance.order.Order\n        The order whose limit price to check.\n\n    Returns\n    -------\n    bool: Whether the fill price is above the limit price (for a buy) or below\n    the limit price (for a sell).\n    \"\"\"\n    if order.limit:\n        # this is tricky! if an order with a limit price has reached\n        # the limit price, we will try to fill the order. do not fill\n        # these shares if the impacted price is worse than the limit\n        # price. return early to avoid creating the transaction.\n\n        # buy order is worse if the impacted price is greater than\n        # the limit price. sell order is worse if the impacted price\n        # is less than the limit price\n        if (order.direction > 0 and fill_price > order.limit) or \\\n                (order.direction < 0 and fill_price < order.limit):\n            return True\n\n    return False", "code_tokens": ["def", "fill_price_worse_than_limit_price", "(", "fill_price", ",", "order", ")", ":", "if", "order", ".", "limit", ":", "# this is tricky! if an order with a limit price has reached", "# the limit price, we will try to fill the order. do not fill", "# these shares if the impacted price is worse than the limit", "# price. return early to avoid creating the transaction.", "# buy order is worse if the impacted price is greater than", "# the limit price. sell order is worse if the impacted price", "# is less than the limit price", "if", "(", "order", ".", "direction", ">", "0", "and", "fill_price", ">", "order", ".", "limit", ")", "or", "(", "order", ".", "direction", "<", "0", "and", "fill_price", "<", "order", ".", "limit", ")", ":", "return", "True", "return", "False"], "original_string": "def fill_price_worse_than_limit_price(fill_price, order):\n    \"\"\"\n    Checks whether the fill price is worse than the order's limit price.\n\n    Parameters\n    ----------\n    fill_price: float\n        The price to check.\n\n    order: zipline.finance.order.Order\n        The order whose limit price to check.\n\n    Returns\n    -------\n    bool: Whether the fill price is above the limit price (for a buy) or below\n    the limit price (for a sell).\n    \"\"\"\n    if order.limit:\n        # this is tricky! if an order with a limit price has reached\n        # the limit price, we will try to fill the order. do not fill\n        # these shares if the impacted price is worse than the limit\n        # price. return early to avoid creating the transaction.\n\n        # buy order is worse if the impacted price is greater than\n        # the limit price. sell order is worse if the impacted price\n        # is less than the limit price\n        if (order.direction > 0 and fill_price > order.limit) or \\\n                (order.direction < 0 and fill_price < order.limit):\n            return True\n\n    return False"}, {"code": "def _maybe_wrap_exception(exception):\n    \"\"\"Wraps a gRPC exception class, if needed.\"\"\"\n    if isinstance(exception, grpc.RpcError):\n        return exceptions.from_grpc_error(exception)\n    return exception", "code_tokens": ["def", "_maybe_wrap_exception", "(", "exception", ")", ":", "if", "isinstance", "(", "exception", ",", "grpc", ".", "RpcError", ")", ":", "return", "exceptions", ".", "from_grpc_error", "(", "exception", ")", "return", "exception"], "original_string": "def _maybe_wrap_exception(exception):\n    \"\"\"Wraps a gRPC exception class, if needed.\"\"\"\n    if isinstance(exception, grpc.RpcError):\n        return exceptions.from_grpc_error(exception)\n    return exception"}, {"code": "def close(self):\n        \"\"\"\n        Close the pickle file, and the zip archive file. The single zip archive\n        file can now be shipped around to be loaded by the unpickler.\n        \"\"\"\n        if self.file is None:\n            return\n\n        # Close the pickle file.\n        self.file.close()\n        self.file = None\n\n        for f in self.mark_for_delete:\n            error = [False]\n\n            def register_error(*args):\n                error[0] = True\n\n            _shutil.rmtree(f, onerror = register_error)\n\n            if error[0]:\n                _atexit.register(_shutil.rmtree, f, ignore_errors=True)", "code_tokens": ["def", "close", "(", "self", ")", ":", "if", "self", ".", "file", "is", "None", ":", "return", "# Close the pickle file.", "self", ".", "file", ".", "close", "(", ")", "self", ".", "file", "=", "None", "for", "f", "in", "self", ".", "mark_for_delete", ":", "error", "=", "[", "False", "]", "def", "register_error", "(", "*", "args", ")", ":", "error", "[", "0", "]", "=", "True", "_shutil", ".", "rmtree", "(", "f", ",", "onerror", "=", "register_error", ")", "if", "error", "[", "0", "]", ":", "_atexit", ".", "register", "(", "_shutil", ".", "rmtree", ",", "f", ",", "ignore_errors", "=", "True", ")"], "original_string": "def close(self):\n        \"\"\"\n        Close the pickle file, and the zip archive file. The single zip archive\n        file can now be shipped around to be loaded by the unpickler.\n        \"\"\"\n        if self.file is None:\n            return\n\n        # Close the pickle file.\n        self.file.close()\n        self.file = None\n\n        for f in self.mark_for_delete:\n            error = [False]\n\n            def register_error(*args):\n                error[0] = True\n\n            _shutil.rmtree(f, onerror = register_error)\n\n            if error[0]:\n                _atexit.register(_shutil.rmtree, f, ignore_errors=True)"}, {"code": "def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        This method overrides ``Model.decode``, which gets called after ``Model.forward``, at test\n        time, to finalize predictions.  This is (confusingly) a separate notion from the \"decoder\"\n        in \"encoder/decoder\", where that decoder logic lives in the ``TransitionFunction``.\n\n        This method trims the output predictions to the first end symbol, replaces indices with\n        corresponding tokens, and adds a field called ``predicted_tokens`` to the ``output_dict``.\n        \"\"\"\n        action_mapping = output_dict['action_mapping']\n        best_actions = output_dict[\"best_action_sequence\"]\n        debug_infos = output_dict['debug_info']\n        batch_action_info = []\n        for batch_index, (predicted_actions, debug_info) in enumerate(zip(best_actions, debug_infos)):\n            instance_action_info = []\n            for predicted_action, action_debug_info in zip(predicted_actions, debug_info):\n                action_info = {}\n                action_info['predicted_action'] = predicted_action\n                considered_actions = action_debug_info['considered_actions']\n                probabilities = action_debug_info['probabilities']\n                actions = []\n                for action, probability in zip(considered_actions, probabilities):\n                    if action != -1:\n                        actions.append((action_mapping[(batch_index, action)], probability))\n                actions.sort()\n                considered_actions, probabilities = zip(*actions)\n                action_info['considered_actions'] = considered_actions\n                action_info['action_probabilities'] = probabilities\n                action_info['question_attention'] = action_debug_info.get('question_attention', [])\n                instance_action_info.append(action_info)\n            batch_action_info.append(instance_action_info)\n        output_dict[\"predicted_actions\"] = batch_action_info\n        return output_dict", "code_tokens": ["def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "action_mapping", "=", "output_dict", "[", "'action_mapping'", "]", "best_actions", "=", "output_dict", "[", "\"best_action_sequence\"", "]", "debug_infos", "=", "output_dict", "[", "'debug_info'", "]", "batch_action_info", "=", "[", "]", "for", "batch_index", ",", "(", "predicted_actions", ",", "debug_info", ")", "in", "enumerate", "(", "zip", "(", "best_actions", ",", "debug_infos", ")", ")", ":", "instance_action_info", "=", "[", "]", "for", "predicted_action", ",", "action_debug_info", "in", "zip", "(", "predicted_actions", ",", "debug_info", ")", ":", "action_info", "=", "{", "}", "action_info", "[", "'predicted_action'", "]", "=", "predicted_action", "considered_actions", "=", "action_debug_info", "[", "'considered_actions'", "]", "probabilities", "=", "action_debug_info", "[", "'probabilities'", "]", "actions", "=", "[", "]", "for", "action", ",", "probability", "in", "zip", "(", "considered_actions", ",", "probabilities", ")", ":", "if", "action", "!=", "-", "1", ":", "actions", ".", "append", "(", "(", "action_mapping", "[", "(", "batch_index", ",", "action", ")", "]", ",", "probability", ")", ")", "actions", ".", "sort", "(", ")", "considered_actions", ",", "probabilities", "=", "zip", "(", "*", "actions", ")", "action_info", "[", "'considered_actions'", "]", "=", "considered_actions", "action_info", "[", "'action_probabilities'", "]", "=", "probabilities", "action_info", "[", "'question_attention'", "]", "=", "action_debug_info", ".", "get", "(", "'question_attention'", ",", "[", "]", ")", "instance_action_info", ".", "append", "(", "action_info", ")", "batch_action_info", ".", "append", "(", "instance_action_info", ")", "output_dict", "[", "\"predicted_actions\"", "]", "=", "batch_action_info", "return", "output_dict"], "original_string": "def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        This method overrides ``Model.decode``, which gets called after ``Model.forward``, at test\n        time, to finalize predictions.  This is (confusingly) a separate notion from the \"decoder\"\n        in \"encoder/decoder\", where that decoder logic lives in the ``TransitionFunction``.\n\n        This method trims the output predictions to the first end symbol, replaces indices with\n        corresponding tokens, and adds a field called ``predicted_tokens`` to the ``output_dict``.\n        \"\"\"\n        action_mapping = output_dict['action_mapping']\n        best_actions = output_dict[\"best_action_sequence\"]\n        debug_infos = output_dict['debug_info']\n        batch_action_info = []\n        for batch_index, (predicted_actions, debug_info) in enumerate(zip(best_actions, debug_infos)):\n            instance_action_info = []\n            for predicted_action, action_debug_info in zip(predicted_actions, debug_info):\n                action_info = {}\n                action_info['predicted_action'] = predicted_action\n                considered_actions = action_debug_info['considered_actions']\n                probabilities = action_debug_info['probabilities']\n                actions = []\n                for action, probability in zip(considered_actions, probabilities):\n                    if action != -1:\n                        actions.append((action_mapping[(batch_index, action)], probability))\n                actions.sort()\n                considered_actions, probabilities = zip(*actions)\n                action_info['considered_actions'] = considered_actions\n                action_info['action_probabilities'] = probabilities\n                action_info['question_attention'] = action_debug_info.get('question_attention', [])\n                instance_action_info.append(action_info)\n            batch_action_info.append(instance_action_info)\n        output_dict[\"predicted_actions\"] = batch_action_info\n        return output_dict"}, {"code": "def textMerge(self, second):\n        \"\"\"Merge two text nodes into one \"\"\"\n        if second is None: second__o = None\n        else: second__o = second._o\n        ret = libxml2mod.xmlTextMerge(self._o, second__o)\n        if ret is None:raise treeError('xmlTextMerge() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "code_tokens": ["def", "textMerge", "(", "self", ",", "second", ")", ":", "if", "second", "is", "None", ":", "second__o", "=", "None", "else", ":", "second__o", "=", "second", ".", "_o", "ret", "=", "libxml2mod", ".", "xmlTextMerge", "(", "self", ".", "_o", ",", "second__o", ")", "if", "ret", "is", "None", ":", "raise", "treeError", "(", "'xmlTextMerge() failed'", ")", "__tmp", "=", "xmlNode", "(", "_obj", "=", "ret", ")", "return", "__tmp"], "original_string": "def textMerge(self, second):\n        \"\"\"Merge two text nodes into one \"\"\"\n        if second is None: second__o = None\n        else: second__o = second._o\n        ret = libxml2mod.xmlTextMerge(self._o, second__o)\n        if ret is None:raise treeError('xmlTextMerge() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp"}, {"code": "def fileno(self):\n        \"\"\"\n        Returns an OS-level file descriptor which can be used for polling, but\n        but not for reading or writing.  This is primarily to allow Python's\n        ``select`` module to work.\n\n        The first time ``fileno`` is called on a channel, a pipe is created to\n        simulate real OS-level file descriptor (FD) behavior.  Because of this,\n        two OS-level FDs are created, which will use up FDs faster than normal.\n        (You won't notice this effect unless you have hundreds of channels\n        open at the same time.)\n\n        :return: an OS-level file descriptor (`int`)\n\n        .. warning::\n            This method causes channel reads to be slightly less efficient.\n        \"\"\"\n        self.lock.acquire()\n        try:\n            if self._pipe is not None:\n                return self._pipe.fileno()\n            # create the pipe and feed in any existing data\n            self._pipe = pipe.make_pipe()\n            p1, p2 = pipe.make_or_pipe(self._pipe)\n            self.in_buffer.set_event(p1)\n            self.in_stderr_buffer.set_event(p2)\n            return self._pipe.fileno()\n        finally:\n            self.lock.release()", "code_tokens": ["def", "fileno", "(", "self", ")", ":", "self", ".", "lock", ".", "acquire", "(", ")", "try", ":", "if", "self", ".", "_pipe", "is", "not", "None", ":", "return", "self", ".", "_pipe", ".", "fileno", "(", ")", "# create the pipe and feed in any existing data", "self", ".", "_pipe", "=", "pipe", ".", "make_pipe", "(", ")", "p1", ",", "p2", "=", "pipe", ".", "make_or_pipe", "(", "self", ".", "_pipe", ")", "self", ".", "in_buffer", ".", "set_event", "(", "p1", ")", "self", ".", "in_stderr_buffer", ".", "set_event", "(", "p2", ")", "return", "self", ".", "_pipe", ".", "fileno", "(", ")", "finally", ":", "self", ".", "lock", ".", "release", "(", ")"], "original_string": "def fileno(self):\n        \"\"\"\n        Returns an OS-level file descriptor which can be used for polling, but\n        but not for reading or writing.  This is primarily to allow Python's\n        ``select`` module to work.\n\n        The first time ``fileno`` is called on a channel, a pipe is created to\n        simulate real OS-level file descriptor (FD) behavior.  Because of this,\n        two OS-level FDs are created, which will use up FDs faster than normal.\n        (You won't notice this effect unless you have hundreds of channels\n        open at the same time.)\n\n        :return: an OS-level file descriptor (`int`)\n\n        .. warning::\n            This method causes channel reads to be slightly less efficient.\n        \"\"\"\n        self.lock.acquire()\n        try:\n            if self._pipe is not None:\n                return self._pipe.fileno()\n            # create the pipe and feed in any existing data\n            self._pipe = pipe.make_pipe()\n            p1, p2 = pipe.make_or_pipe(self._pipe)\n            self.in_buffer.set_event(p1)\n            self.in_stderr_buffer.set_event(p2)\n            return self._pipe.fileno()\n        finally:\n            self.lock.release()"}, {"code": "def list_topic_rules(topic=None, ruleDisabled=None,\n            region=None, key=None, keyid=None, profile=None):\n    '''\n    List all rules (for a given topic, if specified)\n\n    Returns list of rules\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iot.list_topic_rules\n\n    Example Return:\n\n    .. code-block:: yaml\n\n        rules:\n          - {...}\n          - {...}\n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        kwargs = {}\n        if topic is not None:\n            kwargs['topic'] = topic\n        if ruleDisabled is not None:\n            kwargs['ruleDisabled'] = ruleDisabled\n        rules = []\n        for ret in __utils__['boto3.paged_call'](conn.list_topic_rules,\n                                 marker_flag='nextToken',\n                                 marker_arg='nextToken',\n                                 **kwargs):\n            rules.extend(ret['rules'])\n        if not bool(rules):\n            log.warning('No rules found')\n        return {'rules': rules}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "code_tokens": ["def", "list_topic_rules", "(", "topic", "=", "None", ",", "ruleDisabled", "=", "None", ",", "region", "=", "None", ",", "key", "=", "None", ",", "keyid", "=", "None", ",", "profile", "=", "None", ")", ":", "try", ":", "conn", "=", "_get_conn", "(", "region", "=", "region", ",", "key", "=", "key", ",", "keyid", "=", "keyid", ",", "profile", "=", "profile", ")", "kwargs", "=", "{", "}", "if", "topic", "is", "not", "None", ":", "kwargs", "[", "'topic'", "]", "=", "topic", "if", "ruleDisabled", "is", "not", "None", ":", "kwargs", "[", "'ruleDisabled'", "]", "=", "ruleDisabled", "rules", "=", "[", "]", "for", "ret", "in", "__utils__", "[", "'boto3.paged_call'", "]", "(", "conn", ".", "list_topic_rules", ",", "marker_flag", "=", "'nextToken'", ",", "marker_arg", "=", "'nextToken'", ",", "*", "*", "kwargs", ")", ":", "rules", ".", "extend", "(", "ret", "[", "'rules'", "]", ")", "if", "not", "bool", "(", "rules", ")", ":", "log", ".", "warning", "(", "'No rules found'", ")", "return", "{", "'rules'", ":", "rules", "}", "except", "ClientError", "as", "e", ":", "return", "{", "'error'", ":", "__utils__", "[", "'boto3.get_error'", "]", "(", "e", ")", "}"], "original_string": "def list_topic_rules(topic=None, ruleDisabled=None,\n            region=None, key=None, keyid=None, profile=None):\n    '''\n    List all rules (for a given topic, if specified)\n\n    Returns list of rules\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iot.list_topic_rules\n\n    Example Return:\n\n    .. code-block:: yaml\n\n        rules:\n          - {...}\n          - {...}\n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        kwargs = {}\n        if topic is not None:\n            kwargs['topic'] = topic\n        if ruleDisabled is not None:\n            kwargs['ruleDisabled'] = ruleDisabled\n        rules = []\n        for ret in __utils__['boto3.paged_call'](conn.list_topic_rules,\n                                 marker_flag='nextToken',\n                                 marker_arg='nextToken',\n                                 **kwargs):\n            rules.extend(ret['rules'])\n        if not bool(rules):\n            log.warning('No rules found')\n        return {'rules': rules}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}"}, {"code": "def _is_installation_local(name):\n    \"\"\"Check whether the distribution is in the current Python installation.\n\n    This is used to distinguish packages seen by a virtual environment. A venv\n    may be able to see global packages, but we don't want to mess with them.\n    \"\"\"\n    loc = os.path.normcase(pkg_resources.working_set.by_key[name].location)\n    pre = os.path.normcase(sys.prefix)\n    return os.path.commonprefix([loc, pre]) == pre", "code_tokens": ["def", "_is_installation_local", "(", "name", ")", ":", "loc", "=", "os", ".", "path", ".", "normcase", "(", "pkg_resources", ".", "working_set", ".", "by_key", "[", "name", "]", ".", "location", ")", "pre", "=", "os", ".", "path", ".", "normcase", "(", "sys", ".", "prefix", ")", "return", "os", ".", "path", ".", "commonprefix", "(", "[", "loc", ",", "pre", "]", ")", "==", "pre"], "original_string": "def _is_installation_local(name):\n    \"\"\"Check whether the distribution is in the current Python installation.\n\n    This is used to distinguish packages seen by a virtual environment. A venv\n    may be able to see global packages, but we don't want to mess with them.\n    \"\"\"\n    loc = os.path.normcase(pkg_resources.working_set.by_key[name].location)\n    pre = os.path.normcase(sys.prefix)\n    return os.path.commonprefix([loc, pre]) == pre"}, {"code": "def multi_find(*patterns, **kwargs):\n    '''\n    Execute multiple search tasks.\n    This function is based on the `find` function.\n    Depending on the search items, some information might overlap.\n\n    Optional arguments:\n\n    best: ``True``\n        Return only the best match with the interfaces IP networks\n        when the saerching pattern is a valid IP Address or Network.\n\n    display: ``True``\n        Display on the screen or return structured object? Default: `True` (return on the CLI).\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        $ sudo salt-run net.multi_find Ethernet1/49 xe-0/1/2\n\n    Output Example:\n\n    .. code-block:: text\n\n        Pattern \"Ethernet1/49\" found in one of the following LLDP details\n\n            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n            |    Device    | Interface | Parent Interface | Remote Chassis ID | Remote Port Description | Remote Port ID |          Remote System Description          |   Remote System Name   |\n            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n            | edge01.oua04 |  xe-0/1/2 |       ae1        | DE:AD:BE:EF:DE:AD |       Ethernet1/49      |                | Cisco NX-OS(tm) n6000, Software (n6000-uk9) | edge07.oua04.dummy.net |\n            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n        Details for interface xe-0/1/2\n\n            -----------------------------------------------------------------------------------------------------------------------\n            |    Device    | Interface | Interface Description | IP Addresses | Enabled |  UP  |    MAC Address    | Speed [Mbps] |\n            -----------------------------------------------------------------------------------------------------------------------\n            | edge01.oua04 |  xe-0/1/2 |     ae1 sw01.oua04    |              |   True  | True | BE:EF:DE:AD:BE:EF |    10000     |\n            -----------------------------------------------------------------------------------------------------------------------\n\n        LLDP Neighbors for interface xe-0/1/2\n\n            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n            |    Device    | Interface | Parent Interface | Remote Chassis ID | Remote Port Description | Remote Port ID |          Remote System Description          |   Remote System Name   |\n            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n            | edge01.oua04 |  xe-0/1/2 |       ae1        | DE:AD:BE:EF:DE:AD |       Ethernet1/49      |                | Cisco NX-OS(tm) n6000, Software (n6000-uk9) | edge07.oua04.dummy.net |\n            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    '''\n    out = {}\n    for pattern in set(patterns):\n        search_result = find(pattern,\n                             best=kwargs.get('best', True),\n                             display=kwargs.get('display', _DEFAULT_DISPLAY))\n        out[pattern] = search_result\n    if not kwargs.get('display', _DEFAULT_DISPLAY):\n        return out", "code_tokens": ["def", "multi_find", "(", "*", "patterns", ",", "*", "*", "kwargs", ")", ":", "out", "=", "{", "}", "for", "pattern", "in", "set", "(", "patterns", ")", ":", "search_result", "=", "find", "(", "pattern", ",", "best", "=", "kwargs", ".", "get", "(", "'best'", ",", "True", ")", ",", "display", "=", "kwargs", ".", "get", "(", "'display'", ",", "_DEFAULT_DISPLAY", ")", ")", "out", "[", "pattern", "]", "=", "search_result", "if", "not", "kwargs", ".", "get", "(", "'display'", ",", "_DEFAULT_DISPLAY", ")", ":", "return", "out"], "original_string": "def multi_find(*patterns, **kwargs):\n    '''\n    Execute multiple search tasks.\n    This function is based on the `find` function.\n    Depending on the search items, some information might overlap.\n\n    Optional arguments:\n\n    best: ``True``\n        Return only the best match with the interfaces IP networks\n        when the saerching pattern is a valid IP Address or Network.\n\n    display: ``True``\n        Display on the screen or return structured object? Default: `True` (return on the CLI).\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        $ sudo salt-run net.multi_find Ethernet1/49 xe-0/1/2\n\n    Output Example:\n\n    .. code-block:: text\n\n        Pattern \"Ethernet1/49\" found in one of the following LLDP details\n\n            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n            |    Device    | Interface | Parent Interface | Remote Chassis ID | Remote Port Description | Remote Port ID |          Remote System Description          |   Remote System Name   |\n            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n            | edge01.oua04 |  xe-0/1/2 |       ae1        | DE:AD:BE:EF:DE:AD |       Ethernet1/49      |                | Cisco NX-OS(tm) n6000, Software (n6000-uk9) | edge07.oua04.dummy.net |\n            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n        Details for interface xe-0/1/2\n\n            -----------------------------------------------------------------------------------------------------------------------\n            |    Device    | Interface | Interface Description | IP Addresses | Enabled |  UP  |    MAC Address    | Speed [Mbps] |\n            -----------------------------------------------------------------------------------------------------------------------\n            | edge01.oua04 |  xe-0/1/2 |     ae1 sw01.oua04    |              |   True  | True | BE:EF:DE:AD:BE:EF |    10000     |\n            -----------------------------------------------------------------------------------------------------------------------\n\n        LLDP Neighbors for interface xe-0/1/2\n\n            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n            |    Device    | Interface | Parent Interface | Remote Chassis ID | Remote Port Description | Remote Port ID |          Remote System Description          |   Remote System Name   |\n            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n            | edge01.oua04 |  xe-0/1/2 |       ae1        | DE:AD:BE:EF:DE:AD |       Ethernet1/49      |                | Cisco NX-OS(tm) n6000, Software (n6000-uk9) | edge07.oua04.dummy.net |\n            -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    '''\n    out = {}\n    for pattern in set(patterns):\n        search_result = find(pattern,\n                             best=kwargs.get('best', True),\n                             display=kwargs.get('display', _DEFAULT_DISPLAY))\n        out[pattern] = search_result\n    if not kwargs.get('display', _DEFAULT_DISPLAY):\n        return out"}, {"code": "def cub200_iterator(data_path, batch_k, batch_size, data_shape):\n    \"\"\"Return training and testing iterator for the CUB200-2011 dataset.\"\"\"\n    return (CUB200Iter(data_path, batch_k, batch_size, data_shape, is_train=True),\n            CUB200Iter(data_path, batch_k, batch_size, data_shape, is_train=False))", "code_tokens": ["def", "cub200_iterator", "(", "data_path", ",", "batch_k", ",", "batch_size", ",", "data_shape", ")", ":", "return", "(", "CUB200Iter", "(", "data_path", ",", "batch_k", ",", "batch_size", ",", "data_shape", ",", "is_train", "=", "True", ")", ",", "CUB200Iter", "(", "data_path", ",", "batch_k", ",", "batch_size", ",", "data_shape", ",", "is_train", "=", "False", ")", ")"], "original_string": "def cub200_iterator(data_path, batch_k, batch_size, data_shape):\n    \"\"\"Return training and testing iterator for the CUB200-2011 dataset.\"\"\"\n    return (CUB200Iter(data_path, batch_k, batch_size, data_shape, is_train=True),\n            CUB200Iter(data_path, batch_k, batch_size, data_shape, is_train=False))"}, {"code": "def generate(self, x, **kwargs):\n    \"\"\"\n    Generate symbolic graph for adversarial examples and return.\n\n    :param x: The model's symbolic inputs.\n    :param kwargs: Keyword arguments. See `parse_params` for documentation.\n    \"\"\"\n    # Parse and save attack-specific parameters\n    assert self.parse_params(**kwargs)\n\n    asserts = []\n\n    # If a data range was specified, check that the input was in that range\n    if self.clip_min is not None:\n      asserts.append(utils_tf.assert_greater_equal(x,\n                                                   tf.cast(self.clip_min,\n                                                           x.dtype)))\n\n    if self.clip_max is not None:\n      asserts.append(utils_tf.assert_less_equal(x,\n                                                tf.cast(self.clip_max,\n                                                        x.dtype)))\n\n    # Initialize loop variables\n    momentum = tf.zeros_like(x)\n    adv_x = x\n\n    # Fix labels to the first model predictions for loss computation\n    y, _nb_classes = self.get_or_guess_labels(x, kwargs)\n    y = y / reduce_sum(y, 1, keepdims=True)\n    targeted = (self.y_target is not None)\n\n    def cond(i, _, __):\n      \"\"\"Iterate until number of iterations completed\"\"\"\n      return tf.less(i, self.nb_iter)\n\n    def body(i, ax, m):\n      \"\"\"Do a momentum step\"\"\"\n      logits = self.model.get_logits(ax)\n      loss = softmax_cross_entropy_with_logits(labels=y, logits=logits)\n      if targeted:\n        loss = -loss\n\n      # Define gradient of loss wrt input\n      grad, = tf.gradients(loss, ax)\n\n      # Normalize current gradient and add it to the accumulated gradient\n      red_ind = list(range(1, len(grad.get_shape())))\n      avoid_zero_div = tf.cast(1e-12, grad.dtype)\n      grad = grad / tf.maximum(\n          avoid_zero_div,\n          reduce_mean(tf.abs(grad), red_ind, keepdims=True))\n      m = self.decay_factor * m + grad\n\n      optimal_perturbation = optimize_linear(m, self.eps_iter, self.ord)\n      if self.ord == 1:\n        raise NotImplementedError(\"This attack hasn't been tested for ord=1.\"\n                                  \"It's not clear that FGM makes a good inner \"\n                                  \"loop step for iterative optimization since \"\n                                  \"it updates just one coordinate at a time.\")\n\n      # Update and clip adversarial example in current iteration\n      ax = ax + optimal_perturbation\n      ax = x + utils_tf.clip_eta(ax - x, self.ord, self.eps)\n\n      if self.clip_min is not None and self.clip_max is not None:\n        ax = utils_tf.clip_by_value(ax, self.clip_min, self.clip_max)\n\n      ax = tf.stop_gradient(ax)\n\n      return i + 1, ax, m\n\n    _, adv_x, _ = tf.while_loop(\n        cond, body, (tf.zeros([]), adv_x, momentum), back_prop=True,\n        maximum_iterations=self.nb_iter)\n\n    if self.sanity_checks:\n      with tf.control_dependencies(asserts):\n        adv_x = tf.identity(adv_x)\n\n    return adv_x", "code_tokens": ["def", "generate", "(", "self", ",", "x", ",", "*", "*", "kwargs", ")", ":", "# Parse and save attack-specific parameters", "assert", "self", ".", "parse_params", "(", "*", "*", "kwargs", ")", "asserts", "=", "[", "]", "# If a data range was specified, check that the input was in that range", "if", "self", ".", "clip_min", "is", "not", "None", ":", "asserts", ".", "append", "(", "utils_tf", ".", "assert_greater_equal", "(", "x", ",", "tf", ".", "cast", "(", "self", ".", "clip_min", ",", "x", ".", "dtype", ")", ")", ")", "if", "self", ".", "clip_max", "is", "not", "None", ":", "asserts", ".", "append", "(", "utils_tf", ".", "assert_less_equal", "(", "x", ",", "tf", ".", "cast", "(", "self", ".", "clip_max", ",", "x", ".", "dtype", ")", ")", ")", "# Initialize loop variables", "momentum", "=", "tf", ".", "zeros_like", "(", "x", ")", "adv_x", "=", "x", "# Fix labels to the first model predictions for loss computation", "y", ",", "_nb_classes", "=", "self", ".", "get_or_guess_labels", "(", "x", ",", "kwargs", ")", "y", "=", "y", "/", "reduce_sum", "(", "y", ",", "1", ",", "keepdims", "=", "True", ")", "targeted", "=", "(", "self", ".", "y_target", "is", "not", "None", ")", "def", "cond", "(", "i", ",", "_", ",", "__", ")", ":", "\"\"\"Iterate until number of iterations completed\"\"\"", "return", "tf", ".", "less", "(", "i", ",", "self", ".", "nb_iter", ")", "def", "body", "(", "i", ",", "ax", ",", "m", ")", ":", "\"\"\"Do a momentum step\"\"\"", "logits", "=", "self", ".", "model", ".", "get_logits", "(", "ax", ")", "loss", "=", "softmax_cross_entropy_with_logits", "(", "labels", "=", "y", ",", "logits", "=", "logits", ")", "if", "targeted", ":", "loss", "=", "-", "loss", "# Define gradient of loss wrt input", "grad", ",", "=", "tf", ".", "gradients", "(", "loss", ",", "ax", ")", "# Normalize current gradient and add it to the accumulated gradient", "red_ind", "=", "list", "(", "range", "(", "1", ",", "len", "(", "grad", ".", "get_shape", "(", ")", ")", ")", ")", "avoid_zero_div", "=", "tf", ".", "cast", "(", "1e-12", ",", "grad", ".", "dtype", ")", "grad", "=", "grad", "/", "tf", ".", "maximum", "(", "avoid_zero_div", ",", "reduce_mean", "(", "tf", ".", "abs", "(", "grad", ")", ",", "red_ind", ",", "keepdims", "=", "True", ")", ")", "m", "=", "self", ".", "decay_factor", "*", "m", "+", "grad", "optimal_perturbation", "=", "optimize_linear", "(", "m", ",", "self", ".", "eps_iter", ",", "self", ".", "ord", ")", "if", "self", ".", "ord", "==", "1", ":", "raise", "NotImplementedError", "(", "\"This attack hasn't been tested for ord=1.\"", "\"It's not clear that FGM makes a good inner \"", "\"loop step for iterative optimization since \"", "\"it updates just one coordinate at a time.\"", ")", "# Update and clip adversarial example in current iteration", "ax", "=", "ax", "+", "optimal_perturbation", "ax", "=", "x", "+", "utils_tf", ".", "clip_eta", "(", "ax", "-", "x", ",", "self", ".", "ord", ",", "self", ".", "eps", ")", "if", "self", ".", "clip_min", "is", "not", "None", "and", "self", ".", "clip_max", "is", "not", "None", ":", "ax", "=", "utils_tf", ".", "clip_by_value", "(", "ax", ",", "self", ".", "clip_min", ",", "self", ".", "clip_max", ")", "ax", "=", "tf", ".", "stop_gradient", "(", "ax", ")", "return", "i", "+", "1", ",", "ax", ",", "m", "_", ",", "adv_x", ",", "_", "=", "tf", ".", "while_loop", "(", "cond", ",", "body", ",", "(", "tf", ".", "zeros", "(", "[", "]", ")", ",", "adv_x", ",", "momentum", ")", ",", "back_prop", "=", "True", ",", "maximum_iterations", "=", "self", ".", "nb_iter", ")", "if", "self", ".", "sanity_checks", ":", "with", "tf", ".", "control_dependencies", "(", "asserts", ")", ":", "adv_x", "=", "tf", ".", "identity", "(", "adv_x", ")", "return", "adv_x"], "original_string": "def generate(self, x, **kwargs):\n    \"\"\"\n    Generate symbolic graph for adversarial examples and return.\n\n    :param x: The model's symbolic inputs.\n    :param kwargs: Keyword arguments. See `parse_params` for documentation.\n    \"\"\"\n    # Parse and save attack-specific parameters\n    assert self.parse_params(**kwargs)\n\n    asserts = []\n\n    # If a data range was specified, check that the input was in that range\n    if self.clip_min is not None:\n      asserts.append(utils_tf.assert_greater_equal(x,\n                                                   tf.cast(self.clip_min,\n                                                           x.dtype)))\n\n    if self.clip_max is not None:\n      asserts.append(utils_tf.assert_less_equal(x,\n                                                tf.cast(self.clip_max,\n                                                        x.dtype)))\n\n    # Initialize loop variables\n    momentum = tf.zeros_like(x)\n    adv_x = x\n\n    # Fix labels to the first model predictions for loss computation\n    y, _nb_classes = self.get_or_guess_labels(x, kwargs)\n    y = y / reduce_sum(y, 1, keepdims=True)\n    targeted = (self.y_target is not None)\n\n    def cond(i, _, __):\n      \"\"\"Iterate until number of iterations completed\"\"\"\n      return tf.less(i, self.nb_iter)\n\n    def body(i, ax, m):\n      \"\"\"Do a momentum step\"\"\"\n      logits = self.model.get_logits(ax)\n      loss = softmax_cross_entropy_with_logits(labels=y, logits=logits)\n      if targeted:\n        loss = -loss\n\n      # Define gradient of loss wrt input\n      grad, = tf.gradients(loss, ax)\n\n      # Normalize current gradient and add it to the accumulated gradient\n      red_ind = list(range(1, len(grad.get_shape())))\n      avoid_zero_div = tf.cast(1e-12, grad.dtype)\n      grad = grad / tf.maximum(\n          avoid_zero_div,\n          reduce_mean(tf.abs(grad), red_ind, keepdims=True))\n      m = self.decay_factor * m + grad\n\n      optimal_perturbation = optimize_linear(m, self.eps_iter, self.ord)\n      if self.ord == 1:\n        raise NotImplementedError(\"This attack hasn't been tested for ord=1.\"\n                                  \"It's not clear that FGM makes a good inner \"\n                                  \"loop step for iterative optimization since \"\n                                  \"it updates just one coordinate at a time.\")\n\n      # Update and clip adversarial example in current iteration\n      ax = ax + optimal_perturbation\n      ax = x + utils_tf.clip_eta(ax - x, self.ord, self.eps)\n\n      if self.clip_min is not None and self.clip_max is not None:\n        ax = utils_tf.clip_by_value(ax, self.clip_min, self.clip_max)\n\n      ax = tf.stop_gradient(ax)\n\n      return i + 1, ax, m\n\n    _, adv_x, _ = tf.while_loop(\n        cond, body, (tf.zeros([]), adv_x, momentum), back_prop=True,\n        maximum_iterations=self.nb_iter)\n\n    if self.sanity_checks:\n      with tf.control_dependencies(asserts):\n        adv_x = tf.identity(adv_x)\n\n    return adv_x"}, {"code": "def cert(name,\n         aliases=None,\n         email=None,\n         webroot=None,\n         test_cert=False,\n         renew=None,\n         keysize=None,\n         server=None,\n         owner='root',\n         group='root',\n         mode='0640',\n         certname=None,\n         preferred_challenges=None,\n         tls_sni_01_port=None,\n         tls_sni_01_address=None,\n         http_01_port=None,\n         http_01_address=None,\n         dns_plugin=None,\n         dns_plugin_credentials=None):\n    '''\n    Obtain/renew a certificate from an ACME CA, probably Let's Encrypt.\n\n    :param name: Common Name of the certificate (DNS name of certificate)\n    :param aliases: subjectAltNames (Additional DNS names on certificate)\n    :param email: e-mail address for interaction with ACME provider\n    :param webroot: True or a full path to webroot. Otherwise use standalone mode\n    :param test_cert: Request a certificate from the Happy Hacker Fake CA (mutually exclusive with 'server')\n    :param renew: True/'force' to force a renewal, or a window of renewal before expiry in days\n    :param keysize: RSA key bits\n    :param server: API endpoint to talk to\n    :param owner: owner of the private key file\n    :param group: group of the private key file\n    :param mode: mode of the private key file\n    :param certname: Name of the certificate to save\n    :param preferred_challenges: A sorted, comma delimited list of the preferred\n                                 challenge to use during authorization with the\n                                 most preferred challenge listed first.\n    :param tls_sni_01_port: Port used during tls-sni-01 challenge. This only affects\n                            the port Certbot listens on. A conforming ACME server\n                            will still attempt to connect on port 443.\n    :param tls_sni_01_address: The address the server listens to during tls-sni-01\n                               challenge.\n    :param http_01_port: Port used in the http-01 challenge. This only affects\n                         the port Certbot listens on. A conforming ACME server\n                         will still attempt to connect on port 80.\n    :param https_01_address: The address the server listens to during http-01 challenge.\n    :param dns_plugin: Name of a DNS plugin to use (currently only 'cloudflare')\n    :param dns_plugin_credentials: Path to the credentials file if required by the specified DNS plugin\n    '''\n\n    if __opts__['test']:\n        ret = {\n            'name': name,\n            'changes': {},\n            'result': None\n        }\n        window = None\n        try:\n            window = int(renew)\n        except Exception:\n            pass\n\n        comment = 'Certificate {0} '.format(name)\n        if not __salt__['acme.has'](name):\n            comment += 'would have been obtained'\n        elif __salt__['acme.needs_renewal'](name, window):\n            comment += 'would have been renewed'\n        else:\n            comment += 'would not have been touched'\n            ret['result'] = True\n        ret['comment'] = comment\n        return ret\n\n    if not __salt__['acme.has'](name):\n        old = None\n    else:\n        old = __salt__['acme.info'](name)\n\n    res = __salt__['acme.cert'](\n        name,\n        aliases=aliases,\n        email=email,\n        webroot=webroot,\n        certname=certname,\n        test_cert=test_cert,\n        renew=renew,\n        keysize=keysize,\n        server=server,\n        owner=owner,\n        group=group,\n        mode=mode,\n        preferred_challenges=preferred_challenges,\n        tls_sni_01_port=tls_sni_01_port,\n        tls_sni_01_address=tls_sni_01_address,\n        http_01_port=http_01_port,\n        http_01_address=http_01_address,\n        dns_plugin=dns_plugin,\n        dns_plugin_credentials=dns_plugin_credentials,\n    )\n\n    ret = {\n        'name': name,\n        'result': res['result'] is not False,\n        'comment': res['comment']\n    }\n\n    if res['result'] is None:\n        ret['changes'] = {}\n    else:\n        if not __salt__['acme.has'](name):\n            new = None\n        else:\n            new = __salt__['acme.info'](name)\n\n        ret['changes'] = {\n            'old': old,\n            'new': new\n        }\n\n    return ret", "code_tokens": ["def", "cert", "(", "name", ",", "aliases", "=", "None", ",", "email", "=", "None", ",", "webroot", "=", "None", ",", "test_cert", "=", "False", ",", "renew", "=", "None", ",", "keysize", "=", "None", ",", "server", "=", "None", ",", "owner", "=", "'root'", ",", "group", "=", "'root'", ",", "mode", "=", "'0640'", ",", "certname", "=", "None", ",", "preferred_challenges", "=", "None", ",", "tls_sni_01_port", "=", "None", ",", "tls_sni_01_address", "=", "None", ",", "http_01_port", "=", "None", ",", "http_01_address", "=", "None", ",", "dns_plugin", "=", "None", ",", "dns_plugin_credentials", "=", "None", ")", ":", "if", "__opts__", "[", "'test'", "]", ":", "ret", "=", "{", "'name'", ":", "name", ",", "'changes'", ":", "{", "}", ",", "'result'", ":", "None", "}", "window", "=", "None", "try", ":", "window", "=", "int", "(", "renew", ")", "except", "Exception", ":", "pass", "comment", "=", "'Certificate {0} '", ".", "format", "(", "name", ")", "if", "not", "__salt__", "[", "'acme.has'", "]", "(", "name", ")", ":", "comment", "+=", "'would have been obtained'", "elif", "__salt__", "[", "'acme.needs_renewal'", "]", "(", "name", ",", "window", ")", ":", "comment", "+=", "'would have been renewed'", "else", ":", "comment", "+=", "'would not have been touched'", "ret", "[", "'result'", "]", "=", "True", "ret", "[", "'comment'", "]", "=", "comment", "return", "ret", "if", "not", "__salt__", "[", "'acme.has'", "]", "(", "name", ")", ":", "old", "=", "None", "else", ":", "old", "=", "__salt__", "[", "'acme.info'", "]", "(", "name", ")", "res", "=", "__salt__", "[", "'acme.cert'", "]", "(", "name", ",", "aliases", "=", "aliases", ",", "email", "=", "email", ",", "webroot", "=", "webroot", ",", "certname", "=", "certname", ",", "test_cert", "=", "test_cert", ",", "renew", "=", "renew", ",", "keysize", "=", "keysize", ",", "server", "=", "server", ",", "owner", "=", "owner", ",", "group", "=", "group", ",", "mode", "=", "mode", ",", "preferred_challenges", "=", "preferred_challenges", ",", "tls_sni_01_port", "=", "tls_sni_01_port", ",", "tls_sni_01_address", "=", "tls_sni_01_address", ",", "http_01_port", "=", "http_01_port", ",", "http_01_address", "=", "http_01_address", ",", "dns_plugin", "=", "dns_plugin", ",", "dns_plugin_credentials", "=", "dns_plugin_credentials", ",", ")", "ret", "=", "{", "'name'", ":", "name", ",", "'result'", ":", "res", "[", "'result'", "]", "is", "not", "False", ",", "'comment'", ":", "res", "[", "'comment'", "]", "}", "if", "res", "[", "'result'", "]", "is", "None", ":", "ret", "[", "'changes'", "]", "=", "{", "}", "else", ":", "if", "not", "__salt__", "[", "'acme.has'", "]", "(", "name", ")", ":", "new", "=", "None", "else", ":", "new", "=", "__salt__", "[", "'acme.info'", "]", "(", "name", ")", "ret", "[", "'changes'", "]", "=", "{", "'old'", ":", "old", ",", "'new'", ":", "new", "}", "return", "ret"], "original_string": "def cert(name,\n         aliases=None,\n         email=None,\n         webroot=None,\n         test_cert=False,\n         renew=None,\n         keysize=None,\n         server=None,\n         owner='root',\n         group='root',\n         mode='0640',\n         certname=None,\n         preferred_challenges=None,\n         tls_sni_01_port=None,\n         tls_sni_01_address=None,\n         http_01_port=None,\n         http_01_address=None,\n         dns_plugin=None,\n         dns_plugin_credentials=None):\n    '''\n    Obtain/renew a certificate from an ACME CA, probably Let's Encrypt.\n\n    :param name: Common Name of the certificate (DNS name of certificate)\n    :param aliases: subjectAltNames (Additional DNS names on certificate)\n    :param email: e-mail address for interaction with ACME provider\n    :param webroot: True or a full path to webroot. Otherwise use standalone mode\n    :param test_cert: Request a certificate from the Happy Hacker Fake CA (mutually exclusive with 'server')\n    :param renew: True/'force' to force a renewal, or a window of renewal before expiry in days\n    :param keysize: RSA key bits\n    :param server: API endpoint to talk to\n    :param owner: owner of the private key file\n    :param group: group of the private key file\n    :param mode: mode of the private key file\n    :param certname: Name of the certificate to save\n    :param preferred_challenges: A sorted, comma delimited list of the preferred\n                                 challenge to use during authorization with the\n                                 most preferred challenge listed first.\n    :param tls_sni_01_port: Port used during tls-sni-01 challenge. This only affects\n                            the port Certbot listens on. A conforming ACME server\n                            will still attempt to connect on port 443.\n    :param tls_sni_01_address: The address the server listens to during tls-sni-01\n                               challenge.\n    :param http_01_port: Port used in the http-01 challenge. This only affects\n                         the port Certbot listens on. A conforming ACME server\n                         will still attempt to connect on port 80.\n    :param https_01_address: The address the server listens to during http-01 challenge.\n    :param dns_plugin: Name of a DNS plugin to use (currently only 'cloudflare')\n    :param dns_plugin_credentials: Path to the credentials file if required by the specified DNS plugin\n    '''\n\n    if __opts__['test']:\n        ret = {\n            'name': name,\n            'changes': {},\n            'result': None\n        }\n        window = None\n        try:\n            window = int(renew)\n        except Exception:\n            pass\n\n        comment = 'Certificate {0} '.format(name)\n        if not __salt__['acme.has'](name):\n            comment += 'would have been obtained'\n        elif __salt__['acme.needs_renewal'](name, window):\n            comment += 'would have been renewed'\n        else:\n            comment += 'would not have been touched'\n            ret['result'] = True\n        ret['comment'] = comment\n        return ret\n\n    if not __salt__['acme.has'](name):\n        old = None\n    else:\n        old = __salt__['acme.info'](name)\n\n    res = __salt__['acme.cert'](\n        name,\n        aliases=aliases,\n        email=email,\n        webroot=webroot,\n        certname=certname,\n        test_cert=test_cert,\n        renew=renew,\n        keysize=keysize,\n        server=server,\n        owner=owner,\n        group=group,\n        mode=mode,\n        preferred_challenges=preferred_challenges,\n        tls_sni_01_port=tls_sni_01_port,\n        tls_sni_01_address=tls_sni_01_address,\n        http_01_port=http_01_port,\n        http_01_address=http_01_address,\n        dns_plugin=dns_plugin,\n        dns_plugin_credentials=dns_plugin_credentials,\n    )\n\n    ret = {\n        'name': name,\n        'result': res['result'] is not False,\n        'comment': res['comment']\n    }\n\n    if res['result'] is None:\n        ret['changes'] = {}\n    else:\n        if not __salt__['acme.has'](name):\n            new = None\n        else:\n            new = __salt__['acme.info'](name)\n\n        ret['changes'] = {\n            'old': old,\n            'new': new\n        }\n\n    return ret"}, {"code": "def _get_additional_options(runtime, debug_options):\n        \"\"\"\n        Return additional Docker container options. Used by container debug mode to enable certain container\n        security options.\n        :param DebugContext debug_options: DebugContext for the runtime of the container.\n        :return dict: Dictionary containing additional arguments to be passed to container creation.\n        \"\"\"\n        if not debug_options:\n            return None\n\n        opts = {}\n\n        if runtime == Runtime.go1x.value:\n            # These options are required for delve to function properly inside a docker container on docker < 1.12\n            # See https://github.com/moby/moby/issues/21051\n            opts[\"security_opt\"] = [\"seccomp:unconfined\"]\n            opts[\"cap_add\"] = [\"SYS_PTRACE\"]\n\n        return opts", "code_tokens": ["def", "_get_additional_options", "(", "runtime", ",", "debug_options", ")", ":", "if", "not", "debug_options", ":", "return", "None", "opts", "=", "{", "}", "if", "runtime", "==", "Runtime", ".", "go1x", ".", "value", ":", "# These options are required for delve to function properly inside a docker container on docker < 1.12", "# See https://github.com/moby/moby/issues/21051", "opts", "[", "\"security_opt\"", "]", "=", "[", "\"seccomp:unconfined\"", "]", "opts", "[", "\"cap_add\"", "]", "=", "[", "\"SYS_PTRACE\"", "]", "return", "opts"], "original_string": "def _get_additional_options(runtime, debug_options):\n        \"\"\"\n        Return additional Docker container options. Used by container debug mode to enable certain container\n        security options.\n        :param DebugContext debug_options: DebugContext for the runtime of the container.\n        :return dict: Dictionary containing additional arguments to be passed to container creation.\n        \"\"\"\n        if not debug_options:\n            return None\n\n        opts = {}\n\n        if runtime == Runtime.go1x.value:\n            # These options are required for delve to function properly inside a docker container on docker < 1.12\n            # See https://github.com/moby/moby/issues/21051\n            opts[\"security_opt\"] = [\"seccomp:unconfined\"]\n            opts[\"cap_add\"] = [\"SYS_PTRACE\"]\n\n        return opts"}, {"code": "def read_file(name):\n    '''\n    output the contents of a file:\n\n    this is a workaround if the cp.push module does not work.\n    https://github.com/saltstack/salt/issues/37133\n\n    help the master output the contents of a document\n    that might be saved on the minions filesystem.\n\n    .. code-block:: python\n\n        #!/bin/python\n        import os\n        import salt.client\n        s = salt.client.LocalClient()\n        o = s.cmd('*', 'highstate_doc.read_file', ['/root/README.md'])\n        for m in o:\n            d = o.get(m)\n            if d and not d.endswith('is not available.'):\n                # mkdir m\n                #directory = os.path.dirname(file_path)\n                if not os.path.exists(m):\n                    os.makedirs(m)\n                with open(m + '/README.md','wb') as fin:\n                    fin.write(d)\n                print('ADDED: ' + m + '/README.md')\n    '''\n    out = ''\n    try:\n        with salt.utils.files.fopen(name, 'r') as f:\n            out = salt.utils.stringutils.to_unicode(f.read())\n    except Exception as ex:\n        log.error(ex)\n        return None\n    return out", "code_tokens": ["def", "read_file", "(", "name", ")", ":", "out", "=", "''", "try", ":", "with", "salt", ".", "utils", ".", "files", ".", "fopen", "(", "name", ",", "'r'", ")", "as", "f", ":", "out", "=", "salt", ".", "utils", ".", "stringutils", ".", "to_unicode", "(", "f", ".", "read", "(", ")", ")", "except", "Exception", "as", "ex", ":", "log", ".", "error", "(", "ex", ")", "return", "None", "return", "out"], "original_string": "def read_file(name):\n    '''\n    output the contents of a file:\n\n    this is a workaround if the cp.push module does not work.\n    https://github.com/saltstack/salt/issues/37133\n\n    help the master output the contents of a document\n    that might be saved on the minions filesystem.\n\n    .. code-block:: python\n\n        #!/bin/python\n        import os\n        import salt.client\n        s = salt.client.LocalClient()\n        o = s.cmd('*', 'highstate_doc.read_file', ['/root/README.md'])\n        for m in o:\n            d = o.get(m)\n            if d and not d.endswith('is not available.'):\n                # mkdir m\n                #directory = os.path.dirname(file_path)\n                if not os.path.exists(m):\n                    os.makedirs(m)\n                with open(m + '/README.md','wb') as fin:\n                    fin.write(d)\n                print('ADDED: ' + m + '/README.md')\n    '''\n    out = ''\n    try:\n        with salt.utils.files.fopen(name, 'r') as f:\n            out = salt.utils.stringutils.to_unicode(f.read())\n    except Exception as ex:\n        log.error(ex)\n        return None\n    return out"}, {"code": "def create_mutating_webhook_configuration(self, body, **kwargs):\n        \"\"\"\n        create a MutatingWebhookConfiguration\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_mutating_webhook_configuration(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param V1beta1MutatingWebhookConfiguration body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1beta1MutatingWebhookConfiguration\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_mutating_webhook_configuration_with_http_info(body, **kwargs)\n        else:\n            (data) = self.create_mutating_webhook_configuration_with_http_info(body, **kwargs)\n            return data", "code_tokens": ["def", "create_mutating_webhook_configuration", "(", "self", ",", "body", ",", "*", "*", "kwargs", ")", ":", "kwargs", "[", "'_return_http_data_only'", "]", "=", "True", "if", "kwargs", ".", "get", "(", "'async_req'", ")", ":", "return", "self", ".", "create_mutating_webhook_configuration_with_http_info", "(", "body", ",", "*", "*", "kwargs", ")", "else", ":", "(", "data", ")", "=", "self", ".", "create_mutating_webhook_configuration_with_http_info", "(", "body", ",", "*", "*", "kwargs", ")", "return", "data"], "original_string": "def create_mutating_webhook_configuration(self, body, **kwargs):\n        \"\"\"\n        create a MutatingWebhookConfiguration\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_mutating_webhook_configuration(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param V1beta1MutatingWebhookConfiguration body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1beta1MutatingWebhookConfiguration\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_mutating_webhook_configuration_with_http_info(body, **kwargs)\n        else:\n            (data) = self.create_mutating_webhook_configuration_with_http_info(body, **kwargs)\n            return data"}, {"code": "def _finish_pending_requests(self) -> None:\n        \"\"\"Process any requests that were completed by the last\n        call to multi.socket_action.\n        \"\"\"\n        while True:\n            num_q, ok_list, err_list = self._multi.info_read()\n            for curl in ok_list:\n                self._finish(curl)\n            for curl, errnum, errmsg in err_list:\n                self._finish(curl, errnum, errmsg)\n            if num_q == 0:\n                break\n        self._process_queue()", "code_tokens": ["def", "_finish_pending_requests", "(", "self", ")", "->", "None", ":", "while", "True", ":", "num_q", ",", "ok_list", ",", "err_list", "=", "self", ".", "_multi", ".", "info_read", "(", ")", "for", "curl", "in", "ok_list", ":", "self", ".", "_finish", "(", "curl", ")", "for", "curl", ",", "errnum", ",", "errmsg", "in", "err_list", ":", "self", ".", "_finish", "(", "curl", ",", "errnum", ",", "errmsg", ")", "if", "num_q", "==", "0", ":", "break", "self", ".", "_process_queue", "(", ")"], "original_string": "def _finish_pending_requests(self) -> None:\n        \"\"\"Process any requests that were completed by the last\n        call to multi.socket_action.\n        \"\"\"\n        while True:\n            num_q, ok_list, err_list = self._multi.info_read()\n            for curl in ok_list:\n                self._finish(curl)\n            for curl, errnum, errmsg in err_list:\n                self._finish(curl, errnum, errmsg)\n            if num_q == 0:\n                break\n        self._process_queue()"}, {"code": "def list_dfu_devices(*args, **kwargs):\n    \"\"\"Prints a lits of devices detected in DFU mode.\"\"\"\n    devices = get_dfu_devices(*args, **kwargs)\n    if not devices:\n        print(\"No DFU capable devices found\")\n        return\n    for device in devices:\n        print(\"Bus {} Device {:03d}: ID {:04x}:{:04x}\"\n              .format(device.bus, device.address,\n                      device.idVendor, device.idProduct))\n        layout = get_memory_layout(device)\n        print(\"Memory Layout\")\n        for entry in layout:\n            print(\"    0x{:x} {:2d} pages of {:3d}K bytes\"\n                  .format(entry['addr'], entry['num_pages'],\n                          entry['page_size'] // 1024))", "code_tokens": ["def", "list_dfu_devices", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "devices", "=", "get_dfu_devices", "(", "*", "args", ",", "*", "*", "kwargs", ")", "if", "not", "devices", ":", "print", "(", "\"No DFU capable devices found\"", ")", "return", "for", "device", "in", "devices", ":", "print", "(", "\"Bus {} Device {:03d}: ID {:04x}:{:04x}\"", ".", "format", "(", "device", ".", "bus", ",", "device", ".", "address", ",", "device", ".", "idVendor", ",", "device", ".", "idProduct", ")", ")", "layout", "=", "get_memory_layout", "(", "device", ")", "print", "(", "\"Memory Layout\"", ")", "for", "entry", "in", "layout", ":", "print", "(", "\"    0x{:x} {:2d} pages of {:3d}K bytes\"", ".", "format", "(", "entry", "[", "'addr'", "]", ",", "entry", "[", "'num_pages'", "]", ",", "entry", "[", "'page_size'", "]", "//", "1024", ")", ")"], "original_string": "def list_dfu_devices(*args, **kwargs):\n    \"\"\"Prints a lits of devices detected in DFU mode.\"\"\"\n    devices = get_dfu_devices(*args, **kwargs)\n    if not devices:\n        print(\"No DFU capable devices found\")\n        return\n    for device in devices:\n        print(\"Bus {} Device {:03d}: ID {:04x}:{:04x}\"\n              .format(device.bus, device.address,\n                      device.idVendor, device.idProduct))\n        layout = get_memory_layout(device)\n        print(\"Memory Layout\")\n        for entry in layout:\n            print(\"    0x{:x} {:2d} pages of {:3d}K bytes\"\n                  .format(entry['addr'], entry['num_pages'],\n                          entry['page_size'] // 1024))"}, {"code": "def _parse_thead_tbody_tfoot(self, table_html):\n        \"\"\"\n        Given a table, return parsed header, body, and foot.\n\n        Parameters\n        ----------\n        table_html : node-like\n\n        Returns\n        -------\n        tuple of (header, body, footer), each a list of list-of-text rows.\n\n        Notes\n        -----\n        Header and body are lists-of-lists. Top level list is a list of\n        rows. Each row is a list of str text.\n\n        Logic: Use <thead>, <tbody>, <tfoot> elements to identify\n               header, body, and footer, otherwise:\n               - Put all rows into body\n               - Move rows from top of body to header only if\n                 all elements inside row are <th>\n               - Move rows from bottom of body to footer only if\n                 all elements inside row are <th>\n        \"\"\"\n\n        header_rows = self._parse_thead_tr(table_html)\n        body_rows = self._parse_tbody_tr(table_html)\n        footer_rows = self._parse_tfoot_tr(table_html)\n\n        def row_is_all_th(row):\n            return all(self._equals_tag(t, 'th') for t in\n                       self._parse_td(row))\n\n        if not header_rows:\n            # The table has no <thead>. Move the top all-<th> rows from\n            # body_rows to header_rows. (This is a common case because many\n            # tables in the wild have no <thead> or <tfoot>\n            while body_rows and row_is_all_th(body_rows[0]):\n                header_rows.append(body_rows.pop(0))\n\n        header = self._expand_colspan_rowspan(header_rows)\n        body = self._expand_colspan_rowspan(body_rows)\n        footer = self._expand_colspan_rowspan(footer_rows)\n\n        return header, body, footer", "code_tokens": ["def", "_parse_thead_tbody_tfoot", "(", "self", ",", "table_html", ")", ":", "header_rows", "=", "self", ".", "_parse_thead_tr", "(", "table_html", ")", "body_rows", "=", "self", ".", "_parse_tbody_tr", "(", "table_html", ")", "footer_rows", "=", "self", ".", "_parse_tfoot_tr", "(", "table_html", ")", "def", "row_is_all_th", "(", "row", ")", ":", "return", "all", "(", "self", ".", "_equals_tag", "(", "t", ",", "'th'", ")", "for", "t", "in", "self", ".", "_parse_td", "(", "row", ")", ")", "if", "not", "header_rows", ":", "# The table has no <thead>. Move the top all-<th> rows from", "# body_rows to header_rows. (This is a common case because many", "# tables in the wild have no <thead> or <tfoot>", "while", "body_rows", "and", "row_is_all_th", "(", "body_rows", "[", "0", "]", ")", ":", "header_rows", ".", "append", "(", "body_rows", ".", "pop", "(", "0", ")", ")", "header", "=", "self", ".", "_expand_colspan_rowspan", "(", "header_rows", ")", "body", "=", "self", ".", "_expand_colspan_rowspan", "(", "body_rows", ")", "footer", "=", "self", ".", "_expand_colspan_rowspan", "(", "footer_rows", ")", "return", "header", ",", "body", ",", "footer"], "original_string": "def _parse_thead_tbody_tfoot(self, table_html):\n        \"\"\"\n        Given a table, return parsed header, body, and foot.\n\n        Parameters\n        ----------\n        table_html : node-like\n\n        Returns\n        -------\n        tuple of (header, body, footer), each a list of list-of-text rows.\n\n        Notes\n        -----\n        Header and body are lists-of-lists. Top level list is a list of\n        rows. Each row is a list of str text.\n\n        Logic: Use <thead>, <tbody>, <tfoot> elements to identify\n               header, body, and footer, otherwise:\n               - Put all rows into body\n               - Move rows from top of body to header only if\n                 all elements inside row are <th>\n               - Move rows from bottom of body to footer only if\n                 all elements inside row are <th>\n        \"\"\"\n\n        header_rows = self._parse_thead_tr(table_html)\n        body_rows = self._parse_tbody_tr(table_html)\n        footer_rows = self._parse_tfoot_tr(table_html)\n\n        def row_is_all_th(row):\n            return all(self._equals_tag(t, 'th') for t in\n                       self._parse_td(row))\n\n        if not header_rows:\n            # The table has no <thead>. Move the top all-<th> rows from\n            # body_rows to header_rows. (This is a common case because many\n            # tables in the wild have no <thead> or <tfoot>\n            while body_rows and row_is_all_th(body_rows[0]):\n                header_rows.append(body_rows.pop(0))\n\n        header = self._expand_colspan_rowspan(header_rows)\n        body = self._expand_colspan_rowspan(body_rows)\n        footer = self._expand_colspan_rowspan(footer_rows)\n\n        return header, body, footer"}, {"code": "def _sendto(self, data, addr=None, attempts=10):\n        '''\n        On multi-master environments, running on the same machine,\n        transport sending to the destination can be allowed only at once.\n        Since every machine will immediately respond, high chance to\n        get sending fired at the same time, which will result to a PermissionError\n        at socket level. We are attempting to send it in a different time.\n\n        :param data:\n        :param addr:\n        :return:\n        '''\n        tries = 0\n        slp_time = lambda: 0.5 / random.randint(10, 30)\n        slp = slp_time()\n        while tries < attempts:\n            try:\n                self.transport.sendto(data, addr=addr)\n                self.log.debug('Sent successfully')\n                return\n            except AttributeError as ex:\n                self.log.debug('Permission error: %s', ex)\n                time.sleep(slp)\n                tries += 1\n                slp += slp_time()", "code_tokens": ["def", "_sendto", "(", "self", ",", "data", ",", "addr", "=", "None", ",", "attempts", "=", "10", ")", ":", "tries", "=", "0", "slp_time", "=", "lambda", ":", "0.5", "/", "random", ".", "randint", "(", "10", ",", "30", ")", "slp", "=", "slp_time", "(", ")", "while", "tries", "<", "attempts", ":", "try", ":", "self", ".", "transport", ".", "sendto", "(", "data", ",", "addr", "=", "addr", ")", "self", ".", "log", ".", "debug", "(", "'Sent successfully'", ")", "return", "except", "AttributeError", "as", "ex", ":", "self", ".", "log", ".", "debug", "(", "'Permission error: %s'", ",", "ex", ")", "time", ".", "sleep", "(", "slp", ")", "tries", "+=", "1", "slp", "+=", "slp_time", "(", ")"], "original_string": "def _sendto(self, data, addr=None, attempts=10):\n        '''\n        On multi-master environments, running on the same machine,\n        transport sending to the destination can be allowed only at once.\n        Since every machine will immediately respond, high chance to\n        get sending fired at the same time, which will result to a PermissionError\n        at socket level. We are attempting to send it in a different time.\n\n        :param data:\n        :param addr:\n        :return:\n        '''\n        tries = 0\n        slp_time = lambda: 0.5 / random.randint(10, 30)\n        slp = slp_time()\n        while tries < attempts:\n            try:\n                self.transport.sendto(data, addr=addr)\n                self.log.debug('Sent successfully')\n                return\n            except AttributeError as ex:\n                self.log.debug('Permission error: %s', ex)\n                time.sleep(slp)\n                tries += 1\n                slp += slp_time()"}, {"code": "def to_pandas(self):\r\n        \"\"\"Converts Modin DataFrame to Pandas DataFrame.\r\n\r\n        Returns:\r\n            Pandas DataFrame of the DataManager.\r\n        \"\"\"\r\n        df = self.data.to_pandas(is_transposed=self._is_transposed)\r\n        if df.empty:\r\n            dtype_dict = {\r\n                col_name: pandas.Series(dtype=self.dtypes[col_name])\r\n                for col_name in self.columns\r\n            }\r\n            df = pandas.DataFrame(dtype_dict, self.index)\r\n        else:\r\n            ErrorMessage.catch_bugs_and_request_email(\r\n                len(df.index) != len(self.index) or len(df.columns) != len(self.columns)\r\n            )\r\n            df.index = self.index\r\n            df.columns = self.columns\r\n        return df", "code_tokens": ["def", "to_pandas", "(", "self", ")", ":", "df", "=", "self", ".", "data", ".", "to_pandas", "(", "is_transposed", "=", "self", ".", "_is_transposed", ")", "if", "df", ".", "empty", ":", "dtype_dict", "=", "{", "col_name", ":", "pandas", ".", "Series", "(", "dtype", "=", "self", ".", "dtypes", "[", "col_name", "]", ")", "for", "col_name", "in", "self", ".", "columns", "}", "df", "=", "pandas", ".", "DataFrame", "(", "dtype_dict", ",", "self", ".", "index", ")", "else", ":", "ErrorMessage", ".", "catch_bugs_and_request_email", "(", "len", "(", "df", ".", "index", ")", "!=", "len", "(", "self", ".", "index", ")", "or", "len", "(", "df", ".", "columns", ")", "!=", "len", "(", "self", ".", "columns", ")", ")", "df", ".", "index", "=", "self", ".", "index", "df", ".", "columns", "=", "self", ".", "columns", "return", "df"], "original_string": "def to_pandas(self):\r\n        \"\"\"Converts Modin DataFrame to Pandas DataFrame.\r\n\r\n        Returns:\r\n            Pandas DataFrame of the DataManager.\r\n        \"\"\"\r\n        df = self.data.to_pandas(is_transposed=self._is_transposed)\r\n        if df.empty:\r\n            dtype_dict = {\r\n                col_name: pandas.Series(dtype=self.dtypes[col_name])\r\n                for col_name in self.columns\r\n            }\r\n            df = pandas.DataFrame(dtype_dict, self.index)\r\n        else:\r\n            ErrorMessage.catch_bugs_and_request_email(\r\n                len(df.index) != len(self.index) or len(df.columns) != len(self.columns)\r\n            )\r\n            df.index = self.index\r\n            df.columns = self.columns\r\n        return df"}, {"code": "def setup_default_layouts(self, index, settings):\r\n        \"\"\"Setup default layouts when run for the first time.\"\"\"\r\n        self.setUpdatesEnabled(False)\r\n\r\n        first_spyder_run = bool(self.first_spyder_run)  # Store copy\r\n\r\n        if first_spyder_run:\r\n            self.set_window_settings(*settings)\r\n        else:\r\n            if self.last_plugin:\r\n                if self.last_plugin.ismaximized:\r\n                    self.maximize_dockwidget(restore=True)\r\n\r\n            if not (self.isMaximized() or self.maximized_flag):\r\n                self.showMaximized()\r\n\r\n            min_width = self.minimumWidth()\r\n            max_width = self.maximumWidth()\r\n            base_width = self.width()\r\n            self.setFixedWidth(base_width)\r\n\r\n        # IMPORTANT: order has to be the same as defined in the config file\r\n        MATLAB, RSTUDIO, VERTICAL, HORIZONTAL = range(self.DEFAULT_LAYOUTS)\r\n\r\n        # Define widgets locally\r\n        editor = self.editor\r\n        console_ipy = self.ipyconsole\r\n        console_int = self.console\r\n        outline = self.outlineexplorer\r\n        explorer_project = self.projects\r\n        explorer_file = self.explorer\r\n        explorer_variable = self.variableexplorer\r\n        plots = self.plots\r\n        history = self.historylog\r\n        finder = self.findinfiles\r\n        help_plugin = self.help\r\n        helper = self.onlinehelp\r\n        plugins = self.thirdparty_plugins\r\n\r\n        # Stored for tests\r\n        global_hidden_widgets = [finder, console_int, explorer_project,\r\n                                 helper] + plugins\r\n        global_hidden_toolbars = [self.source_toolbar, self.edit_toolbar,\r\n                                  self.search_toolbar]\r\n        # Layout definition\r\n        # --------------------------------------------------------------------\r\n        # Layouts are organized by columns, each column is organized by rows.\r\n        # Widths have to add 1.0 (except if hidden), height per column has to\r\n        # add 1.0 as well\r\n\r\n        # Spyder Default Initial Layout\r\n        s_layout = {\r\n            'widgets': [\r\n                # Column 0\r\n                [[explorer_project]],\r\n                # Column 1\r\n                [[editor]],\r\n                # Column 2\r\n                [[outline]],\r\n                # Column 3\r\n                [[help_plugin, explorer_variable, plots,     # Row 0\r\n                  helper, explorer_file, finder] + plugins,\r\n                 [console_int, console_ipy, history]]        # Row 1\r\n                ],\r\n            'width fraction': [0.05,            # Column 0 width\r\n                               0.55,            # Column 1 width\r\n                               0.05,            # Column 2 width\r\n                               0.45],           # Column 3 width\r\n            'height fraction': [[1.0],          # Column 0, row heights\r\n                                [1.0],          # Column 1, row heights\r\n                                [1.0],          # Column 2, row heights\r\n                                [0.46, 0.54]],  # Column 3, row heights\r\n            'hidden widgets': [outline] + global_hidden_widgets,\r\n            'hidden toolbars': [],\r\n        }\r\n\r\n        # RStudio\r\n        r_layout = {\r\n            'widgets': [\r\n                # column 0\r\n                [[editor],                            # Row 0\r\n                 [console_ipy, console_int]],         # Row 1\r\n                # column 1\r\n                [[explorer_variable, plots, history,  # Row 0\r\n                  outline, finder] + plugins,\r\n                 [explorer_file, explorer_project,    # Row 1\r\n                  help_plugin, helper]]\r\n                ],\r\n            'width fraction': [0.55,            # Column 0 width\r\n                               0.45],           # Column 1 width\r\n            'height fraction': [[0.55, 0.45],   # Column 0, row heights\r\n                                [0.55, 0.45]],  # Column 1, row heights\r\n            'hidden widgets': [outline] + global_hidden_widgets,\r\n            'hidden toolbars': [],\r\n        }\r\n\r\n        # Matlab\r\n        m_layout = {\r\n            'widgets': [\r\n                # column 0\r\n                [[explorer_file, explorer_project],\r\n                 [outline]],\r\n                # column 1\r\n                [[editor],\r\n                 [console_ipy, console_int]],\r\n                # column 2\r\n                [[explorer_variable, plots, finder] + plugins,\r\n                 [history, help_plugin, helper]]\r\n                ],\r\n            'width fraction': [0.10,            # Column 0 width\r\n                               0.45,            # Column 1 width\r\n                               0.45],           # Column 2 width\r\n            'height fraction': [[0.55, 0.45],   # Column 0, row heights\r\n                                [0.55, 0.45],   # Column 1, row heights\r\n                                [0.55, 0.45]],  # Column 2, row heights\r\n            'hidden widgets': global_hidden_widgets,\r\n            'hidden toolbars': [],\r\n        }\r\n\r\n        # Vertically split\r\n        v_layout = {\r\n            'widgets': [\r\n                # column 0\r\n                [[editor],                                  # Row 0\r\n                 [console_ipy, console_int, explorer_file,  # Row 1\r\n                  explorer_project, help_plugin, explorer_variable, plots,\r\n                  history, outline, finder, helper] + plugins]\r\n                ],\r\n            'width fraction': [1.0],            # Column 0 width\r\n            'height fraction': [[0.55, 0.45]],  # Column 0, row heights\r\n            'hidden widgets': [outline] + global_hidden_widgets,\r\n            'hidden toolbars': [],\r\n        }\r\n\r\n        # Horizontally split\r\n        h_layout = {\r\n            'widgets': [\r\n                # column 0\r\n                [[editor]],                                 # Row 0\r\n                # column 1\r\n                [[console_ipy, console_int, explorer_file,  # Row 0\r\n                  explorer_project, help_plugin, explorer_variable, plots,\r\n                  history, outline, finder, helper] + plugins]\r\n                ],\r\n            'width fraction': [0.55,      # Column 0 width\r\n                               0.45],     # Column 1 width\r\n            'height fraction': [[1.0],    # Column 0, row heights\r\n                                [1.0]],   # Column 1, row heights\r\n            'hidden widgets': [outline] + global_hidden_widgets,\r\n            'hidden toolbars': []\r\n        }\r\n\r\n        # Layout selection\r\n        layouts = {\r\n            'default': s_layout,\r\n            RSTUDIO: r_layout,\r\n            MATLAB: m_layout,\r\n            VERTICAL: v_layout,\r\n            HORIZONTAL: h_layout,\r\n        }\r\n\r\n        layout = layouts[index]\r\n\r\n        # Remove None from widgets layout\r\n        widgets_layout = layout['widgets']\r\n        widgets_layout_clean = []\r\n        for column in widgets_layout:\r\n            clean_col = []\r\n            for row in column:\r\n                clean_row = [w for w in row if w is not None]\r\n                if clean_row:\r\n                    clean_col.append(clean_row)\r\n            if clean_col:\r\n                widgets_layout_clean.append(clean_col)\r\n\r\n        # Flatten widgets list\r\n        widgets = []\r\n        for column in widgets_layout_clean:\r\n            for row in column:\r\n                for widget in row:\r\n                    widgets.append(widget)\r\n\r\n        # Make every widget visible\r\n        for widget in widgets:\r\n            widget.toggle_view(True)\r\n            widget.toggle_view_action.setChecked(True)\r\n\r\n        # We use both directions to ensure proper update when moving from\r\n        # 'Horizontal Split' to 'Spyder Default'\r\n        # This also seems to help on random cases where the display seems\r\n        # 'empty'\r\n        for direction in (Qt.Vertical, Qt.Horizontal):\r\n            # Arrange the widgets in one direction\r\n            for idx in range(len(widgets) - 1):\r\n                first, second = widgets[idx], widgets[idx+1]\r\n                if first is not None and second is not None:\r\n                    self.splitDockWidget(first.dockwidget, second.dockwidget,\r\n                                         direction)\r\n\r\n        # Arrange the widgets in the other direction\r\n        for column in widgets_layout_clean:\r\n            for idx in range(len(column) - 1):\r\n                first_row, second_row = column[idx], column[idx+1]\r\n                self.splitDockWidget(first_row[0].dockwidget,\r\n                                     second_row[0].dockwidget,\r\n                                     Qt.Vertical)\r\n\r\n        # Tabify\r\n        for column in widgets_layout_clean:\r\n            for row in column:\r\n                for idx in range(len(row) - 1):\r\n                    first, second = row[idx], row[idx+1]\r\n                    self.tabify_plugins(first, second)\r\n\r\n                # Raise front widget per row\r\n                row[0].dockwidget.show()\r\n                row[0].dockwidget.raise_()\r\n\r\n        # Set dockwidget widths\r\n        width_fractions = layout['width fraction']\r\n        if len(width_fractions) > 1:\r\n            _widgets = [col[0][0].dockwidget for col in widgets_layout]\r\n            self.resizeDocks(_widgets, width_fractions, Qt.Horizontal)\r\n\r\n        # Set dockwidget heights\r\n        height_fractions = layout['height fraction']\r\n        for idx, column in enumerate(widgets_layout_clean):\r\n            if len(column) > 1:\r\n                _widgets = [row[0].dockwidget for row in column]\r\n                self.resizeDocks(_widgets, height_fractions[idx], Qt.Vertical)\r\n\r\n        # Hide toolbars\r\n        hidden_toolbars = global_hidden_toolbars + layout['hidden toolbars']\r\n        for toolbar in hidden_toolbars:\r\n            if toolbar is not None:\r\n                toolbar.close()\r\n\r\n        # Hide widgets\r\n        hidden_widgets = layout['hidden widgets']\r\n        for widget in hidden_widgets:\r\n            if widget is not None:\r\n                widget.dockwidget.close()\r\n\r\n        if first_spyder_run:\r\n            self.first_spyder_run = False\r\n        else:\r\n            self.setMinimumWidth(min_width)\r\n            self.setMaximumWidth(max_width)\r\n\r\n            if not (self.isMaximized() or self.maximized_flag):\r\n                self.showMaximized()\r\n\r\n        self.setUpdatesEnabled(True)\r\n        self.sig_layout_setup_ready.emit(layout)\r\n\r\n        return layout", "code_tokens": ["def", "setup_default_layouts", "(", "self", ",", "index", ",", "settings", ")", ":", "self", ".", "setUpdatesEnabled", "(", "False", ")", "first_spyder_run", "=", "bool", "(", "self", ".", "first_spyder_run", ")", "# Store copy\r", "if", "first_spyder_run", ":", "self", ".", "set_window_settings", "(", "*", "settings", ")", "else", ":", "if", "self", ".", "last_plugin", ":", "if", "self", ".", "last_plugin", ".", "ismaximized", ":", "self", ".", "maximize_dockwidget", "(", "restore", "=", "True", ")", "if", "not", "(", "self", ".", "isMaximized", "(", ")", "or", "self", ".", "maximized_flag", ")", ":", "self", ".", "showMaximized", "(", ")", "min_width", "=", "self", ".", "minimumWidth", "(", ")", "max_width", "=", "self", ".", "maximumWidth", "(", ")", "base_width", "=", "self", ".", "width", "(", ")", "self", ".", "setFixedWidth", "(", "base_width", ")", "# IMPORTANT: order has to be the same as defined in the config file\r", "MATLAB", ",", "RSTUDIO", ",", "VERTICAL", ",", "HORIZONTAL", "=", "range", "(", "self", ".", "DEFAULT_LAYOUTS", ")", "# Define widgets locally\r", "editor", "=", "self", ".", "editor", "console_ipy", "=", "self", ".", "ipyconsole", "console_int", "=", "self", ".", "console", "outline", "=", "self", ".", "outlineexplorer", "explorer_project", "=", "self", ".", "projects", "explorer_file", "=", "self", ".", "explorer", "explorer_variable", "=", "self", ".", "variableexplorer", "plots", "=", "self", ".", "plots", "history", "=", "self", ".", "historylog", "finder", "=", "self", ".", "findinfiles", "help_plugin", "=", "self", ".", "help", "helper", "=", "self", ".", "onlinehelp", "plugins", "=", "self", ".", "thirdparty_plugins", "# Stored for tests\r", "global_hidden_widgets", "=", "[", "finder", ",", "console_int", ",", "explorer_project", ",", "helper", "]", "+", "plugins", "global_hidden_toolbars", "=", "[", "self", ".", "source_toolbar", ",", "self", ".", "edit_toolbar", ",", "self", ".", "search_toolbar", "]", "# Layout definition\r", "# --------------------------------------------------------------------\r", "# Layouts are organized by columns, each column is organized by rows.\r", "# Widths have to add 1.0 (except if hidden), height per column has to\r", "# add 1.0 as well\r", "# Spyder Default Initial Layout\r", "s_layout", "=", "{", "'widgets'", ":", "[", "# Column 0\r", "[", "[", "explorer_project", "]", "]", ",", "# Column 1\r", "[", "[", "editor", "]", "]", ",", "# Column 2\r", "[", "[", "outline", "]", "]", ",", "# Column 3\r", "[", "[", "help_plugin", ",", "explorer_variable", ",", "plots", ",", "# Row 0\r", "helper", ",", "explorer_file", ",", "finder", "]", "+", "plugins", ",", "[", "console_int", ",", "console_ipy", ",", "history", "]", "]", "# Row 1\r", "]", ",", "'width fraction'", ":", "[", "0.05", ",", "# Column 0 width\r", "0.55", ",", "# Column 1 width\r", "0.05", ",", "# Column 2 width\r", "0.45", "]", ",", "# Column 3 width\r", "'height fraction'", ":", "[", "[", "1.0", "]", ",", "# Column 0, row heights\r", "[", "1.0", "]", ",", "# Column 1, row heights\r", "[", "1.0", "]", ",", "# Column 2, row heights\r", "[", "0.46", ",", "0.54", "]", "]", ",", "# Column 3, row heights\r", "'hidden widgets'", ":", "[", "outline", "]", "+", "global_hidden_widgets", ",", "'hidden toolbars'", ":", "[", "]", ",", "}", "# RStudio\r", "r_layout", "=", "{", "'widgets'", ":", "[", "# column 0\r", "[", "[", "editor", "]", ",", "# Row 0\r", "[", "console_ipy", ",", "console_int", "]", "]", ",", "# Row 1\r", "# column 1\r", "[", "[", "explorer_variable", ",", "plots", ",", "history", ",", "# Row 0\r", "outline", ",", "finder", "]", "+", "plugins", ",", "[", "explorer_file", ",", "explorer_project", ",", "# Row 1\r", "help_plugin", ",", "helper", "]", "]", "]", ",", "'width fraction'", ":", "[", "0.55", ",", "# Column 0 width\r", "0.45", "]", ",", "# Column 1 width\r", "'height fraction'", ":", "[", "[", "0.55", ",", "0.45", "]", ",", "# Column 0, row heights\r", "[", "0.55", ",", "0.45", "]", "]", ",", "# Column 1, row heights\r", "'hidden widgets'", ":", "[", "outline", "]", "+", "global_hidden_widgets", ",", "'hidden toolbars'", ":", "[", "]", ",", "}", "# Matlab\r", "m_layout", "=", "{", "'widgets'", ":", "[", "# column 0\r", "[", "[", "explorer_file", ",", "explorer_project", "]", ",", "[", "outline", "]", "]", ",", "# column 1\r", "[", "[", "editor", "]", ",", "[", "console_ipy", ",", "console_int", "]", "]", ",", "# column 2\r", "[", "[", "explorer_variable", ",", "plots", ",", "finder", "]", "+", "plugins", ",", "[", "history", ",", "help_plugin", ",", "helper", "]", "]", "]", ",", "'width fraction'", ":", "[", "0.10", ",", "# Column 0 width\r", "0.45", ",", "# Column 1 width\r", "0.45", "]", ",", "# Column 2 width\r", "'height fraction'", ":", "[", "[", "0.55", ",", "0.45", "]", ",", "# Column 0, row heights\r", "[", "0.55", ",", "0.45", "]", ",", "# Column 1, row heights\r", "[", "0.55", ",", "0.45", "]", "]", ",", "# Column 2, row heights\r", "'hidden widgets'", ":", "global_hidden_widgets", ",", "'hidden toolbars'", ":", "[", "]", ",", "}", "# Vertically split\r", "v_layout", "=", "{", "'widgets'", ":", "[", "# column 0\r", "[", "[", "editor", "]", ",", "# Row 0\r", "[", "console_ipy", ",", "console_int", ",", "explorer_file", ",", "# Row 1\r", "explorer_project", ",", "help_plugin", ",", "explorer_variable", ",", "plots", ",", "history", ",", "outline", ",", "finder", ",", "helper", "]", "+", "plugins", "]", "]", ",", "'width fraction'", ":", "[", "1.0", "]", ",", "# Column 0 width\r", "'height fraction'", ":", "[", "[", "0.55", ",", "0.45", "]", "]", ",", "# Column 0, row heights\r", "'hidden widgets'", ":", "[", "outline", "]", "+", "global_hidden_widgets", ",", "'hidden toolbars'", ":", "[", "]", ",", "}", "# Horizontally split\r", "h_layout", "=", "{", "'widgets'", ":", "[", "# column 0\r", "[", "[", "editor", "]", "]", ",", "# Row 0\r", "# column 1\r", "[", "[", "console_ipy", ",", "console_int", ",", "explorer_file", ",", "# Row 0\r", "explorer_project", ",", "help_plugin", ",", "explorer_variable", ",", "plots", ",", "history", ",", "outline", ",", "finder", ",", "helper", "]", "+", "plugins", "]", "]", ",", "'width fraction'", ":", "[", "0.55", ",", "# Column 0 width\r", "0.45", "]", ",", "# Column 1 width\r", "'height fraction'", ":", "[", "[", "1.0", "]", ",", "# Column 0, row heights\r", "[", "1.0", "]", "]", ",", "# Column 1, row heights\r", "'hidden widgets'", ":", "[", "outline", "]", "+", "global_hidden_widgets", ",", "'hidden toolbars'", ":", "[", "]", "}", "# Layout selection\r", "layouts", "=", "{", "'default'", ":", "s_layout", ",", "RSTUDIO", ":", "r_layout", ",", "MATLAB", ":", "m_layout", ",", "VERTICAL", ":", "v_layout", ",", "HORIZONTAL", ":", "h_layout", ",", "}", "layout", "=", "layouts", "[", "index", "]", "# Remove None from widgets layout\r", "widgets_layout", "=", "layout", "[", "'widgets'", "]", "widgets_layout_clean", "=", "[", "]", "for", "column", "in", "widgets_layout", ":", "clean_col", "=", "[", "]", "for", "row", "in", "column", ":", "clean_row", "=", "[", "w", "for", "w", "in", "row", "if", "w", "is", "not", "None", "]", "if", "clean_row", ":", "clean_col", ".", "append", "(", "clean_row", ")", "if", "clean_col", ":", "widgets_layout_clean", ".", "append", "(", "clean_col", ")", "# Flatten widgets list\r", "widgets", "=", "[", "]", "for", "column", "in", "widgets_layout_clean", ":", "for", "row", "in", "column", ":", "for", "widget", "in", "row", ":", "widgets", ".", "append", "(", "widget", ")", "# Make every widget visible\r", "for", "widget", "in", "widgets", ":", "widget", ".", "toggle_view", "(", "True", ")", "widget", ".", "toggle_view_action", ".", "setChecked", "(", "True", ")", "# We use both directions to ensure proper update when moving from\r", "# 'Horizontal Split' to 'Spyder Default'\r", "# This also seems to help on random cases where the display seems\r", "# 'empty'\r", "for", "direction", "in", "(", "Qt", ".", "Vertical", ",", "Qt", ".", "Horizontal", ")", ":", "# Arrange the widgets in one direction\r", "for", "idx", "in", "range", "(", "len", "(", "widgets", ")", "-", "1", ")", ":", "first", ",", "second", "=", "widgets", "[", "idx", "]", ",", "widgets", "[", "idx", "+", "1", "]", "if", "first", "is", "not", "None", "and", "second", "is", "not", "None", ":", "self", ".", "splitDockWidget", "(", "first", ".", "dockwidget", ",", "second", ".", "dockwidget", ",", "direction", ")", "# Arrange the widgets in the other direction\r", "for", "column", "in", "widgets_layout_clean", ":", "for", "idx", "in", "range", "(", "len", "(", "column", ")", "-", "1", ")", ":", "first_row", ",", "second_row", "=", "column", "[", "idx", "]", ",", "column", "[", "idx", "+", "1", "]", "self", ".", "splitDockWidget", "(", "first_row", "[", "0", "]", ".", "dockwidget", ",", "second_row", "[", "0", "]", ".", "dockwidget", ",", "Qt", ".", "Vertical", ")", "# Tabify\r", "for", "column", "in", "widgets_layout_clean", ":", "for", "row", "in", "column", ":", "for", "idx", "in", "range", "(", "len", "(", "row", ")", "-", "1", ")", ":", "first", ",", "second", "=", "row", "[", "idx", "]", ",", "row", "[", "idx", "+", "1", "]", "self", ".", "tabify_plugins", "(", "first", ",", "second", ")", "# Raise front widget per row\r", "row", "[", "0", "]", ".", "dockwidget", ".", "show", "(", ")", "row", "[", "0", "]", ".", "dockwidget", ".", "raise_", "(", ")", "# Set dockwidget widths\r", "width_fractions", "=", "layout", "[", "'width fraction'", "]", "if", "len", "(", "width_fractions", ")", ">", "1", ":", "_widgets", "=", "[", "col", "[", "0", "]", "[", "0", "]", ".", "dockwidget", "for", "col", "in", "widgets_layout", "]", "self", ".", "resizeDocks", "(", "_widgets", ",", "width_fractions", ",", "Qt", ".", "Horizontal", ")", "# Set dockwidget heights\r", "height_fractions", "=", "layout", "[", "'height fraction'", "]", "for", "idx", ",", "column", "in", "enumerate", "(", "widgets_layout_clean", ")", ":", "if", "len", "(", "column", ")", ">", "1", ":", "_widgets", "=", "[", "row", "[", "0", "]", ".", "dockwidget", "for", "row", "in", "column", "]", "self", ".", "resizeDocks", "(", "_widgets", ",", "height_fractions", "[", "idx", "]", ",", "Qt", ".", "Vertical", ")", "# Hide toolbars\r", "hidden_toolbars", "=", "global_hidden_toolbars", "+", "layout", "[", "'hidden toolbars'", "]", "for", "toolbar", "in", "hidden_toolbars", ":", "if", "toolbar", "is", "not", "None", ":", "toolbar", ".", "close", "(", ")", "# Hide widgets\r", "hidden_widgets", "=", "layout", "[", "'hidden widgets'", "]", "for", "widget", "in", "hidden_widgets", ":", "if", "widget", "is", "not", "None", ":", "widget", ".", "dockwidget", ".", "close", "(", ")", "if", "first_spyder_run", ":", "self", ".", "first_spyder_run", "=", "False", "else", ":", "self", ".", "setMinimumWidth", "(", "min_width", ")", "self", ".", "setMaximumWidth", "(", "max_width", ")", "if", "not", "(", "self", ".", "isMaximized", "(", ")", "or", "self", ".", "maximized_flag", ")", ":", "self", ".", "showMaximized", "(", ")", "self", ".", "setUpdatesEnabled", "(", "True", ")", "self", ".", "sig_layout_setup_ready", ".", "emit", "(", "layout", ")", "return", "layout"], "original_string": "def setup_default_layouts(self, index, settings):\r\n        \"\"\"Setup default layouts when run for the first time.\"\"\"\r\n        self.setUpdatesEnabled(False)\r\n\r\n        first_spyder_run = bool(self.first_spyder_run)  # Store copy\r\n\r\n        if first_spyder_run:\r\n            self.set_window_settings(*settings)\r\n        else:\r\n            if self.last_plugin:\r\n                if self.last_plugin.ismaximized:\r\n                    self.maximize_dockwidget(restore=True)\r\n\r\n            if not (self.isMaximized() or self.maximized_flag):\r\n                self.showMaximized()\r\n\r\n            min_width = self.minimumWidth()\r\n            max_width = self.maximumWidth()\r\n            base_width = self.width()\r\n            self.setFixedWidth(base_width)\r\n\r\n        # IMPORTANT: order has to be the same as defined in the config file\r\n        MATLAB, RSTUDIO, VERTICAL, HORIZONTAL = range(self.DEFAULT_LAYOUTS)\r\n\r\n        # Define widgets locally\r\n        editor = self.editor\r\n        console_ipy = self.ipyconsole\r\n        console_int = self.console\r\n        outline = self.outlineexplorer\r\n        explorer_project = self.projects\r\n        explorer_file = self.explorer\r\n        explorer_variable = self.variableexplorer\r\n        plots = self.plots\r\n        history = self.historylog\r\n        finder = self.findinfiles\r\n        help_plugin = self.help\r\n        helper = self.onlinehelp\r\n        plugins = self.thirdparty_plugins\r\n\r\n        # Stored for tests\r\n        global_hidden_widgets = [finder, console_int, explorer_project,\r\n                                 helper] + plugins\r\n        global_hidden_toolbars = [self.source_toolbar, self.edit_toolbar,\r\n                                  self.search_toolbar]\r\n        # Layout definition\r\n        # --------------------------------------------------------------------\r\n        # Layouts are organized by columns, each column is organized by rows.\r\n        # Widths have to add 1.0 (except if hidden), height per column has to\r\n        # add 1.0 as well\r\n\r\n        # Spyder Default Initial Layout\r\n        s_layout = {\r\n            'widgets': [\r\n                # Column 0\r\n                [[explorer_project]],\r\n                # Column 1\r\n                [[editor]],\r\n                # Column 2\r\n                [[outline]],\r\n                # Column 3\r\n                [[help_plugin, explorer_variable, plots,     # Row 0\r\n                  helper, explorer_file, finder] + plugins,\r\n                 [console_int, console_ipy, history]]        # Row 1\r\n                ],\r\n            'width fraction': [0.05,            # Column 0 width\r\n                               0.55,            # Column 1 width\r\n                               0.05,            # Column 2 width\r\n                               0.45],           # Column 3 width\r\n            'height fraction': [[1.0],          # Column 0, row heights\r\n                                [1.0],          # Column 1, row heights\r\n                                [1.0],          # Column 2, row heights\r\n                                [0.46, 0.54]],  # Column 3, row heights\r\n            'hidden widgets': [outline] + global_hidden_widgets,\r\n            'hidden toolbars': [],\r\n        }\r\n\r\n        # RStudio\r\n        r_layout = {\r\n            'widgets': [\r\n                # column 0\r\n                [[editor],                            # Row 0\r\n                 [console_ipy, console_int]],         # Row 1\r\n                # column 1\r\n                [[explorer_variable, plots, history,  # Row 0\r\n                  outline, finder] + plugins,\r\n                 [explorer_file, explorer_project,    # Row 1\r\n                  help_plugin, helper]]\r\n                ],\r\n            'width fraction': [0.55,            # Column 0 width\r\n                               0.45],           # Column 1 width\r\n            'height fraction': [[0.55, 0.45],   # Column 0, row heights\r\n                                [0.55, 0.45]],  # Column 1, row heights\r\n            'hidden widgets': [outline] + global_hidden_widgets,\r\n            'hidden toolbars': [],\r\n        }\r\n\r\n        # Matlab\r\n        m_layout = {\r\n            'widgets': [\r\n                # column 0\r\n                [[explorer_file, explorer_project],\r\n                 [outline]],\r\n                # column 1\r\n                [[editor],\r\n                 [console_ipy, console_int]],\r\n                # column 2\r\n                [[explorer_variable, plots, finder] + plugins,\r\n                 [history, help_plugin, helper]]\r\n                ],\r\n            'width fraction': [0.10,            # Column 0 width\r\n                               0.45,            # Column 1 width\r\n                               0.45],           # Column 2 width\r\n            'height fraction': [[0.55, 0.45],   # Column 0, row heights\r\n                                [0.55, 0.45],   # Column 1, row heights\r\n                                [0.55, 0.45]],  # Column 2, row heights\r\n            'hidden widgets': global_hidden_widgets,\r\n            'hidden toolbars': [],\r\n        }\r\n\r\n        # Vertically split\r\n        v_layout = {\r\n            'widgets': [\r\n                # column 0\r\n                [[editor],                                  # Row 0\r\n                 [console_ipy, console_int, explorer_file,  # Row 1\r\n                  explorer_project, help_plugin, explorer_variable, plots,\r\n                  history, outline, finder, helper] + plugins]\r\n                ],\r\n            'width fraction': [1.0],            # Column 0 width\r\n            'height fraction': [[0.55, 0.45]],  # Column 0, row heights\r\n            'hidden widgets': [outline] + global_hidden_widgets,\r\n            'hidden toolbars': [],\r\n        }\r\n\r\n        # Horizontally split\r\n        h_layout = {\r\n            'widgets': [\r\n                # column 0\r\n                [[editor]],                                 # Row 0\r\n                # column 1\r\n                [[console_ipy, console_int, explorer_file,  # Row 0\r\n                  explorer_project, help_plugin, explorer_variable, plots,\r\n                  history, outline, finder, helper] + plugins]\r\n                ],\r\n            'width fraction': [0.55,      # Column 0 width\r\n                               0.45],     # Column 1 width\r\n            'height fraction': [[1.0],    # Column 0, row heights\r\n                                [1.0]],   # Column 1, row heights\r\n            'hidden widgets': [outline] + global_hidden_widgets,\r\n            'hidden toolbars': []\r\n        }\r\n\r\n        # Layout selection\r\n        layouts = {\r\n            'default': s_layout,\r\n            RSTUDIO: r_layout,\r\n            MATLAB: m_layout,\r\n            VERTICAL: v_layout,\r\n            HORIZONTAL: h_layout,\r\n        }\r\n\r\n        layout = layouts[index]\r\n\r\n        # Remove None from widgets layout\r\n        widgets_layout = layout['widgets']\r\n        widgets_layout_clean = []\r\n        for column in widgets_layout:\r\n            clean_col = []\r\n            for row in column:\r\n                clean_row = [w for w in row if w is not None]\r\n                if clean_row:\r\n                    clean_col.append(clean_row)\r\n            if clean_col:\r\n                widgets_layout_clean.append(clean_col)\r\n\r\n        # Flatten widgets list\r\n        widgets = []\r\n        for column in widgets_layout_clean:\r\n            for row in column:\r\n                for widget in row:\r\n                    widgets.append(widget)\r\n\r\n        # Make every widget visible\r\n        for widget in widgets:\r\n            widget.toggle_view(True)\r\n            widget.toggle_view_action.setChecked(True)\r\n\r\n        # We use both directions to ensure proper update when moving from\r\n        # 'Horizontal Split' to 'Spyder Default'\r\n        # This also seems to help on random cases where the display seems\r\n        # 'empty'\r\n        for direction in (Qt.Vertical, Qt.Horizontal):\r\n            # Arrange the widgets in one direction\r\n            for idx in range(len(widgets) - 1):\r\n                first, second = widgets[idx], widgets[idx+1]\r\n                if first is not None and second is not None:\r\n                    self.splitDockWidget(first.dockwidget, second.dockwidget,\r\n                                         direction)\r\n\r\n        # Arrange the widgets in the other direction\r\n        for column in widgets_layout_clean:\r\n            for idx in range(len(column) - 1):\r\n                first_row, second_row = column[idx], column[idx+1]\r\n                self.splitDockWidget(first_row[0].dockwidget,\r\n                                     second_row[0].dockwidget,\r\n                                     Qt.Vertical)\r\n\r\n        # Tabify\r\n        for column in widgets_layout_clean:\r\n            for row in column:\r\n                for idx in range(len(row) - 1):\r\n                    first, second = row[idx], row[idx+1]\r\n                    self.tabify_plugins(first, second)\r\n\r\n                # Raise front widget per row\r\n                row[0].dockwidget.show()\r\n                row[0].dockwidget.raise_()\r\n\r\n        # Set dockwidget widths\r\n        width_fractions = layout['width fraction']\r\n        if len(width_fractions) > 1:\r\n            _widgets = [col[0][0].dockwidget for col in widgets_layout]\r\n            self.resizeDocks(_widgets, width_fractions, Qt.Horizontal)\r\n\r\n        # Set dockwidget heights\r\n        height_fractions = layout['height fraction']\r\n        for idx, column in enumerate(widgets_layout_clean):\r\n            if len(column) > 1:\r\n                _widgets = [row[0].dockwidget for row in column]\r\n                self.resizeDocks(_widgets, height_fractions[idx], Qt.Vertical)\r\n\r\n        # Hide toolbars\r\n        hidden_toolbars = global_hidden_toolbars + layout['hidden toolbars']\r\n        for toolbar in hidden_toolbars:\r\n            if toolbar is not None:\r\n                toolbar.close()\r\n\r\n        # Hide widgets\r\n        hidden_widgets = layout['hidden widgets']\r\n        for widget in hidden_widgets:\r\n            if widget is not None:\r\n                widget.dockwidget.close()\r\n\r\n        if first_spyder_run:\r\n            self.first_spyder_run = False\r\n        else:\r\n            self.setMinimumWidth(min_width)\r\n            self.setMaximumWidth(max_width)\r\n\r\n            if not (self.isMaximized() or self.maximized_flag):\r\n                self.showMaximized()\r\n\r\n        self.setUpdatesEnabled(True)\r\n        self.sig_layout_setup_ready.emit(layout)\r\n\r\n        return layout"}, {"code": "def flush(bank, key=None, cachedir=None):\n    '''\n    Remove the key from the cache bank with all the key content.\n    '''\n    if cachedir is None:\n        cachedir = __cachedir()\n\n    try:\n        if key is None:\n            target = os.path.join(cachedir, os.path.normpath(bank))\n            if not os.path.isdir(target):\n                return False\n            shutil.rmtree(target)\n        else:\n            target = os.path.join(cachedir, os.path.normpath(bank), '{0}.p'.format(key))\n            if not os.path.isfile(target):\n                return False\n            os.remove(target)\n    except OSError as exc:\n        raise SaltCacheError(\n            'There was an error removing \"{0}\": {1}'.format(\n                target, exc\n            )\n        )\n    return True", "code_tokens": ["def", "flush", "(", "bank", ",", "key", "=", "None", ",", "cachedir", "=", "None", ")", ":", "if", "cachedir", "is", "None", ":", "cachedir", "=", "__cachedir", "(", ")", "try", ":", "if", "key", "is", "None", ":", "target", "=", "os", ".", "path", ".", "join", "(", "cachedir", ",", "os", ".", "path", ".", "normpath", "(", "bank", ")", ")", "if", "not", "os", ".", "path", ".", "isdir", "(", "target", ")", ":", "return", "False", "shutil", ".", "rmtree", "(", "target", ")", "else", ":", "target", "=", "os", ".", "path", ".", "join", "(", "cachedir", ",", "os", ".", "path", ".", "normpath", "(", "bank", ")", ",", "'{0}.p'", ".", "format", "(", "key", ")", ")", "if", "not", "os", ".", "path", ".", "isfile", "(", "target", ")", ":", "return", "False", "os", ".", "remove", "(", "target", ")", "except", "OSError", "as", "exc", ":", "raise", "SaltCacheError", "(", "'There was an error removing \"{0}\": {1}'", ".", "format", "(", "target", ",", "exc", ")", ")", "return", "True"], "original_string": "def flush(bank, key=None, cachedir=None):\n    '''\n    Remove the key from the cache bank with all the key content.\n    '''\n    if cachedir is None:\n        cachedir = __cachedir()\n\n    try:\n        if key is None:\n            target = os.path.join(cachedir, os.path.normpath(bank))\n            if not os.path.isdir(target):\n                return False\n            shutil.rmtree(target)\n        else:\n            target = os.path.join(cachedir, os.path.normpath(bank), '{0}.p'.format(key))\n            if not os.path.isfile(target):\n                return False\n            os.remove(target)\n    except OSError as exc:\n        raise SaltCacheError(\n            'There was an error removing \"{0}\": {1}'.format(\n                target, exc\n            )\n        )\n    return True"}, {"code": "def read_feather(cls, path, columns=None, use_threads=True):\n        \"\"\"Read a pandas.DataFrame from Feather format.\n           Ray DataFrame only supports pyarrow engine for now.\n\n        Args:\n            path: The filepath of the feather file.\n                  We only support local files for now.\n                multi threading is set to True by default\n            columns: not supported by pandas api, but can be passed here to read only\n                specific columns\n            use_threads: Whether or not to use threads when reading\n\n        Notes:\n            pyarrow feather is used. Please refer to the documentation here\n            https://arrow.apache.org/docs/python/api.html#feather-format\n        \"\"\"\n        if cls.read_feather_remote_task is None:\n            return super(RayIO, cls).read_feather(\n                path, columns=columns, use_threads=use_threads\n            )\n\n        if columns is None:\n            from pyarrow.feather import FeatherReader\n\n            fr = FeatherReader(path)\n            columns = [fr.get_column_name(i) for i in range(fr.num_columns)]\n\n        num_partitions = cls.frame_mgr_cls._compute_num_partitions()\n        num_splits = min(len(columns), num_partitions)\n        # Each item in this list will be a list of column names of the original df\n        column_splits = (\n            len(columns) // num_partitions\n            if len(columns) % num_partitions == 0\n            else len(columns) // num_partitions + 1\n        )\n        col_partitions = [\n            columns[i : i + column_splits]\n            for i in range(0, len(columns), column_splits)\n        ]\n        blk_partitions = np.array(\n            [\n                cls.read_feather_remote_task._remote(\n                    args=(path, cols, num_splits), num_return_vals=num_splits + 1\n                )\n                for cols in col_partitions\n            ]\n        ).T\n        remote_partitions = np.array(\n            [\n                [cls.frame_partition_cls(obj) for obj in row]\n                for row in blk_partitions[:-1]\n            ]\n        )\n        index_len = ray.get(blk_partitions[-1][0])\n        index = pandas.RangeIndex(index_len)\n        new_query_compiler = cls.query_compiler_cls(\n            cls.frame_mgr_cls(remote_partitions), index, columns\n        )\n        return new_query_compiler", "code_tokens": ["def", "read_feather", "(", "cls", ",", "path", ",", "columns", "=", "None", ",", "use_threads", "=", "True", ")", ":", "if", "cls", ".", "read_feather_remote_task", "is", "None", ":", "return", "super", "(", "RayIO", ",", "cls", ")", ".", "read_feather", "(", "path", ",", "columns", "=", "columns", ",", "use_threads", "=", "use_threads", ")", "if", "columns", "is", "None", ":", "from", "pyarrow", ".", "feather", "import", "FeatherReader", "fr", "=", "FeatherReader", "(", "path", ")", "columns", "=", "[", "fr", ".", "get_column_name", "(", "i", ")", "for", "i", "in", "range", "(", "fr", ".", "num_columns", ")", "]", "num_partitions", "=", "cls", ".", "frame_mgr_cls", ".", "_compute_num_partitions", "(", ")", "num_splits", "=", "min", "(", "len", "(", "columns", ")", ",", "num_partitions", ")", "# Each item in this list will be a list of column names of the original df", "column_splits", "=", "(", "len", "(", "columns", ")", "//", "num_partitions", "if", "len", "(", "columns", ")", "%", "num_partitions", "==", "0", "else", "len", "(", "columns", ")", "//", "num_partitions", "+", "1", ")", "col_partitions", "=", "[", "columns", "[", "i", ":", "i", "+", "column_splits", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "columns", ")", ",", "column_splits", ")", "]", "blk_partitions", "=", "np", ".", "array", "(", "[", "cls", ".", "read_feather_remote_task", ".", "_remote", "(", "args", "=", "(", "path", ",", "cols", ",", "num_splits", ")", ",", "num_return_vals", "=", "num_splits", "+", "1", ")", "for", "cols", "in", "col_partitions", "]", ")", ".", "T", "remote_partitions", "=", "np", ".", "array", "(", "[", "[", "cls", ".", "frame_partition_cls", "(", "obj", ")", "for", "obj", "in", "row", "]", "for", "row", "in", "blk_partitions", "[", ":", "-", "1", "]", "]", ")", "index_len", "=", "ray", ".", "get", "(", "blk_partitions", "[", "-", "1", "]", "[", "0", "]", ")", "index", "=", "pandas", ".", "RangeIndex", "(", "index_len", ")", "new_query_compiler", "=", "cls", ".", "query_compiler_cls", "(", "cls", ".", "frame_mgr_cls", "(", "remote_partitions", ")", ",", "index", ",", "columns", ")", "return", "new_query_compiler"], "original_string": "def read_feather(cls, path, columns=None, use_threads=True):\n        \"\"\"Read a pandas.DataFrame from Feather format.\n           Ray DataFrame only supports pyarrow engine for now.\n\n        Args:\n            path: The filepath of the feather file.\n                  We only support local files for now.\n                multi threading is set to True by default\n            columns: not supported by pandas api, but can be passed here to read only\n                specific columns\n            use_threads: Whether or not to use threads when reading\n\n        Notes:\n            pyarrow feather is used. Please refer to the documentation here\n            https://arrow.apache.org/docs/python/api.html#feather-format\n        \"\"\"\n        if cls.read_feather_remote_task is None:\n            return super(RayIO, cls).read_feather(\n                path, columns=columns, use_threads=use_threads\n            )\n\n        if columns is None:\n            from pyarrow.feather import FeatherReader\n\n            fr = FeatherReader(path)\n            columns = [fr.get_column_name(i) for i in range(fr.num_columns)]\n\n        num_partitions = cls.frame_mgr_cls._compute_num_partitions()\n        num_splits = min(len(columns), num_partitions)\n        # Each item in this list will be a list of column names of the original df\n        column_splits = (\n            len(columns) // num_partitions\n            if len(columns) % num_partitions == 0\n            else len(columns) // num_partitions + 1\n        )\n        col_partitions = [\n            columns[i : i + column_splits]\n            for i in range(0, len(columns), column_splits)\n        ]\n        blk_partitions = np.array(\n            [\n                cls.read_feather_remote_task._remote(\n                    args=(path, cols, num_splits), num_return_vals=num_splits + 1\n                )\n                for cols in col_partitions\n            ]\n        ).T\n        remote_partitions = np.array(\n            [\n                [cls.frame_partition_cls(obj) for obj in row]\n                for row in blk_partitions[:-1]\n            ]\n        )\n        index_len = ray.get(blk_partitions[-1][0])\n        index = pandas.RangeIndex(index_len)\n        new_query_compiler = cls.query_compiler_cls(\n            cls.frame_mgr_cls(remote_partitions), index, columns\n        )\n        return new_query_compiler"}, {"code": "def list_disk_partitions(disk_id=None, scsi_address=None,\n                          service_instance=None):\n    '''\n    Lists the partitions on a disk.\n    The disk can be specified either by the canonical name, or by the\n    scsi_address.\n\n    disk_id\n        Canonical name of the disk.\n        Either ``disk_id`` or ``scsi_address`` needs to be specified\n        (``disk_id`` supersedes ``scsi_address``.\n\n    scsi_address`\n        Scsi address of the disk.\n        ``disk_id`` or ``scsi_address`` needs to be specified\n        (``disk_id`` supersedes ``scsi_address``.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter/ESXi host.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.list_disk_partitions scsi_address='vmhaba0:C0:T0:L0'\n\n        salt '*' vsphere.list_disk_partitions disk_id='naa.000000000000001'\n    '''\n    if not disk_id and not scsi_address:\n        raise ArgumentValueError('Either \\'disk_id\\' or \\'scsi_address\\' '\n                                 'needs to be specified')\n    host_ref = _get_proxy_target(service_instance)\n    hostname = __proxy__['esxi.get_details']()['esxi_host']\n    if not disk_id:\n        scsi_address_to_lun = \\\n                salt.utils.vmware.get_scsi_address_to_lun_map(host_ref)\n        if scsi_address not in scsi_address_to_lun:\n            raise VMwareObjectRetrievalError(\n                'Scsi lun with address \\'{0}\\' was not found on host \\'{1}\\''\n                ''.format(scsi_address, hostname))\n        disk_id = scsi_address_to_lun[scsi_address].canonicalName\n        log.trace('[%s] Got disk id \\'%s\\' for scsi address \\'%s\\'',\n                  hostname, disk_id, scsi_address)\n    log.trace('Listing disk partitions on disk \\'%s\\' in host \\'%s\\'',\n              disk_id, hostname)\n    partition_info = \\\n            salt.utils.vmware.get_disk_partition_info(host_ref, disk_id)\n    ret_list = []\n    # NOTE: 1. The layout view has an extra 'None' partition for free space\n    #       2. The orders in the layout/partition views are not the same\n    for part_spec in partition_info.spec.partition:\n        part_layout = [p for p in partition_info.layout.partition\n                       if p.partition == part_spec.partition][0]\n        part_dict = {'hostname': hostname,\n                     'device': disk_id,\n                     'format': partition_info.spec.partitionFormat,\n                     'partition': part_spec.partition,\n                     'type': part_spec.type,\n                     'sectors':\n                     part_spec.endSector - part_spec.startSector + 1,\n                     'size_KB':\n                     (part_layout.end.block - part_layout.start.block + 1) *\n                     part_layout.start.blockSize / 1024}\n        ret_list.append(part_dict)\n    return ret_list", "code_tokens": ["def", "list_disk_partitions", "(", "disk_id", "=", "None", ",", "scsi_address", "=", "None", ",", "service_instance", "=", "None", ")", ":", "if", "not", "disk_id", "and", "not", "scsi_address", ":", "raise", "ArgumentValueError", "(", "'Either \\'disk_id\\' or \\'scsi_address\\' '", "'needs to be specified'", ")", "host_ref", "=", "_get_proxy_target", "(", "service_instance", ")", "hostname", "=", "__proxy__", "[", "'esxi.get_details'", "]", "(", ")", "[", "'esxi_host'", "]", "if", "not", "disk_id", ":", "scsi_address_to_lun", "=", "salt", ".", "utils", ".", "vmware", ".", "get_scsi_address_to_lun_map", "(", "host_ref", ")", "if", "scsi_address", "not", "in", "scsi_address_to_lun", ":", "raise", "VMwareObjectRetrievalError", "(", "'Scsi lun with address \\'{0}\\' was not found on host \\'{1}\\''", "''", ".", "format", "(", "scsi_address", ",", "hostname", ")", ")", "disk_id", "=", "scsi_address_to_lun", "[", "scsi_address", "]", ".", "canonicalName", "log", ".", "trace", "(", "'[%s] Got disk id \\'%s\\' for scsi address \\'%s\\''", ",", "hostname", ",", "disk_id", ",", "scsi_address", ")", "log", ".", "trace", "(", "'Listing disk partitions on disk \\'%s\\' in host \\'%s\\''", ",", "disk_id", ",", "hostname", ")", "partition_info", "=", "salt", ".", "utils", ".", "vmware", ".", "get_disk_partition_info", "(", "host_ref", ",", "disk_id", ")", "ret_list", "=", "[", "]", "# NOTE: 1. The layout view has an extra 'None' partition for free space", "#       2. The orders in the layout/partition views are not the same", "for", "part_spec", "in", "partition_info", ".", "spec", ".", "partition", ":", "part_layout", "=", "[", "p", "for", "p", "in", "partition_info", ".", "layout", ".", "partition", "if", "p", ".", "partition", "==", "part_spec", ".", "partition", "]", "[", "0", "]", "part_dict", "=", "{", "'hostname'", ":", "hostname", ",", "'device'", ":", "disk_id", ",", "'format'", ":", "partition_info", ".", "spec", ".", "partitionFormat", ",", "'partition'", ":", "part_spec", ".", "partition", ",", "'type'", ":", "part_spec", ".", "type", ",", "'sectors'", ":", "part_spec", ".", "endSector", "-", "part_spec", ".", "startSector", "+", "1", ",", "'size_KB'", ":", "(", "part_layout", ".", "end", ".", "block", "-", "part_layout", ".", "start", ".", "block", "+", "1", ")", "*", "part_layout", ".", "start", ".", "blockSize", "/", "1024", "}", "ret_list", ".", "append", "(", "part_dict", ")", "return", "ret_list"], "original_string": "def list_disk_partitions(disk_id=None, scsi_address=None,\n                          service_instance=None):\n    '''\n    Lists the partitions on a disk.\n    The disk can be specified either by the canonical name, or by the\n    scsi_address.\n\n    disk_id\n        Canonical name of the disk.\n        Either ``disk_id`` or ``scsi_address`` needs to be specified\n        (``disk_id`` supersedes ``scsi_address``.\n\n    scsi_address`\n        Scsi address of the disk.\n        ``disk_id`` or ``scsi_address`` needs to be specified\n        (``disk_id`` supersedes ``scsi_address``.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter/ESXi host.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.list_disk_partitions scsi_address='vmhaba0:C0:T0:L0'\n\n        salt '*' vsphere.list_disk_partitions disk_id='naa.000000000000001'\n    '''\n    if not disk_id and not scsi_address:\n        raise ArgumentValueError('Either \\'disk_id\\' or \\'scsi_address\\' '\n                                 'needs to be specified')\n    host_ref = _get_proxy_target(service_instance)\n    hostname = __proxy__['esxi.get_details']()['esxi_host']\n    if not disk_id:\n        scsi_address_to_lun = \\\n                salt.utils.vmware.get_scsi_address_to_lun_map(host_ref)\n        if scsi_address not in scsi_address_to_lun:\n            raise VMwareObjectRetrievalError(\n                'Scsi lun with address \\'{0}\\' was not found on host \\'{1}\\''\n                ''.format(scsi_address, hostname))\n        disk_id = scsi_address_to_lun[scsi_address].canonicalName\n        log.trace('[%s] Got disk id \\'%s\\' for scsi address \\'%s\\'',\n                  hostname, disk_id, scsi_address)\n    log.trace('Listing disk partitions on disk \\'%s\\' in host \\'%s\\'',\n              disk_id, hostname)\n    partition_info = \\\n            salt.utils.vmware.get_disk_partition_info(host_ref, disk_id)\n    ret_list = []\n    # NOTE: 1. The layout view has an extra 'None' partition for free space\n    #       2. The orders in the layout/partition views are not the same\n    for part_spec in partition_info.spec.partition:\n        part_layout = [p for p in partition_info.layout.partition\n                       if p.partition == part_spec.partition][0]\n        part_dict = {'hostname': hostname,\n                     'device': disk_id,\n                     'format': partition_info.spec.partitionFormat,\n                     'partition': part_spec.partition,\n                     'type': part_spec.type,\n                     'sectors':\n                     part_spec.endSector - part_spec.startSector + 1,\n                     'size_KB':\n                     (part_layout.end.block - part_layout.start.block + 1) *\n                     part_layout.start.blockSize / 1024}\n        ret_list.append(part_dict)\n    return ret_list"}, {"code": "def _get_popularity_baseline(self):\n\n        \"\"\"\n        Returns a new popularity model matching the data set this model was\n        trained with.  Can be used for comparison purposes.\n        \"\"\"\n\n        response = self.__proxy__.get_popularity_baseline()\n        from .popularity_recommender import PopularityRecommender\n        return PopularityRecommender(response)", "code_tokens": ["def", "_get_popularity_baseline", "(", "self", ")", ":", "response", "=", "self", ".", "__proxy__", ".", "get_popularity_baseline", "(", ")", "from", ".", "popularity_recommender", "import", "PopularityRecommender", "return", "PopularityRecommender", "(", "response", ")"], "original_string": "def _get_popularity_baseline(self):\n\n        \"\"\"\n        Returns a new popularity model matching the data set this model was\n        trained with.  Can be used for comparison purposes.\n        \"\"\"\n\n        response = self.__proxy__.get_popularity_baseline()\n        from .popularity_recommender import PopularityRecommender\n        return PopularityRecommender(response)"}, {"code": "def deployments(namespace='default', **kwargs):\n    '''\n    Return a list of kubernetes deployments defined in the namespace\n\n    CLI Examples::\n\n        salt '*' kubernetes.deployments\n        salt '*' kubernetes.deployments namespace=default\n    '''\n    cfg = _setup_conn(**kwargs)\n    try:\n        api_instance = kubernetes.client.ExtensionsV1beta1Api()\n        api_response = api_instance.list_namespaced_deployment(namespace)\n\n        return [dep['metadata']['name'] for dep in api_response.to_dict().get('items')]\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception(\n                'Exception when calling '\n                'ExtensionsV1beta1Api->list_namespaced_deployment'\n            )\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)", "code_tokens": ["def", "deployments", "(", "namespace", "=", "'default'", ",", "*", "*", "kwargs", ")", ":", "cfg", "=", "_setup_conn", "(", "*", "*", "kwargs", ")", "try", ":", "api_instance", "=", "kubernetes", ".", "client", ".", "ExtensionsV1beta1Api", "(", ")", "api_response", "=", "api_instance", ".", "list_namespaced_deployment", "(", "namespace", ")", "return", "[", "dep", "[", "'metadata'", "]", "[", "'name'", "]", "for", "dep", "in", "api_response", ".", "to_dict", "(", ")", ".", "get", "(", "'items'", ")", "]", "except", "(", "ApiException", ",", "HTTPError", ")", "as", "exc", ":", "if", "isinstance", "(", "exc", ",", "ApiException", ")", "and", "exc", ".", "status", "==", "404", ":", "return", "None", "else", ":", "log", ".", "exception", "(", "'Exception when calling '", "'ExtensionsV1beta1Api->list_namespaced_deployment'", ")", "raise", "CommandExecutionError", "(", "exc", ")", "finally", ":", "_cleanup", "(", "*", "*", "cfg", ")"], "original_string": "def deployments(namespace='default', **kwargs):\n    '''\n    Return a list of kubernetes deployments defined in the namespace\n\n    CLI Examples::\n\n        salt '*' kubernetes.deployments\n        salt '*' kubernetes.deployments namespace=default\n    '''\n    cfg = _setup_conn(**kwargs)\n    try:\n        api_instance = kubernetes.client.ExtensionsV1beta1Api()\n        api_response = api_instance.list_namespaced_deployment(namespace)\n\n        return [dep['metadata']['name'] for dep in api_response.to_dict().get('items')]\n    except (ApiException, HTTPError) as exc:\n        if isinstance(exc, ApiException) and exc.status == 404:\n            return None\n        else:\n            log.exception(\n                'Exception when calling '\n                'ExtensionsV1beta1Api->list_namespaced_deployment'\n            )\n            raise CommandExecutionError(exc)\n    finally:\n        _cleanup(**cfg)"}, {"code": "def _get_portfolio_info(self, portfolio_code):\n        \"\"\"\n        \u83b7\u53d6\u7ec4\u5408\u4fe1\u606f\n        :return: \u5b57\u5178\n        \"\"\"\n        url = self.config[\"portfolio_url\"] + portfolio_code\n        html = self._get_html(url)\n        match_info = re.search(r\"(?<=SNB.cubeInfo = ).*(?=;\\n)\", html)\n        if match_info is None:\n            raise Exception(\n                \"cant get portfolio info, portfolio html : {}\".format(html)\n            )\n        try:\n            portfolio_info = json.loads(match_info.group())\n        except Exception as e:\n            raise Exception(\"get portfolio info error: {}\".format(e))\n        return portfolio_info", "code_tokens": ["def", "_get_portfolio_info", "(", "self", ",", "portfolio_code", ")", ":", "url", "=", "self", ".", "config", "[", "\"portfolio_url\"", "]", "+", "portfolio_code", "html", "=", "self", ".", "_get_html", "(", "url", ")", "match_info", "=", "re", ".", "search", "(", "r\"(?<=SNB.cubeInfo = ).*(?=;\\n)\"", ",", "html", ")", "if", "match_info", "is", "None", ":", "raise", "Exception", "(", "\"cant get portfolio info, portfolio html : {}\"", ".", "format", "(", "html", ")", ")", "try", ":", "portfolio_info", "=", "json", ".", "loads", "(", "match_info", ".", "group", "(", ")", ")", "except", "Exception", "as", "e", ":", "raise", "Exception", "(", "\"get portfolio info error: {}\"", ".", "format", "(", "e", ")", ")", "return", "portfolio_info"], "original_string": "def _get_portfolio_info(self, portfolio_code):\n        \"\"\"\n        \u83b7\u53d6\u7ec4\u5408\u4fe1\u606f\n        :return: \u5b57\u5178\n        \"\"\"\n        url = self.config[\"portfolio_url\"] + portfolio_code\n        html = self._get_html(url)\n        match_info = re.search(r\"(?<=SNB.cubeInfo = ).*(?=;\\n)\", html)\n        if match_info is None:\n            raise Exception(\n                \"cant get portfolio info, portfolio html : {}\".format(html)\n            )\n        try:\n            portfolio_info = json.loads(match_info.group())\n        except Exception as e:\n            raise Exception(\"get portfolio info error: {}\".format(e))\n        return portfolio_info"}, {"code": "def get_gated_grpc_tensors(self, matching_debug_op=None):\n    \"\"\"Extract all nodes with gated-gRPC debug ops attached.\n\n    Uses cached values if available.\n    This method is thread-safe.\n\n    Args:\n      graph_def: A tf.GraphDef proto.\n      matching_debug_op: Return tensors and nodes with only matching the\n        specified debug op name (optional). If `None`, will extract only\n        `DebugIdentity` debug ops.\n\n    Returns:\n      A list of (node_name, op_type, output_slot, debug_op) tuples.\n    \"\"\"\n    with self._grpc_gated_lock:\n      matching_debug_op = matching_debug_op or 'DebugIdentity'\n      if matching_debug_op not in self._grpc_gated_tensors:\n        # First, construct a map from node name to op type.\n        node_name_to_op_type = dict(\n            (node.name, node.op) for node in self._graph_def.node)\n\n        # Second, populate the output list.\n        gated = []\n        for node in self._graph_def.node:\n          if node.op == matching_debug_op:\n            for attr_key in node.attr:\n              if attr_key == 'gated_grpc' and node.attr[attr_key].b:\n                node_name, output_slot, _, debug_op = (\n                    debug_graphs.parse_debug_node_name(node.name))\n                gated.append(\n                    (node_name, node_name_to_op_type[node_name], output_slot,\n                     debug_op))\n                break\n        self._grpc_gated_tensors[matching_debug_op] = gated\n\n      return self._grpc_gated_tensors[matching_debug_op]", "code_tokens": ["def", "get_gated_grpc_tensors", "(", "self", ",", "matching_debug_op", "=", "None", ")", ":", "with", "self", ".", "_grpc_gated_lock", ":", "matching_debug_op", "=", "matching_debug_op", "or", "'DebugIdentity'", "if", "matching_debug_op", "not", "in", "self", ".", "_grpc_gated_tensors", ":", "# First, construct a map from node name to op type.", "node_name_to_op_type", "=", "dict", "(", "(", "node", ".", "name", ",", "node", ".", "op", ")", "for", "node", "in", "self", ".", "_graph_def", ".", "node", ")", "# Second, populate the output list.", "gated", "=", "[", "]", "for", "node", "in", "self", ".", "_graph_def", ".", "node", ":", "if", "node", ".", "op", "==", "matching_debug_op", ":", "for", "attr_key", "in", "node", ".", "attr", ":", "if", "attr_key", "==", "'gated_grpc'", "and", "node", ".", "attr", "[", "attr_key", "]", ".", "b", ":", "node_name", ",", "output_slot", ",", "_", ",", "debug_op", "=", "(", "debug_graphs", ".", "parse_debug_node_name", "(", "node", ".", "name", ")", ")", "gated", ".", "append", "(", "(", "node_name", ",", "node_name_to_op_type", "[", "node_name", "]", ",", "output_slot", ",", "debug_op", ")", ")", "break", "self", ".", "_grpc_gated_tensors", "[", "matching_debug_op", "]", "=", "gated", "return", "self", ".", "_grpc_gated_tensors", "[", "matching_debug_op", "]"], "original_string": "def get_gated_grpc_tensors(self, matching_debug_op=None):\n    \"\"\"Extract all nodes with gated-gRPC debug ops attached.\n\n    Uses cached values if available.\n    This method is thread-safe.\n\n    Args:\n      graph_def: A tf.GraphDef proto.\n      matching_debug_op: Return tensors and nodes with only matching the\n        specified debug op name (optional). If `None`, will extract only\n        `DebugIdentity` debug ops.\n\n    Returns:\n      A list of (node_name, op_type, output_slot, debug_op) tuples.\n    \"\"\"\n    with self._grpc_gated_lock:\n      matching_debug_op = matching_debug_op or 'DebugIdentity'\n      if matching_debug_op not in self._grpc_gated_tensors:\n        # First, construct a map from node name to op type.\n        node_name_to_op_type = dict(\n            (node.name, node.op) for node in self._graph_def.node)\n\n        # Second, populate the output list.\n        gated = []\n        for node in self._graph_def.node:\n          if node.op == matching_debug_op:\n            for attr_key in node.attr:\n              if attr_key == 'gated_grpc' and node.attr[attr_key].b:\n                node_name, output_slot, _, debug_op = (\n                    debug_graphs.parse_debug_node_name(node.name))\n                gated.append(\n                    (node_name, node_name_to_op_type[node_name], output_slot,\n                     debug_op))\n                break\n        self._grpc_gated_tensors[matching_debug_op] = gated\n\n      return self._grpc_gated_tensors[matching_debug_op]"}, {"code": "def lstm_area_attention_base():\n  \"\"\"Hparams for LSTM with area attention.\"\"\"\n  hparams = lstm_luong_attention()\n  hparams.batch_size = 16384\n  hparams.num_hidden_layers = 2\n  hparams.hidden_size = 1024\n  hparams.num_heads = 4\n  hparams.dropout = 0.2\n  hparams.learning_rate = 0.1\n  hparams.max_area_width = 2\n  hparams.area_key_mode = \"mean\"\n  hparams.area_value_mode = \"sum\"\n  return hparams", "code_tokens": ["def", "lstm_area_attention_base", "(", ")", ":", "hparams", "=", "lstm_luong_attention", "(", ")", "hparams", ".", "batch_size", "=", "16384", "hparams", ".", "num_hidden_layers", "=", "2", "hparams", ".", "hidden_size", "=", "1024", "hparams", ".", "num_heads", "=", "4", "hparams", ".", "dropout", "=", "0.2", "hparams", ".", "learning_rate", "=", "0.1", "hparams", ".", "max_area_width", "=", "2", "hparams", ".", "area_key_mode", "=", "\"mean\"", "hparams", ".", "area_value_mode", "=", "\"sum\"", "return", "hparams"], "original_string": "def lstm_area_attention_base():\n  \"\"\"Hparams for LSTM with area attention.\"\"\"\n  hparams = lstm_luong_attention()\n  hparams.batch_size = 16384\n  hparams.num_hidden_layers = 2\n  hparams.hidden_size = 1024\n  hparams.num_heads = 4\n  hparams.dropout = 0.2\n  hparams.learning_rate = 0.1\n  hparams.max_area_width = 2\n  hparams.area_key_mode = \"mean\"\n  hparams.area_value_mode = \"sum\"\n  return hparams"}, {"code": "def get_storage_policies(profile_manager, policy_names=None,\n                         get_all_policies=False):\n    '''\n    Returns a list of the storage policies, filtered by name.\n\n    profile_manager\n        Reference to the profile manager.\n\n    policy_names\n        List of policy names to filter by.\n        Default is None.\n\n    get_all_policies\n        Flag specifying to return all policies, regardless of the specified\n        filter.\n    '''\n    res_type = pbm.profile.ResourceType(\n        resourceType=pbm.profile.ResourceTypeEnum.STORAGE)\n    try:\n        policy_ids = profile_manager.QueryProfile(res_type)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise VMwareApiError('Not enough permissions. Required privilege: '\n                             '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise VMwareRuntimeError(exc.msg)\n    log.trace('policy_ids = %s', policy_ids)\n    # More policies are returned so we need to filter again\n    policies = [p for p in get_policies_by_id(profile_manager, policy_ids)\n                if p.resourceType.resourceType ==\n                pbm.profile.ResourceTypeEnum.STORAGE]\n    if get_all_policies:\n        return policies\n    if not policy_names:\n        policy_names = []\n    return [p for p in policies if p.name in policy_names]", "code_tokens": ["def", "get_storage_policies", "(", "profile_manager", ",", "policy_names", "=", "None", ",", "get_all_policies", "=", "False", ")", ":", "res_type", "=", "pbm", ".", "profile", ".", "ResourceType", "(", "resourceType", "=", "pbm", ".", "profile", ".", "ResourceTypeEnum", ".", "STORAGE", ")", "try", ":", "policy_ids", "=", "profile_manager", ".", "QueryProfile", "(", "res_type", ")", "except", "vim", ".", "fault", ".", "NoPermission", "as", "exc", ":", "log", ".", "exception", "(", "exc", ")", "raise", "VMwareApiError", "(", "'Not enough permissions. Required privilege: '", "'{0}'", ".", "format", "(", "exc", ".", "privilegeId", ")", ")", "except", "vim", ".", "fault", ".", "VimFault", "as", "exc", ":", "log", ".", "exception", "(", "exc", ")", "raise", "VMwareApiError", "(", "exc", ".", "msg", ")", "except", "vmodl", ".", "RuntimeFault", "as", "exc", ":", "log", ".", "exception", "(", "exc", ")", "raise", "VMwareRuntimeError", "(", "exc", ".", "msg", ")", "log", ".", "trace", "(", "'policy_ids = %s'", ",", "policy_ids", ")", "# More policies are returned so we need to filter again", "policies", "=", "[", "p", "for", "p", "in", "get_policies_by_id", "(", "profile_manager", ",", "policy_ids", ")", "if", "p", ".", "resourceType", ".", "resourceType", "==", "pbm", ".", "profile", ".", "ResourceTypeEnum", ".", "STORAGE", "]", "if", "get_all_policies", ":", "return", "policies", "if", "not", "policy_names", ":", "policy_names", "=", "[", "]", "return", "[", "p", "for", "p", "in", "policies", "if", "p", ".", "name", "in", "policy_names", "]"], "original_string": "def get_storage_policies(profile_manager, policy_names=None,\n                         get_all_policies=False):\n    '''\n    Returns a list of the storage policies, filtered by name.\n\n    profile_manager\n        Reference to the profile manager.\n\n    policy_names\n        List of policy names to filter by.\n        Default is None.\n\n    get_all_policies\n        Flag specifying to return all policies, regardless of the specified\n        filter.\n    '''\n    res_type = pbm.profile.ResourceType(\n        resourceType=pbm.profile.ResourceTypeEnum.STORAGE)\n    try:\n        policy_ids = profile_manager.QueryProfile(res_type)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise VMwareApiError('Not enough permissions. Required privilege: '\n                             '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise VMwareRuntimeError(exc.msg)\n    log.trace('policy_ids = %s', policy_ids)\n    # More policies are returned so we need to filter again\n    policies = [p for p in get_policies_by_id(profile_manager, policy_ids)\n                if p.resourceType.resourceType ==\n                pbm.profile.ResourceTypeEnum.STORAGE]\n    if get_all_policies:\n        return policies\n    if not policy_names:\n        policy_names = []\n    return [p for p in policies if p.name in policy_names]"}, {"code": "def import_numpy():\n    \"\"\"\n    Returns the numpy module if it exists on the system,\n    otherwise returns None.\n    \"\"\"\n    try:\n        imp.find_module('numpy')\n        numpy_exists = True\n    except ImportError:\n        numpy_exists = False\n\n    if numpy_exists:\n        # We do this outside of try/except block in case numpy exists\n        # but is not installed correctly. We do not want to catch an\n        # incorrect installation which would manifest as an\n        # ImportError.\n        import numpy as np\n    else:\n        np = None\n\n    return np", "code_tokens": ["def", "import_numpy", "(", ")", ":", "try", ":", "imp", ".", "find_module", "(", "'numpy'", ")", "numpy_exists", "=", "True", "except", "ImportError", ":", "numpy_exists", "=", "False", "if", "numpy_exists", ":", "# We do this outside of try/except block in case numpy exists", "# but is not installed correctly. We do not want to catch an", "# incorrect installation which would manifest as an", "# ImportError.", "import", "numpy", "as", "np", "else", ":", "np", "=", "None", "return", "np"], "original_string": "def import_numpy():\n    \"\"\"\n    Returns the numpy module if it exists on the system,\n    otherwise returns None.\n    \"\"\"\n    try:\n        imp.find_module('numpy')\n        numpy_exists = True\n    except ImportError:\n        numpy_exists = False\n\n    if numpy_exists:\n        # We do this outside of try/except block in case numpy exists\n        # but is not installed correctly. We do not want to catch an\n        # incorrect installation which would manifest as an\n        # ImportError.\n        import numpy as np\n    else:\n        np = None\n\n    return np"}, {"code": "def _init_vocab(self, token_generator, add_reserved_tokens=True):\n    \"\"\"Initialize vocabulary with tokens from token_generator.\"\"\"\n\n    self._id_to_token = {}\n    non_reserved_start_index = 0\n\n    if add_reserved_tokens:\n      self._id_to_token.update(enumerate(RESERVED_TOKENS))\n      non_reserved_start_index = len(RESERVED_TOKENS)\n\n    self._id_to_token.update(\n        enumerate(token_generator, start=non_reserved_start_index))\n\n    # _token_to_id is the reverse of _id_to_token\n    self._token_to_id = dict((v, k)\n                             for k, v in six.iteritems(self._id_to_token))", "code_tokens": ["def", "_init_vocab", "(", "self", ",", "token_generator", ",", "add_reserved_tokens", "=", "True", ")", ":", "self", ".", "_id_to_token", "=", "{", "}", "non_reserved_start_index", "=", "0", "if", "add_reserved_tokens", ":", "self", ".", "_id_to_token", ".", "update", "(", "enumerate", "(", "RESERVED_TOKENS", ")", ")", "non_reserved_start_index", "=", "len", "(", "RESERVED_TOKENS", ")", "self", ".", "_id_to_token", ".", "update", "(", "enumerate", "(", "token_generator", ",", "start", "=", "non_reserved_start_index", ")", ")", "# _token_to_id is the reverse of _id_to_token", "self", ".", "_token_to_id", "=", "dict", "(", "(", "v", ",", "k", ")", "for", "k", ",", "v", "in", "six", ".", "iteritems", "(", "self", ".", "_id_to_token", ")", ")"], "original_string": "def _init_vocab(self, token_generator, add_reserved_tokens=True):\n    \"\"\"Initialize vocabulary with tokens from token_generator.\"\"\"\n\n    self._id_to_token = {}\n    non_reserved_start_index = 0\n\n    if add_reserved_tokens:\n      self._id_to_token.update(enumerate(RESERVED_TOKENS))\n      non_reserved_start_index = len(RESERVED_TOKENS)\n\n    self._id_to_token.update(\n        enumerate(token_generator, start=non_reserved_start_index))\n\n    # _token_to_id is the reverse of _id_to_token\n    self._token_to_id = dict((v, k)\n                             for k, v in six.iteritems(self._id_to_token))"}, {"code": "def write_data(self, chunksize, dropna=False):\n        \"\"\" we form the data into a 2-d including indexes,values,mask\n            write chunk-by-chunk \"\"\"\n\n        names = self.dtype.names\n        nrows = self.nrows_expected\n\n        # if dropna==True, then drop ALL nan rows\n        masks = []\n        if dropna:\n\n            for a in self.values_axes:\n\n                # figure the mask: only do if we can successfully process this\n                # column, otherwise ignore the mask\n                mask = isna(a.data).all(axis=0)\n                if isinstance(mask, np.ndarray):\n                    masks.append(mask.astype('u1', copy=False))\n\n        # consolidate masks\n        if len(masks):\n            mask = masks[0]\n            for m in masks[1:]:\n                mask = mask & m\n            mask = mask.ravel()\n        else:\n            mask = None\n\n        # broadcast the indexes if needed\n        indexes = [a.cvalues for a in self.index_axes]\n        nindexes = len(indexes)\n        bindexes = []\n        for i, idx in enumerate(indexes):\n\n            # broadcast to all other indexes except myself\n            if i > 0 and i < nindexes:\n                repeater = np.prod(\n                    [indexes[bi].shape[0] for bi in range(0, i)])\n                idx = np.tile(idx, repeater)\n\n            if i < nindexes - 1:\n                repeater = np.prod([indexes[bi].shape[0]\n                                    for bi in range(i + 1, nindexes)])\n                idx = np.repeat(idx, repeater)\n\n            bindexes.append(idx)\n\n        # transpose the values so first dimension is last\n        # reshape the values if needed\n        values = [a.take_data() for a in self.values_axes]\n        values = [v.transpose(np.roll(np.arange(v.ndim), v.ndim - 1))\n                  for v in values]\n        bvalues = []\n        for i, v in enumerate(values):\n            new_shape = (nrows,) + self.dtype[names[nindexes + i]].shape\n            bvalues.append(values[i].reshape(new_shape))\n\n        # write the chunks\n        if chunksize is None:\n            chunksize = 100000\n\n        rows = np.empty(min(chunksize, nrows), dtype=self.dtype)\n        chunks = int(nrows / chunksize) + 1\n        for i in range(chunks):\n            start_i = i * chunksize\n            end_i = min((i + 1) * chunksize, nrows)\n            if start_i >= end_i:\n                break\n\n            self.write_data_chunk(\n                rows,\n                indexes=[a[start_i:end_i] for a in bindexes],\n                mask=mask[start_i:end_i] if mask is not None else None,\n                values=[v[start_i:end_i] for v in bvalues])", "code_tokens": ["def", "write_data", "(", "self", ",", "chunksize", ",", "dropna", "=", "False", ")", ":", "names", "=", "self", ".", "dtype", ".", "names", "nrows", "=", "self", ".", "nrows_expected", "# if dropna==True, then drop ALL nan rows", "masks", "=", "[", "]", "if", "dropna", ":", "for", "a", "in", "self", ".", "values_axes", ":", "# figure the mask: only do if we can successfully process this", "# column, otherwise ignore the mask", "mask", "=", "isna", "(", "a", ".", "data", ")", ".", "all", "(", "axis", "=", "0", ")", "if", "isinstance", "(", "mask", ",", "np", ".", "ndarray", ")", ":", "masks", ".", "append", "(", "mask", ".", "astype", "(", "'u1'", ",", "copy", "=", "False", ")", ")", "# consolidate masks", "if", "len", "(", "masks", ")", ":", "mask", "=", "masks", "[", "0", "]", "for", "m", "in", "masks", "[", "1", ":", "]", ":", "mask", "=", "mask", "&", "m", "mask", "=", "mask", ".", "ravel", "(", ")", "else", ":", "mask", "=", "None", "# broadcast the indexes if needed", "indexes", "=", "[", "a", ".", "cvalues", "for", "a", "in", "self", ".", "index_axes", "]", "nindexes", "=", "len", "(", "indexes", ")", "bindexes", "=", "[", "]", "for", "i", ",", "idx", "in", "enumerate", "(", "indexes", ")", ":", "# broadcast to all other indexes except myself", "if", "i", ">", "0", "and", "i", "<", "nindexes", ":", "repeater", "=", "np", ".", "prod", "(", "[", "indexes", "[", "bi", "]", ".", "shape", "[", "0", "]", "for", "bi", "in", "range", "(", "0", ",", "i", ")", "]", ")", "idx", "=", "np", ".", "tile", "(", "idx", ",", "repeater", ")", "if", "i", "<", "nindexes", "-", "1", ":", "repeater", "=", "np", ".", "prod", "(", "[", "indexes", "[", "bi", "]", ".", "shape", "[", "0", "]", "for", "bi", "in", "range", "(", "i", "+", "1", ",", "nindexes", ")", "]", ")", "idx", "=", "np", ".", "repeat", "(", "idx", ",", "repeater", ")", "bindexes", ".", "append", "(", "idx", ")", "# transpose the values so first dimension is last", "# reshape the values if needed", "values", "=", "[", "a", ".", "take_data", "(", ")", "for", "a", "in", "self", ".", "values_axes", "]", "values", "=", "[", "v", ".", "transpose", "(", "np", ".", "roll", "(", "np", ".", "arange", "(", "v", ".", "ndim", ")", ",", "v", ".", "ndim", "-", "1", ")", ")", "for", "v", "in", "values", "]", "bvalues", "=", "[", "]", "for", "i", ",", "v", "in", "enumerate", "(", "values", ")", ":", "new_shape", "=", "(", "nrows", ",", ")", "+", "self", ".", "dtype", "[", "names", "[", "nindexes", "+", "i", "]", "]", ".", "shape", "bvalues", ".", "append", "(", "values", "[", "i", "]", ".", "reshape", "(", "new_shape", ")", ")", "# write the chunks", "if", "chunksize", "is", "None", ":", "chunksize", "=", "100000", "rows", "=", "np", ".", "empty", "(", "min", "(", "chunksize", ",", "nrows", ")", ",", "dtype", "=", "self", ".", "dtype", ")", "chunks", "=", "int", "(", "nrows", "/", "chunksize", ")", "+", "1", "for", "i", "in", "range", "(", "chunks", ")", ":", "start_i", "=", "i", "*", "chunksize", "end_i", "=", "min", "(", "(", "i", "+", "1", ")", "*", "chunksize", ",", "nrows", ")", "if", "start_i", ">=", "end_i", ":", "break", "self", ".", "write_data_chunk", "(", "rows", ",", "indexes", "=", "[", "a", "[", "start_i", ":", "end_i", "]", "for", "a", "in", "bindexes", "]", ",", "mask", "=", "mask", "[", "start_i", ":", "end_i", "]", "if", "mask", "is", "not", "None", "else", "None", ",", "values", "=", "[", "v", "[", "start_i", ":", "end_i", "]", "for", "v", "in", "bvalues", "]", ")"], "original_string": "def write_data(self, chunksize, dropna=False):\n        \"\"\" we form the data into a 2-d including indexes,values,mask\n            write chunk-by-chunk \"\"\"\n\n        names = self.dtype.names\n        nrows = self.nrows_expected\n\n        # if dropna==True, then drop ALL nan rows\n        masks = []\n        if dropna:\n\n            for a in self.values_axes:\n\n                # figure the mask: only do if we can successfully process this\n                # column, otherwise ignore the mask\n                mask = isna(a.data).all(axis=0)\n                if isinstance(mask, np.ndarray):\n                    masks.append(mask.astype('u1', copy=False))\n\n        # consolidate masks\n        if len(masks):\n            mask = masks[0]\n            for m in masks[1:]:\n                mask = mask & m\n            mask = mask.ravel()\n        else:\n            mask = None\n\n        # broadcast the indexes if needed\n        indexes = [a.cvalues for a in self.index_axes]\n        nindexes = len(indexes)\n        bindexes = []\n        for i, idx in enumerate(indexes):\n\n            # broadcast to all other indexes except myself\n            if i > 0 and i < nindexes:\n                repeater = np.prod(\n                    [indexes[bi].shape[0] for bi in range(0, i)])\n                idx = np.tile(idx, repeater)\n\n            if i < nindexes - 1:\n                repeater = np.prod([indexes[bi].shape[0]\n                                    for bi in range(i + 1, nindexes)])\n                idx = np.repeat(idx, repeater)\n\n            bindexes.append(idx)\n\n        # transpose the values so first dimension is last\n        # reshape the values if needed\n        values = [a.take_data() for a in self.values_axes]\n        values = [v.transpose(np.roll(np.arange(v.ndim), v.ndim - 1))\n                  for v in values]\n        bvalues = []\n        for i, v in enumerate(values):\n            new_shape = (nrows,) + self.dtype[names[nindexes + i]].shape\n            bvalues.append(values[i].reshape(new_shape))\n\n        # write the chunks\n        if chunksize is None:\n            chunksize = 100000\n\n        rows = np.empty(min(chunksize, nrows), dtype=self.dtype)\n        chunks = int(nrows / chunksize) + 1\n        for i in range(chunks):\n            start_i = i * chunksize\n            end_i = min((i + 1) * chunksize, nrows)\n            if start_i >= end_i:\n                break\n\n            self.write_data_chunk(\n                rows,\n                indexes=[a[start_i:end_i] for a in bindexes],\n                mask=mask[start_i:end_i] if mask is not None else None,\n                values=[v[start_i:end_i] for v in bvalues])"}, {"code": "def kernels_pull_cli(self,\n                         kernel,\n                         kernel_opt=None,\n                         path=None,\n                         metadata=False):\n        \"\"\" client wrapper for kernels_pull\n        \"\"\"\n        kernel = kernel or kernel_opt\n        effective_path = self.kernels_pull(\n            kernel, path=path, metadata=metadata, quiet=False)\n        if metadata:\n            print('Source code and metadata downloaded to ' + effective_path)\n        else:\n            print('Source code downloaded to ' + effective_path)", "code_tokens": ["def", "kernels_pull_cli", "(", "self", ",", "kernel", ",", "kernel_opt", "=", "None", ",", "path", "=", "None", ",", "metadata", "=", "False", ")", ":", "kernel", "=", "kernel", "or", "kernel_opt", "effective_path", "=", "self", ".", "kernels_pull", "(", "kernel", ",", "path", "=", "path", ",", "metadata", "=", "metadata", ",", "quiet", "=", "False", ")", "if", "metadata", ":", "print", "(", "'Source code and metadata downloaded to '", "+", "effective_path", ")", "else", ":", "print", "(", "'Source code downloaded to '", "+", "effective_path", ")"], "original_string": "def kernels_pull_cli(self,\n                         kernel,\n                         kernel_opt=None,\n                         path=None,\n                         metadata=False):\n        \"\"\" client wrapper for kernels_pull\n        \"\"\"\n        kernel = kernel or kernel_opt\n        effective_path = self.kernels_pull(\n            kernel, path=path, metadata=metadata, quiet=False)\n        if metadata:\n            print('Source code and metadata downloaded to ' + effective_path)\n        else:\n            print('Source code downloaded to ' + effective_path)"}, {"code": "def relaxNGNewMemParserCtxt(buffer, size):\n    \"\"\"Create an XML RelaxNGs parse context for that memory buffer\n       expected to contain an XML RelaxNGs file. \"\"\"\n    ret = libxml2mod.xmlRelaxNGNewMemParserCtxt(buffer, size)\n    if ret is None:raise parserError('xmlRelaxNGNewMemParserCtxt() failed')\n    return relaxNgParserCtxt(_obj=ret)", "code_tokens": ["def", "relaxNGNewMemParserCtxt", "(", "buffer", ",", "size", ")", ":", "ret", "=", "libxml2mod", ".", "xmlRelaxNGNewMemParserCtxt", "(", "buffer", ",", "size", ")", "if", "ret", "is", "None", ":", "raise", "parserError", "(", "'xmlRelaxNGNewMemParserCtxt() failed'", ")", "return", "relaxNgParserCtxt", "(", "_obj", "=", "ret", ")"], "original_string": "def relaxNGNewMemParserCtxt(buffer, size):\n    \"\"\"Create an XML RelaxNGs parse context for that memory buffer\n       expected to contain an XML RelaxNGs file. \"\"\"\n    ret = libxml2mod.xmlRelaxNGNewMemParserCtxt(buffer, size)\n    if ret is None:raise parserError('xmlRelaxNGNewMemParserCtxt() failed')\n    return relaxNgParserCtxt(_obj=ret)"}, {"code": "def target(self, project_module):\n        \"\"\"Returns the project target corresponding to the 'project-module'.\"\"\"\n        assert isinstance(project_module, basestring)\n        if project_module not in self.module2target:\n            self.module2target[project_module] = \\\n                b2.build.targets.ProjectTarget(project_module, project_module,\n                              self.attribute(project_module, \"requirements\"))\n\n        return self.module2target[project_module]", "code_tokens": ["def", "target", "(", "self", ",", "project_module", ")", ":", "assert", "isinstance", "(", "project_module", ",", "basestring", ")", "if", "project_module", "not", "in", "self", ".", "module2target", ":", "self", ".", "module2target", "[", "project_module", "]", "=", "b2", ".", "build", ".", "targets", ".", "ProjectTarget", "(", "project_module", ",", "project_module", ",", "self", ".", "attribute", "(", "project_module", ",", "\"requirements\"", ")", ")", "return", "self", ".", "module2target", "[", "project_module", "]"], "original_string": "def target(self, project_module):\n        \"\"\"Returns the project target corresponding to the 'project-module'.\"\"\"\n        assert isinstance(project_module, basestring)\n        if project_module not in self.module2target:\n            self.module2target[project_module] = \\\n                b2.build.targets.ProjectTarget(project_module, project_module,\n                              self.attribute(project_module, \"requirements\"))\n\n        return self.module2target[project_module]"}, {"code": "def seek(self, pos=0):\n        \"\"\"Set the stream's file pointer to pos. Negative seeking\n           is forbidden.\n        \"\"\"\n        if pos - self.pos >= 0:\n            blocks, remainder = divmod(pos - self.pos, self.bufsize)\n            for i in range(blocks):\n                self.read(self.bufsize)\n            self.read(remainder)\n        else:\n            raise StreamError(\"seeking backwards is not allowed\")\n        return self.pos", "code_tokens": ["def", "seek", "(", "self", ",", "pos", "=", "0", ")", ":", "if", "pos", "-", "self", ".", "pos", ">=", "0", ":", "blocks", ",", "remainder", "=", "divmod", "(", "pos", "-", "self", ".", "pos", ",", "self", ".", "bufsize", ")", "for", "i", "in", "range", "(", "blocks", ")", ":", "self", ".", "read", "(", "self", ".", "bufsize", ")", "self", ".", "read", "(", "remainder", ")", "else", ":", "raise", "StreamError", "(", "\"seeking backwards is not allowed\"", ")", "return", "self", ".", "pos"], "original_string": "def seek(self, pos=0):\n        \"\"\"Set the stream's file pointer to pos. Negative seeking\n           is forbidden.\n        \"\"\"\n        if pos - self.pos >= 0:\n            blocks, remainder = divmod(pos - self.pos, self.bufsize)\n            for i in range(blocks):\n                self.read(self.bufsize)\n            self.read(remainder)\n        else:\n            raise StreamError(\"seeking backwards is not allowed\")\n        return self.pos"}, {"code": "def from_api_repr(cls, resource):\n        \"\"\"Factory: construct parameter from JSON resource.\n\n        :type resource: dict\n        :param resource: JSON mapping of parameter\n\n        :rtype: :class:`~google.cloud.bigquery.query.ScalarQueryParameter`\n        :returns: instance\n        \"\"\"\n        name = resource.get(\"name\")\n        type_ = resource[\"parameterType\"][\"type\"]\n        value = resource[\"parameterValue\"][\"value\"]\n        converted = _QUERY_PARAMS_FROM_JSON[type_](value, None)\n        return cls(name, type_, converted)", "code_tokens": ["def", "from_api_repr", "(", "cls", ",", "resource", ")", ":", "name", "=", "resource", ".", "get", "(", "\"name\"", ")", "type_", "=", "resource", "[", "\"parameterType\"", "]", "[", "\"type\"", "]", "value", "=", "resource", "[", "\"parameterValue\"", "]", "[", "\"value\"", "]", "converted", "=", "_QUERY_PARAMS_FROM_JSON", "[", "type_", "]", "(", "value", ",", "None", ")", "return", "cls", "(", "name", ",", "type_", ",", "converted", ")"], "original_string": "def from_api_repr(cls, resource):\n        \"\"\"Factory: construct parameter from JSON resource.\n\n        :type resource: dict\n        :param resource: JSON mapping of parameter\n\n        :rtype: :class:`~google.cloud.bigquery.query.ScalarQueryParameter`\n        :returns: instance\n        \"\"\"\n        name = resource.get(\"name\")\n        type_ = resource[\"parameterType\"][\"type\"]\n        value = resource[\"parameterValue\"][\"value\"]\n        converted = _QUERY_PARAMS_FROM_JSON[type_](value, None)\n        return cls(name, type_, converted)"}, {"code": "def free_memory(self):\r\n        \"\"\"Free memory signal.\"\"\"\r\n        self.main.free_memory()\r\n        QTimer.singleShot(self.INITIAL_FREE_MEMORY_TIME_TRIGGER,\r\n                          lambda: self.main.free_memory())\r\n        QTimer.singleShot(self.SECONDARY_FREE_MEMORY_TIME_TRIGGER,\r\n                          lambda: self.main.free_memory())", "code_tokens": ["def", "free_memory", "(", "self", ")", ":", "self", ".", "main", ".", "free_memory", "(", ")", "QTimer", ".", "singleShot", "(", "self", ".", "INITIAL_FREE_MEMORY_TIME_TRIGGER", ",", "lambda", ":", "self", ".", "main", ".", "free_memory", "(", ")", ")", "QTimer", ".", "singleShot", "(", "self", ".", "SECONDARY_FREE_MEMORY_TIME_TRIGGER", ",", "lambda", ":", "self", ".", "main", ".", "free_memory", "(", ")", ")"], "original_string": "def free_memory(self):\r\n        \"\"\"Free memory signal.\"\"\"\r\n        self.main.free_memory()\r\n        QTimer.singleShot(self.INITIAL_FREE_MEMORY_TIME_TRIGGER,\r\n                          lambda: self.main.free_memory())\r\n        QTimer.singleShot(self.SECONDARY_FREE_MEMORY_TIME_TRIGGER,\r\n                          lambda: self.main.free_memory())"}, {"code": "def run(ctx, commandline):\n    \"\"\"Run command with environment variables present.\"\"\"\n    file = ctx.obj['FILE']\n    dotenv_as_dict = dotenv_values(file)\n    if not commandline:\n        click.echo('No command given.')\n        exit(1)\n    ret = run_command(commandline, dotenv_as_dict)\n    exit(ret)", "code_tokens": ["def", "run", "(", "ctx", ",", "commandline", ")", ":", "file", "=", "ctx", ".", "obj", "[", "'FILE'", "]", "dotenv_as_dict", "=", "dotenv_values", "(", "file", ")", "if", "not", "commandline", ":", "click", ".", "echo", "(", "'No command given.'", ")", "exit", "(", "1", ")", "ret", "=", "run_command", "(", "commandline", ",", "dotenv_as_dict", ")", "exit", "(", "ret", ")"], "original_string": "def run(ctx, commandline):\n    \"\"\"Run command with environment variables present.\"\"\"\n    file = ctx.obj['FILE']\n    dotenv_as_dict = dotenv_values(file)\n    if not commandline:\n        click.echo('No command given.')\n        exit(1)\n    ret = run_command(commandline, dotenv_as_dict)\n    exit(ret)"}, {"code": "def setSystemProperty(cls, key, value):\n        \"\"\"\n        Set a Java system property, such as spark.executor.memory. This must\n        must be invoked before instantiating SparkContext.\n        \"\"\"\n        SparkContext._ensure_initialized()\n        SparkContext._jvm.java.lang.System.setProperty(key, value)", "code_tokens": ["def", "setSystemProperty", "(", "cls", ",", "key", ",", "value", ")", ":", "SparkContext", ".", "_ensure_initialized", "(", ")", "SparkContext", ".", "_jvm", ".", "java", ".", "lang", ".", "System", ".", "setProperty", "(", "key", ",", "value", ")"], "original_string": "def setSystemProperty(cls, key, value):\n        \"\"\"\n        Set a Java system property, such as spark.executor.memory. This must\n        must be invoked before instantiating SparkContext.\n        \"\"\"\n        SparkContext._ensure_initialized()\n        SparkContext._jvm.java.lang.System.setProperty(key, value)"}, {"code": "def wait_for_compute_global_operation(project_name, operation):\n    \"\"\"Poll for global compute operation until finished.\"\"\"\n    logger.info(\"wait_for_compute_global_operation: \"\n                \"Waiting for operation {} to finish...\".format(\n                    operation[\"name\"]))\n\n    for _ in range(MAX_POLLS):\n        result = compute.globalOperations().get(\n            project=project_name,\n            operation=operation[\"name\"],\n        ).execute()\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n\n        if result[\"status\"] == \"DONE\":\n            logger.info(\"wait_for_compute_global_operation: \"\n                        \"Operation done.\")\n            break\n\n        time.sleep(POLL_INTERVAL)\n\n    return result", "code_tokens": ["def", "wait_for_compute_global_operation", "(", "project_name", ",", "operation", ")", ":", "logger", ".", "info", "(", "\"wait_for_compute_global_operation: \"", "\"Waiting for operation {} to finish...\"", ".", "format", "(", "operation", "[", "\"name\"", "]", ")", ")", "for", "_", "in", "range", "(", "MAX_POLLS", ")", ":", "result", "=", "compute", ".", "globalOperations", "(", ")", ".", "get", "(", "project", "=", "project_name", ",", "operation", "=", "operation", "[", "\"name\"", "]", ",", ")", ".", "execute", "(", ")", "if", "\"error\"", "in", "result", ":", "raise", "Exception", "(", "result", "[", "\"error\"", "]", ")", "if", "result", "[", "\"status\"", "]", "==", "\"DONE\"", ":", "logger", ".", "info", "(", "\"wait_for_compute_global_operation: \"", "\"Operation done.\"", ")", "break", "time", ".", "sleep", "(", "POLL_INTERVAL", ")", "return", "result"], "original_string": "def wait_for_compute_global_operation(project_name, operation):\n    \"\"\"Poll for global compute operation until finished.\"\"\"\n    logger.info(\"wait_for_compute_global_operation: \"\n                \"Waiting for operation {} to finish...\".format(\n                    operation[\"name\"]))\n\n    for _ in range(MAX_POLLS):\n        result = compute.globalOperations().get(\n            project=project_name,\n            operation=operation[\"name\"],\n        ).execute()\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n\n        if result[\"status\"] == \"DONE\":\n            logger.info(\"wait_for_compute_global_operation: \"\n                        \"Operation done.\")\n            break\n\n        time.sleep(POLL_INTERVAL)\n\n    return result"}, {"code": "def is_iter(y, ignore=six.string_types):\n    '''\n    Test if an object is iterable, but not a string type.\n\n    Test if an object is an iterator or is iterable itself. By default this\n    does not return True for string objects.\n\n    The `ignore` argument defaults to a list of string types that are not\n    considered iterable. This can be used to also exclude things like\n    dictionaries or named tuples.\n\n    Based on https://bitbucket.org/petershinners/yter\n    '''\n\n    if ignore and isinstance(y, ignore):\n        return False\n    try:\n        iter(y)\n        return True\n    except TypeError:\n        return False", "code_tokens": ["def", "is_iter", "(", "y", ",", "ignore", "=", "six", ".", "string_types", ")", ":", "if", "ignore", "and", "isinstance", "(", "y", ",", "ignore", ")", ":", "return", "False", "try", ":", "iter", "(", "y", ")", "return", "True", "except", "TypeError", ":", "return", "False"], "original_string": "def is_iter(y, ignore=six.string_types):\n    '''\n    Test if an object is iterable, but not a string type.\n\n    Test if an object is an iterator or is iterable itself. By default this\n    does not return True for string objects.\n\n    The `ignore` argument defaults to a list of string types that are not\n    considered iterable. This can be used to also exclude things like\n    dictionaries or named tuples.\n\n    Based on https://bitbucket.org/petershinners/yter\n    '''\n\n    if ignore and isinstance(y, ignore):\n        return False\n    try:\n        iter(y)\n        return True\n    except TypeError:\n        return False"}, {"code": "def _unfuse(self):\n        \"\"\"Unfuses the fused RNN in to a stack of rnn cells.\"\"\"\n        assert not self._projection_size, \"_unfuse does not support projection layer yet!\"\n        assert not self._lstm_state_clip_min and not self._lstm_state_clip_max, \\\n                \"_unfuse does not support state clipping yet!\"\n        get_cell = {'rnn_relu': lambda **kwargs: rnn_cell.RNNCell(self._hidden_size,\n                                                                  activation='relu',\n                                                                  **kwargs),\n                    'rnn_tanh': lambda **kwargs: rnn_cell.RNNCell(self._hidden_size,\n                                                                  activation='tanh',\n                                                                  **kwargs),\n                    'lstm': lambda **kwargs: rnn_cell.LSTMCell(self._hidden_size,\n                                                               **kwargs),\n                    'gru': lambda **kwargs: rnn_cell.GRUCell(self._hidden_size,\n                                                             **kwargs)}[self._mode]\n\n        stack = rnn_cell.HybridSequentialRNNCell(prefix=self.prefix, params=self.params)\n        with stack.name_scope():\n            ni = self._input_size\n            for i in range(self._num_layers):\n                kwargs = {'input_size': ni,\n                          'i2h_weight_initializer': self._i2h_weight_initializer,\n                          'h2h_weight_initializer': self._h2h_weight_initializer,\n                          'i2h_bias_initializer': self._i2h_bias_initializer,\n                          'h2h_bias_initializer': self._h2h_bias_initializer}\n                if self._dir == 2:\n                    stack.add(rnn_cell.BidirectionalCell(\n                        get_cell(prefix='l%d_'%i, **kwargs),\n                        get_cell(prefix='r%d_'%i, **kwargs)))\n                else:\n                    stack.add(get_cell(prefix='l%d_'%i, **kwargs))\n\n                if self._dropout > 0 and i != self._num_layers - 1:\n                    stack.add(rnn_cell.DropoutCell(self._dropout))\n\n                ni = self._hidden_size * self._dir\n\n        return stack", "code_tokens": ["def", "_unfuse", "(", "self", ")", ":", "assert", "not", "self", ".", "_projection_size", ",", "\"_unfuse does not support projection layer yet!\"", "assert", "not", "self", ".", "_lstm_state_clip_min", "and", "not", "self", ".", "_lstm_state_clip_max", ",", "\"_unfuse does not support state clipping yet!\"", "get_cell", "=", "{", "'rnn_relu'", ":", "lambda", "*", "*", "kwargs", ":", "rnn_cell", ".", "RNNCell", "(", "self", ".", "_hidden_size", ",", "activation", "=", "'relu'", ",", "*", "*", "kwargs", ")", ",", "'rnn_tanh'", ":", "lambda", "*", "*", "kwargs", ":", "rnn_cell", ".", "RNNCell", "(", "self", ".", "_hidden_size", ",", "activation", "=", "'tanh'", ",", "*", "*", "kwargs", ")", ",", "'lstm'", ":", "lambda", "*", "*", "kwargs", ":", "rnn_cell", ".", "LSTMCell", "(", "self", ".", "_hidden_size", ",", "*", "*", "kwargs", ")", ",", "'gru'", ":", "lambda", "*", "*", "kwargs", ":", "rnn_cell", ".", "GRUCell", "(", "self", ".", "_hidden_size", ",", "*", "*", "kwargs", ")", "}", "[", "self", ".", "_mode", "]", "stack", "=", "rnn_cell", ".", "HybridSequentialRNNCell", "(", "prefix", "=", "self", ".", "prefix", ",", "params", "=", "self", ".", "params", ")", "with", "stack", ".", "name_scope", "(", ")", ":", "ni", "=", "self", ".", "_input_size", "for", "i", "in", "range", "(", "self", ".", "_num_layers", ")", ":", "kwargs", "=", "{", "'input_size'", ":", "ni", ",", "'i2h_weight_initializer'", ":", "self", ".", "_i2h_weight_initializer", ",", "'h2h_weight_initializer'", ":", "self", ".", "_h2h_weight_initializer", ",", "'i2h_bias_initializer'", ":", "self", ".", "_i2h_bias_initializer", ",", "'h2h_bias_initializer'", ":", "self", ".", "_h2h_bias_initializer", "}", "if", "self", ".", "_dir", "==", "2", ":", "stack", ".", "add", "(", "rnn_cell", ".", "BidirectionalCell", "(", "get_cell", "(", "prefix", "=", "'l%d_'", "%", "i", ",", "*", "*", "kwargs", ")", ",", "get_cell", "(", "prefix", "=", "'r%d_'", "%", "i", ",", "*", "*", "kwargs", ")", ")", ")", "else", ":", "stack", ".", "add", "(", "get_cell", "(", "prefix", "=", "'l%d_'", "%", "i", ",", "*", "*", "kwargs", ")", ")", "if", "self", ".", "_dropout", ">", "0", "and", "i", "!=", "self", ".", "_num_layers", "-", "1", ":", "stack", ".", "add", "(", "rnn_cell", ".", "DropoutCell", "(", "self", ".", "_dropout", ")", ")", "ni", "=", "self", ".", "_hidden_size", "*", "self", ".", "_dir", "return", "stack"], "original_string": "def _unfuse(self):\n        \"\"\"Unfuses the fused RNN in to a stack of rnn cells.\"\"\"\n        assert not self._projection_size, \"_unfuse does not support projection layer yet!\"\n        assert not self._lstm_state_clip_min and not self._lstm_state_clip_max, \\\n                \"_unfuse does not support state clipping yet!\"\n        get_cell = {'rnn_relu': lambda **kwargs: rnn_cell.RNNCell(self._hidden_size,\n                                                                  activation='relu',\n                                                                  **kwargs),\n                    'rnn_tanh': lambda **kwargs: rnn_cell.RNNCell(self._hidden_size,\n                                                                  activation='tanh',\n                                                                  **kwargs),\n                    'lstm': lambda **kwargs: rnn_cell.LSTMCell(self._hidden_size,\n                                                               **kwargs),\n                    'gru': lambda **kwargs: rnn_cell.GRUCell(self._hidden_size,\n                                                             **kwargs)}[self._mode]\n\n        stack = rnn_cell.HybridSequentialRNNCell(prefix=self.prefix, params=self.params)\n        with stack.name_scope():\n            ni = self._input_size\n            for i in range(self._num_layers):\n                kwargs = {'input_size': ni,\n                          'i2h_weight_initializer': self._i2h_weight_initializer,\n                          'h2h_weight_initializer': self._h2h_weight_initializer,\n                          'i2h_bias_initializer': self._i2h_bias_initializer,\n                          'h2h_bias_initializer': self._h2h_bias_initializer}\n                if self._dir == 2:\n                    stack.add(rnn_cell.BidirectionalCell(\n                        get_cell(prefix='l%d_'%i, **kwargs),\n                        get_cell(prefix='r%d_'%i, **kwargs)))\n                else:\n                    stack.add(get_cell(prefix='l%d_'%i, **kwargs))\n\n                if self._dropout > 0 and i != self._num_layers - 1:\n                    stack.add(rnn_cell.DropoutCell(self._dropout))\n\n                ni = self._hidden_size * self._dir\n\n        return stack"}, {"code": "def _add_session_callback(self, callback_obj, callback, one_shot, originator):\n        ''' Internal implementation for adding session callbacks.\n\n        Args:\n            callback_obj (SessionCallback) :\n                A session callback object that wraps a callable and is\n                passed to ``trigger_on_change``.\n\n            callback (callable) :\n                A callable to execute when session events happen.\n\n            one_shot (bool) :\n                Whether the callback should immediately auto-remove itself\n                after one execution.\n\n        Returns:\n            SessionCallback : passed in as ``callback_obj``.\n\n        Raises:\n            ValueError, if the callback has been previously added\n\n        '''\n        if one_shot:\n            @wraps(callback)\n            def remove_then_invoke(*args, **kwargs):\n                if callback_obj in self._session_callbacks:\n                    self._remove_session_callback(callback_obj, originator)\n                return callback(*args, **kwargs)\n            actual_callback = remove_then_invoke\n        else:\n            actual_callback = callback\n\n        callback_obj._callback = self._wrap_with_self_as_curdoc(actual_callback)\n        self._session_callbacks.add(callback_obj)\n        self._callback_objs_by_callable[originator][callback].add(callback_obj)\n\n        # emit event so the session is notified of the new callback\n        self._trigger_on_change(SessionCallbackAdded(self, callback_obj))\n\n        return callback_obj", "code_tokens": ["def", "_add_session_callback", "(", "self", ",", "callback_obj", ",", "callback", ",", "one_shot", ",", "originator", ")", ":", "if", "one_shot", ":", "@", "wraps", "(", "callback", ")", "def", "remove_then_invoke", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "if", "callback_obj", "in", "self", ".", "_session_callbacks", ":", "self", ".", "_remove_session_callback", "(", "callback_obj", ",", "originator", ")", "return", "callback", "(", "*", "args", ",", "*", "*", "kwargs", ")", "actual_callback", "=", "remove_then_invoke", "else", ":", "actual_callback", "=", "callback", "callback_obj", ".", "_callback", "=", "self", ".", "_wrap_with_self_as_curdoc", "(", "actual_callback", ")", "self", ".", "_session_callbacks", ".", "add", "(", "callback_obj", ")", "self", ".", "_callback_objs_by_callable", "[", "originator", "]", "[", "callback", "]", ".", "add", "(", "callback_obj", ")", "# emit event so the session is notified of the new callback", "self", ".", "_trigger_on_change", "(", "SessionCallbackAdded", "(", "self", ",", "callback_obj", ")", ")", "return", "callback_obj"], "original_string": "def _add_session_callback(self, callback_obj, callback, one_shot, originator):\n        ''' Internal implementation for adding session callbacks.\n\n        Args:\n            callback_obj (SessionCallback) :\n                A session callback object that wraps a callable and is\n                passed to ``trigger_on_change``.\n\n            callback (callable) :\n                A callable to execute when session events happen.\n\n            one_shot (bool) :\n                Whether the callback should immediately auto-remove itself\n                after one execution.\n\n        Returns:\n            SessionCallback : passed in as ``callback_obj``.\n\n        Raises:\n            ValueError, if the callback has been previously added\n\n        '''\n        if one_shot:\n            @wraps(callback)\n            def remove_then_invoke(*args, **kwargs):\n                if callback_obj in self._session_callbacks:\n                    self._remove_session_callback(callback_obj, originator)\n                return callback(*args, **kwargs)\n            actual_callback = remove_then_invoke\n        else:\n            actual_callback = callback\n\n        callback_obj._callback = self._wrap_with_self_as_curdoc(actual_callback)\n        self._session_callbacks.add(callback_obj)\n        self._callback_objs_by_callable[originator][callback].add(callback_obj)\n\n        # emit event so the session is notified of the new callback\n        self._trigger_on_change(SessionCallbackAdded(self, callback_obj))\n\n        return callback_obj"}, {"code": "def handle_split(self, asset, ratio):\n        \"\"\"\n        Update the position by the split ratio, and return the resulting\n        fractional share that will be converted into cash.\n\n        Returns the unused cash.\n        \"\"\"\n        if self.asset != asset:\n            raise Exception(\"updating split with the wrong asset!\")\n\n        # adjust the # of shares by the ratio\n        # (if we had 100 shares, and the ratio is 3,\n        #  we now have 33 shares)\n        # (old_share_count / ratio = new_share_count)\n        # (old_price * ratio = new_price)\n\n        # e.g., 33.333\n        raw_share_count = self.amount / float(ratio)\n\n        # e.g., 33\n        full_share_count = np.floor(raw_share_count)\n\n        # e.g., 0.333\n        fractional_share_count = raw_share_count - full_share_count\n\n        # adjust the cost basis to the nearest cent, e.g., 60.0\n        new_cost_basis = round(self.cost_basis * ratio, 2)\n\n        self.cost_basis = new_cost_basis\n        self.amount = full_share_count\n\n        return_cash = round(float(fractional_share_count * new_cost_basis), 2)\n\n        log.info(\"after split: \" + str(self))\n        log.info(\"returning cash: \" + str(return_cash))\n\n        # return the leftover cash, which will be converted into cash\n        # (rounded to the nearest cent)\n        return return_cash", "code_tokens": ["def", "handle_split", "(", "self", ",", "asset", ",", "ratio", ")", ":", "if", "self", ".", "asset", "!=", "asset", ":", "raise", "Exception", "(", "\"updating split with the wrong asset!\"", ")", "# adjust the # of shares by the ratio", "# (if we had 100 shares, and the ratio is 3,", "#  we now have 33 shares)", "# (old_share_count / ratio = new_share_count)", "# (old_price * ratio = new_price)", "# e.g., 33.333", "raw_share_count", "=", "self", ".", "amount", "/", "float", "(", "ratio", ")", "# e.g., 33", "full_share_count", "=", "np", ".", "floor", "(", "raw_share_count", ")", "# e.g., 0.333", "fractional_share_count", "=", "raw_share_count", "-", "full_share_count", "# adjust the cost basis to the nearest cent, e.g., 60.0", "new_cost_basis", "=", "round", "(", "self", ".", "cost_basis", "*", "ratio", ",", "2", ")", "self", ".", "cost_basis", "=", "new_cost_basis", "self", ".", "amount", "=", "full_share_count", "return_cash", "=", "round", "(", "float", "(", "fractional_share_count", "*", "new_cost_basis", ")", ",", "2", ")", "log", ".", "info", "(", "\"after split: \"", "+", "str", "(", "self", ")", ")", "log", ".", "info", "(", "\"returning cash: \"", "+", "str", "(", "return_cash", ")", ")", "# return the leftover cash, which will be converted into cash", "# (rounded to the nearest cent)", "return", "return_cash"], "original_string": "def handle_split(self, asset, ratio):\n        \"\"\"\n        Update the position by the split ratio, and return the resulting\n        fractional share that will be converted into cash.\n\n        Returns the unused cash.\n        \"\"\"\n        if self.asset != asset:\n            raise Exception(\"updating split with the wrong asset!\")\n\n        # adjust the # of shares by the ratio\n        # (if we had 100 shares, and the ratio is 3,\n        #  we now have 33 shares)\n        # (old_share_count / ratio = new_share_count)\n        # (old_price * ratio = new_price)\n\n        # e.g., 33.333\n        raw_share_count = self.amount / float(ratio)\n\n        # e.g., 33\n        full_share_count = np.floor(raw_share_count)\n\n        # e.g., 0.333\n        fractional_share_count = raw_share_count - full_share_count\n\n        # adjust the cost basis to the nearest cent, e.g., 60.0\n        new_cost_basis = round(self.cost_basis * ratio, 2)\n\n        self.cost_basis = new_cost_basis\n        self.amount = full_share_count\n\n        return_cash = round(float(fractional_share_count * new_cost_basis), 2)\n\n        log.info(\"after split: \" + str(self))\n        log.info(\"returning cash: \" + str(return_cash))\n\n        # return the leftover cash, which will be converted into cash\n        # (rounded to the nearest cent)\n        return return_cash"}, {"code": "def _init_posix(vars):\n    \"\"\"Initialize the module as appropriate for POSIX systems.\"\"\"\n    # load the installed Makefile:\n    makefile = get_makefile_filename()\n    try:\n        _parse_makefile(makefile, vars)\n    except IOError as e:\n        msg = \"invalid Python installation: unable to open %s\" % makefile\n        if hasattr(e, \"strerror\"):\n            msg = msg + \" (%s)\" % e.strerror\n        raise IOError(msg)\n    # load the installed pyconfig.h:\n    config_h = get_config_h_filename()\n    try:\n        with open(config_h) as f:\n            parse_config_h(f, vars)\n    except IOError as e:\n        msg = \"invalid Python installation: unable to open %s\" % config_h\n        if hasattr(e, \"strerror\"):\n            msg = msg + \" (%s)\" % e.strerror\n        raise IOError(msg)\n    # On AIX, there are wrong paths to the linker scripts in the Makefile\n    # -- these paths are relative to the Python source, but when installed\n    # the scripts are in another directory.\n    if _PYTHON_BUILD:\n        vars['LDSHARED'] = vars['BLDSHARED']", "code_tokens": ["def", "_init_posix", "(", "vars", ")", ":", "# load the installed Makefile:", "makefile", "=", "get_makefile_filename", "(", ")", "try", ":", "_parse_makefile", "(", "makefile", ",", "vars", ")", "except", "IOError", "as", "e", ":", "msg", "=", "\"invalid Python installation: unable to open %s\"", "%", "makefile", "if", "hasattr", "(", "e", ",", "\"strerror\"", ")", ":", "msg", "=", "msg", "+", "\" (%s)\"", "%", "e", ".", "strerror", "raise", "IOError", "(", "msg", ")", "# load the installed pyconfig.h:", "config_h", "=", "get_config_h_filename", "(", ")", "try", ":", "with", "open", "(", "config_h", ")", "as", "f", ":", "parse_config_h", "(", "f", ",", "vars", ")", "except", "IOError", "as", "e", ":", "msg", "=", "\"invalid Python installation: unable to open %s\"", "%", "config_h", "if", "hasattr", "(", "e", ",", "\"strerror\"", ")", ":", "msg", "=", "msg", "+", "\" (%s)\"", "%", "e", ".", "strerror", "raise", "IOError", "(", "msg", ")", "# On AIX, there are wrong paths to the linker scripts in the Makefile", "# -- these paths are relative to the Python source, but when installed", "# the scripts are in another directory.", "if", "_PYTHON_BUILD", ":", "vars", "[", "'LDSHARED'", "]", "=", "vars", "[", "'BLDSHARED'", "]"], "original_string": "def _init_posix(vars):\n    \"\"\"Initialize the module as appropriate for POSIX systems.\"\"\"\n    # load the installed Makefile:\n    makefile = get_makefile_filename()\n    try:\n        _parse_makefile(makefile, vars)\n    except IOError as e:\n        msg = \"invalid Python installation: unable to open %s\" % makefile\n        if hasattr(e, \"strerror\"):\n            msg = msg + \" (%s)\" % e.strerror\n        raise IOError(msg)\n    # load the installed pyconfig.h:\n    config_h = get_config_h_filename()\n    try:\n        with open(config_h) as f:\n            parse_config_h(f, vars)\n    except IOError as e:\n        msg = \"invalid Python installation: unable to open %s\" % config_h\n        if hasattr(e, \"strerror\"):\n            msg = msg + \" (%s)\" % e.strerror\n        raise IOError(msg)\n    # On AIX, there are wrong paths to the linker scripts in the Makefile\n    # -- these paths are relative to the Python source, but when installed\n    # the scripts are in another directory.\n    if _PYTHON_BUILD:\n        vars['LDSHARED'] = vars['BLDSHARED']"}, {"code": "def save(self, file:PathLikeOrBinaryStream= 'data_save.pkl')->None:\n        \"Save the `DataBunch` in `self.path/file`. `file` can be file-like (file or buffer)\"\n        if not getattr(self, 'label_list', False):\n            warn(\"Serializing the `DataBunch` only works when you created it using the data block API.\")\n            return\n        try_save(self.label_list, self.path, file)", "code_tokens": ["def", "save", "(", "self", ",", "file", ":", "PathLikeOrBinaryStream", "=", "'data_save.pkl'", ")", "->", "None", ":", "if", "not", "getattr", "(", "self", ",", "'label_list'", ",", "False", ")", ":", "warn", "(", "\"Serializing the `DataBunch` only works when you created it using the data block API.\"", ")", "return", "try_save", "(", "self", ".", "label_list", ",", "self", ".", "path", ",", "file", ")"], "original_string": "def save(self, file:PathLikeOrBinaryStream= 'data_save.pkl')->None:\n        \"Save the `DataBunch` in `self.path/file`. `file` can be file-like (file or buffer)\"\n        if not getattr(self, 'label_list', False):\n            warn(\"Serializing the `DataBunch` only works when you created it using the data block API.\")\n            return\n        try_save(self.label_list, self.path, file)"}, {"code": "def __prepare_dataset_parameter(self, dataset):\n        \"\"\"\n        Processes the dataset parameter for type correctness.\n        Returns it as an SFrame.\n        \"\"\"\n\n        # Translate the dataset argument into the proper type\n        if not isinstance(dataset, _SFrame):\n            def raise_dataset_type_exception():\n                raise TypeError(\"The dataset parameter must be either an SFrame, \"\n                                \"or a dictionary of (str : list) or (str : value).\")\n\n            if type(dataset) is dict:\n                if not all(type(k) is str for k in _six.iterkeys(dataset)):\n                    raise_dataset_type_exception()\n\n                if all(type(v) in (list, tuple, _array.array) for v in _six.itervalues(dataset)):\n                    dataset = _SFrame(dataset)\n                else:\n                    dataset = _SFrame({k : [v] for k, v in _six.iteritems(dataset)})\n            else:\n                raise_dataset_type_exception()\n\n        return dataset", "code_tokens": ["def", "__prepare_dataset_parameter", "(", "self", ",", "dataset", ")", ":", "# Translate the dataset argument into the proper type", "if", "not", "isinstance", "(", "dataset", ",", "_SFrame", ")", ":", "def", "raise_dataset_type_exception", "(", ")", ":", "raise", "TypeError", "(", "\"The dataset parameter must be either an SFrame, \"", "\"or a dictionary of (str : list) or (str : value).\"", ")", "if", "type", "(", "dataset", ")", "is", "dict", ":", "if", "not", "all", "(", "type", "(", "k", ")", "is", "str", "for", "k", "in", "_six", ".", "iterkeys", "(", "dataset", ")", ")", ":", "raise_dataset_type_exception", "(", ")", "if", "all", "(", "type", "(", "v", ")", "in", "(", "list", ",", "tuple", ",", "_array", ".", "array", ")", "for", "v", "in", "_six", ".", "itervalues", "(", "dataset", ")", ")", ":", "dataset", "=", "_SFrame", "(", "dataset", ")", "else", ":", "dataset", "=", "_SFrame", "(", "{", "k", ":", "[", "v", "]", "for", "k", ",", "v", "in", "_six", ".", "iteritems", "(", "dataset", ")", "}", ")", "else", ":", "raise_dataset_type_exception", "(", ")", "return", "dataset"], "original_string": "def __prepare_dataset_parameter(self, dataset):\n        \"\"\"\n        Processes the dataset parameter for type correctness.\n        Returns it as an SFrame.\n        \"\"\"\n\n        # Translate the dataset argument into the proper type\n        if not isinstance(dataset, _SFrame):\n            def raise_dataset_type_exception():\n                raise TypeError(\"The dataset parameter must be either an SFrame, \"\n                                \"or a dictionary of (str : list) or (str : value).\")\n\n            if type(dataset) is dict:\n                if not all(type(k) is str for k in _six.iterkeys(dataset)):\n                    raise_dataset_type_exception()\n\n                if all(type(v) in (list, tuple, _array.array) for v in _six.itervalues(dataset)):\n                    dataset = _SFrame(dataset)\n                else:\n                    dataset = _SFrame({k : [v] for k, v in _six.iteritems(dataset)})\n            else:\n                raise_dataset_type_exception()\n\n        return dataset"}, {"code": "def get_storage_policies(profile_manager, policy_names=None,\n                         get_all_policies=False):\n    '''\n    Returns a list of the storage policies, filtered by name.\n\n    profile_manager\n        Reference to the profile manager.\n\n    policy_names\n        List of policy names to filter by.\n        Default is None.\n\n    get_all_policies\n        Flag specifying to return all policies, regardless of the specified\n        filter.\n    '''\n    res_type = pbm.profile.ResourceType(\n        resourceType=pbm.profile.ResourceTypeEnum.STORAGE)\n    try:\n        policy_ids = profile_manager.QueryProfile(res_type)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise VMwareApiError('Not enough permissions. Required privilege: '\n                             '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise VMwareRuntimeError(exc.msg)\n    log.trace('policy_ids = %s', policy_ids)\n    # More policies are returned so we need to filter again\n    policies = [p for p in get_policies_by_id(profile_manager, policy_ids)\n                if p.resourceType.resourceType ==\n                pbm.profile.ResourceTypeEnum.STORAGE]\n    if get_all_policies:\n        return policies\n    if not policy_names:\n        policy_names = []\n    return [p for p in policies if p.name in policy_names]", "code_tokens": ["def", "get_storage_policies", "(", "profile_manager", ",", "policy_names", "=", "None", ",", "get_all_policies", "=", "False", ")", ":", "res_type", "=", "pbm", ".", "profile", ".", "ResourceType", "(", "resourceType", "=", "pbm", ".", "profile", ".", "ResourceTypeEnum", ".", "STORAGE", ")", "try", ":", "policy_ids", "=", "profile_manager", ".", "QueryProfile", "(", "res_type", ")", "except", "vim", ".", "fault", ".", "NoPermission", "as", "exc", ":", "log", ".", "exception", "(", "exc", ")", "raise", "VMwareApiError", "(", "'Not enough permissions. Required privilege: '", "'{0}'", ".", "format", "(", "exc", ".", "privilegeId", ")", ")", "except", "vim", ".", "fault", ".", "VimFault", "as", "exc", ":", "log", ".", "exception", "(", "exc", ")", "raise", "VMwareApiError", "(", "exc", ".", "msg", ")", "except", "vmodl", ".", "RuntimeFault", "as", "exc", ":", "log", ".", "exception", "(", "exc", ")", "raise", "VMwareRuntimeError", "(", "exc", ".", "msg", ")", "log", ".", "trace", "(", "'policy_ids = %s'", ",", "policy_ids", ")", "# More policies are returned so we need to filter again", "policies", "=", "[", "p", "for", "p", "in", "get_policies_by_id", "(", "profile_manager", ",", "policy_ids", ")", "if", "p", ".", "resourceType", ".", "resourceType", "==", "pbm", ".", "profile", ".", "ResourceTypeEnum", ".", "STORAGE", "]", "if", "get_all_policies", ":", "return", "policies", "if", "not", "policy_names", ":", "policy_names", "=", "[", "]", "return", "[", "p", "for", "p", "in", "policies", "if", "p", ".", "name", "in", "policy_names", "]"], "original_string": "def get_storage_policies(profile_manager, policy_names=None,\n                         get_all_policies=False):\n    '''\n    Returns a list of the storage policies, filtered by name.\n\n    profile_manager\n        Reference to the profile manager.\n\n    policy_names\n        List of policy names to filter by.\n        Default is None.\n\n    get_all_policies\n        Flag specifying to return all policies, regardless of the specified\n        filter.\n    '''\n    res_type = pbm.profile.ResourceType(\n        resourceType=pbm.profile.ResourceTypeEnum.STORAGE)\n    try:\n        policy_ids = profile_manager.QueryProfile(res_type)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise VMwareApiError('Not enough permissions. Required privilege: '\n                             '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise VMwareRuntimeError(exc.msg)\n    log.trace('policy_ids = %s', policy_ids)\n    # More policies are returned so we need to filter again\n    policies = [p for p in get_policies_by_id(profile_manager, policy_ids)\n                if p.resourceType.resourceType ==\n                pbm.profile.ResourceTypeEnum.STORAGE]\n    if get_all_policies:\n        return policies\n    if not policy_names:\n        policy_names = []\n    return [p for p in policies if p.name in policy_names]"}, {"code": "def error_info():\n    \"\"\"Return information about failed tasks.\"\"\"\n    worker = global_worker\n    worker.check_connected()\n    return (global_state.error_messages(driver_id=worker.task_driver_id) +\n            global_state.error_messages(driver_id=DriverID.nil()))", "code_tokens": ["def", "error_info", "(", ")", ":", "worker", "=", "global_worker", "worker", ".", "check_connected", "(", ")", "return", "(", "global_state", ".", "error_messages", "(", "driver_id", "=", "worker", ".", "task_driver_id", ")", "+", "global_state", ".", "error_messages", "(", "driver_id", "=", "DriverID", ".", "nil", "(", ")", ")", ")"], "original_string": "def error_info():\n    \"\"\"Return information about failed tasks.\"\"\"\n    worker = global_worker\n    worker.check_connected()\n    return (global_state.error_messages(driver_id=worker.task_driver_id) +\n            global_state.error_messages(driver_id=DriverID.nil()))"}, {"code": "def last_modified(self) -> Optional[datetime.datetime]:\n        \"\"\"The value of Last-Modified HTTP header, or None.\n\n        This header is represented as a `datetime` object.\n        \"\"\"\n        httpdate = self._headers.get(hdrs.LAST_MODIFIED)\n        if httpdate is not None:\n            timetuple = parsedate(httpdate)\n            if timetuple is not None:\n                return datetime.datetime(*timetuple[:6],\n                                         tzinfo=datetime.timezone.utc)\n        return None", "code_tokens": ["def", "last_modified", "(", "self", ")", "->", "Optional", "[", "datetime", ".", "datetime", "]", ":", "httpdate", "=", "self", ".", "_headers", ".", "get", "(", "hdrs", ".", "LAST_MODIFIED", ")", "if", "httpdate", "is", "not", "None", ":", "timetuple", "=", "parsedate", "(", "httpdate", ")", "if", "timetuple", "is", "not", "None", ":", "return", "datetime", ".", "datetime", "(", "*", "timetuple", "[", ":", "6", "]", ",", "tzinfo", "=", "datetime", ".", "timezone", ".", "utc", ")", "return", "None"], "original_string": "def last_modified(self) -> Optional[datetime.datetime]:\n        \"\"\"The value of Last-Modified HTTP header, or None.\n\n        This header is represented as a `datetime` object.\n        \"\"\"\n        httpdate = self._headers.get(hdrs.LAST_MODIFIED)\n        if httpdate is not None:\n            timetuple = parsedate(httpdate)\n            if timetuple is not None:\n                return datetime.datetime(*timetuple[:6],\n                                         tzinfo=datetime.timezone.utc)\n        return None"}, {"code": "def _prefix_from_ip_int(cls, ip_int):\n        \"\"\"Return prefix length from the bitwise netmask.\n\n        Args:\n            ip_int: An integer, the netmask in expanded bitwise format\n\n        Returns:\n            An integer, the prefix length.\n\n        Raises:\n            ValueError: If the input intermingles zeroes & ones\n        \"\"\"\n        trailing_zeroes = _count_righthand_zero_bits(ip_int,\n                                                     cls._max_prefixlen)\n        prefixlen = cls._max_prefixlen - trailing_zeroes\n        leading_ones = ip_int >> trailing_zeroes\n        all_ones = (1 << prefixlen) - 1\n        if leading_ones != all_ones:\n            byteslen = cls._max_prefixlen // 8\n            details = _compat_to_bytes(ip_int, byteslen, 'big')\n            msg = 'Netmask pattern %r mixes zeroes & ones'\n            raise ValueError(msg % details)\n        return prefixlen", "code_tokens": ["def", "_prefix_from_ip_int", "(", "cls", ",", "ip_int", ")", ":", "trailing_zeroes", "=", "_count_righthand_zero_bits", "(", "ip_int", ",", "cls", ".", "_max_prefixlen", ")", "prefixlen", "=", "cls", ".", "_max_prefixlen", "-", "trailing_zeroes", "leading_ones", "=", "ip_int", ">>", "trailing_zeroes", "all_ones", "=", "(", "1", "<<", "prefixlen", ")", "-", "1", "if", "leading_ones", "!=", "all_ones", ":", "byteslen", "=", "cls", ".", "_max_prefixlen", "//", "8", "details", "=", "_compat_to_bytes", "(", "ip_int", ",", "byteslen", ",", "'big'", ")", "msg", "=", "'Netmask pattern %r mixes zeroes & ones'", "raise", "ValueError", "(", "msg", "%", "details", ")", "return", "prefixlen"], "original_string": "def _prefix_from_ip_int(cls, ip_int):\n        \"\"\"Return prefix length from the bitwise netmask.\n\n        Args:\n            ip_int: An integer, the netmask in expanded bitwise format\n\n        Returns:\n            An integer, the prefix length.\n\n        Raises:\n            ValueError: If the input intermingles zeroes & ones\n        \"\"\"\n        trailing_zeroes = _count_righthand_zero_bits(ip_int,\n                                                     cls._max_prefixlen)\n        prefixlen = cls._max_prefixlen - trailing_zeroes\n        leading_ones = ip_int >> trailing_zeroes\n        all_ones = (1 << prefixlen) - 1\n        if leading_ones != all_ones:\n            byteslen = cls._max_prefixlen // 8\n            details = _compat_to_bytes(ip_int, byteslen, 'big')\n            msg = 'Netmask pattern %r mixes zeroes & ones'\n            raise ValueError(msg % details)\n        return prefixlen"}, {"code": "def _parse_example_spec(self):\n    \"\"\"Returns a `tf.Example` parsing spec as dict.\"\"\"\n    height, width = image_util.get_expected_image_size(self.module_spec)\n    input_shape = [height, width, 3]\n    return {self.key: tf_v1.FixedLenFeature(input_shape, tf.float32)}", "code_tokens": ["def", "_parse_example_spec", "(", "self", ")", ":", "height", ",", "width", "=", "image_util", ".", "get_expected_image_size", "(", "self", ".", "module_spec", ")", "input_shape", "=", "[", "height", ",", "width", ",", "3", "]", "return", "{", "self", ".", "key", ":", "tf_v1", ".", "FixedLenFeature", "(", "input_shape", ",", "tf", ".", "float32", ")", "}"], "original_string": "def _parse_example_spec(self):\n    \"\"\"Returns a `tf.Example` parsing spec as dict.\"\"\"\n    height, width = image_util.get_expected_image_size(self.module_spec)\n    input_shape = [height, width, 3]\n    return {self.key: tf_v1.FixedLenFeature(input_shape, tf.float32)}"}, {"code": "def start(vm, options=None, key='uuid'):\n    '''\n    Start a vm\n\n    vm : string\n        vm to be started\n    options : string\n        optional additional options\n    key : string [uuid|alias|hostname]\n        value type of 'vm' parameter\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vmadm.start 186da9ab-7392-4f55-91a5-b8f1fe770543\n        salt '*' vmadm.start 186da9ab-7392-4f55-91a5-b8f1fe770543 'order=c,once=d cdrom=/path/to/image.iso,ide'\n        salt '*' vmadm.start vm=nacl key=alias\n        salt '*' vmadm.start vm=nina.example.org key=hostname\n    '''\n    ret = {}\n    if key not in ['uuid', 'alias', 'hostname']:\n        ret['Error'] = 'Key must be either uuid, alias or hostname'\n        return ret\n    vm = lookup('{0}={1}'.format(key, vm), one=True)\n    if 'Error' in vm:\n        return vm\n    # vmadm start <uuid> [option=value ...]\n    cmd = 'vmadm start {uuid} {options}'.format(\n        uuid=vm,\n        options=options if options else ''\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = res['stderr'] if 'stderr' in res else _exit_status(retcode)\n        return ret\n    return True", "code_tokens": ["def", "start", "(", "vm", ",", "options", "=", "None", ",", "key", "=", "'uuid'", ")", ":", "ret", "=", "{", "}", "if", "key", "not", "in", "[", "'uuid'", ",", "'alias'", ",", "'hostname'", "]", ":", "ret", "[", "'Error'", "]", "=", "'Key must be either uuid, alias or hostname'", "return", "ret", "vm", "=", "lookup", "(", "'{0}={1}'", ".", "format", "(", "key", ",", "vm", ")", ",", "one", "=", "True", ")", "if", "'Error'", "in", "vm", ":", "return", "vm", "# vmadm start <uuid> [option=value ...]", "cmd", "=", "'vmadm start {uuid} {options}'", ".", "format", "(", "uuid", "=", "vm", ",", "options", "=", "options", "if", "options", "else", "''", ")", "res", "=", "__salt__", "[", "'cmd.run_all'", "]", "(", "cmd", ")", "retcode", "=", "res", "[", "'retcode'", "]", "if", "retcode", "!=", "0", ":", "ret", "[", "'Error'", "]", "=", "res", "[", "'stderr'", "]", "if", "'stderr'", "in", "res", "else", "_exit_status", "(", "retcode", ")", "return", "ret", "return", "True"], "original_string": "def start(vm, options=None, key='uuid'):\n    '''\n    Start a vm\n\n    vm : string\n        vm to be started\n    options : string\n        optional additional options\n    key : string [uuid|alias|hostname]\n        value type of 'vm' parameter\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vmadm.start 186da9ab-7392-4f55-91a5-b8f1fe770543\n        salt '*' vmadm.start 186da9ab-7392-4f55-91a5-b8f1fe770543 'order=c,once=d cdrom=/path/to/image.iso,ide'\n        salt '*' vmadm.start vm=nacl key=alias\n        salt '*' vmadm.start vm=nina.example.org key=hostname\n    '''\n    ret = {}\n    if key not in ['uuid', 'alias', 'hostname']:\n        ret['Error'] = 'Key must be either uuid, alias or hostname'\n        return ret\n    vm = lookup('{0}={1}'.format(key, vm), one=True)\n    if 'Error' in vm:\n        return vm\n    # vmadm start <uuid> [option=value ...]\n    cmd = 'vmadm start {uuid} {options}'.format(\n        uuid=vm,\n        options=options if options else ''\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = res['stderr'] if 'stderr' in res else _exit_status(retcode)\n        return ret\n    return True"}, {"code": "def put_string(self, content, destination_s3_path, **kwargs):\n        \"\"\"\n        Put a string to an S3 path.\n        :param content: Data str\n        :param destination_s3_path: URL for target S3 location\n        :param kwargs: Keyword arguments are passed to the boto3 function `put_object`\n        \"\"\"\n        self._check_deprecated_argument(**kwargs)\n        (bucket, key) = self._path_to_bucket_and_key(destination_s3_path)\n\n        # put the file\n        self.s3.meta.client.put_object(\n            Key=key, Bucket=bucket, Body=content, **kwargs)", "code_tokens": ["def", "put_string", "(", "self", ",", "content", ",", "destination_s3_path", ",", "*", "*", "kwargs", ")", ":", "self", ".", "_check_deprecated_argument", "(", "*", "*", "kwargs", ")", "(", "bucket", ",", "key", ")", "=", "self", ".", "_path_to_bucket_and_key", "(", "destination_s3_path", ")", "# put the file", "self", ".", "s3", ".", "meta", ".", "client", ".", "put_object", "(", "Key", "=", "key", ",", "Bucket", "=", "bucket", ",", "Body", "=", "content", ",", "*", "*", "kwargs", ")"], "original_string": "def put_string(self, content, destination_s3_path, **kwargs):\n        \"\"\"\n        Put a string to an S3 path.\n        :param content: Data str\n        :param destination_s3_path: URL for target S3 location\n        :param kwargs: Keyword arguments are passed to the boto3 function `put_object`\n        \"\"\"\n        self._check_deprecated_argument(**kwargs)\n        (bucket, key) = self._path_to_bucket_and_key(destination_s3_path)\n\n        # put the file\n        self.s3.meta.client.put_object(\n            Key=key, Bucket=bucket, Body=content, **kwargs)"}, {"code": "def add_sockets(self, sockets: Iterable[socket.socket]) -> None:\n        \"\"\"Makes this server start accepting connections on the given sockets.\n\n        The ``sockets`` parameter is a list of socket objects such as\n        those returned by `~tornado.netutil.bind_sockets`.\n        `add_sockets` is typically used in combination with that\n        method and `tornado.process.fork_processes` to provide greater\n        control over the initialization of a multi-process server.\n        \"\"\"\n        for sock in sockets:\n            self._sockets[sock.fileno()] = sock\n            self._handlers[sock.fileno()] = add_accept_handler(\n                sock, self._handle_connection\n            )", "code_tokens": ["def", "add_sockets", "(", "self", ",", "sockets", ":", "Iterable", "[", "socket", ".", "socket", "]", ")", "->", "None", ":", "for", "sock", "in", "sockets", ":", "self", ".", "_sockets", "[", "sock", ".", "fileno", "(", ")", "]", "=", "sock", "self", ".", "_handlers", "[", "sock", ".", "fileno", "(", ")", "]", "=", "add_accept_handler", "(", "sock", ",", "self", ".", "_handle_connection", ")"], "original_string": "def add_sockets(self, sockets: Iterable[socket.socket]) -> None:\n        \"\"\"Makes this server start accepting connections on the given sockets.\n\n        The ``sockets`` parameter is a list of socket objects such as\n        those returned by `~tornado.netutil.bind_sockets`.\n        `add_sockets` is typically used in combination with that\n        method and `tornado.process.fork_processes` to provide greater\n        control over the initialization of a multi-process server.\n        \"\"\"\n        for sock in sockets:\n            self._sockets[sock.fileno()] = sock\n            self._handlers[sock.fileno()] = add_accept_handler(\n                sock, self._handle_connection\n            )"}, {"code": "def load_data(self, sess, inputs, state_inputs):\n        \"\"\"Bulk loads the specified inputs into device memory.\n\n        The shape of the inputs must conform to the shapes of the input\n        placeholders this optimizer was constructed with.\n\n        The data is split equally across all the devices. If the data is not\n        evenly divisible by the batch size, excess data will be discarded.\n\n        Args:\n            sess: TensorFlow session.\n            inputs: List of arrays matching the input placeholders, of shape\n                [BATCH_SIZE, ...].\n            state_inputs: List of RNN input arrays. These arrays have size\n                [BATCH_SIZE / MAX_SEQ_LEN, ...].\n\n        Returns:\n            The number of tuples loaded per device.\n        \"\"\"\n\n        if log_once(\"load_data\"):\n            logger.info(\n                \"Training on concatenated sample batches:\\n\\n{}\\n\".format(\n                    summarize({\n                        \"placeholders\": self.loss_inputs,\n                        \"inputs\": inputs,\n                        \"state_inputs\": state_inputs\n                    })))\n\n        feed_dict = {}\n        assert len(self.loss_inputs) == len(inputs + state_inputs), \\\n            (self.loss_inputs, inputs, state_inputs)\n\n        # Let's suppose we have the following input data, and 2 devices:\n        # 1 2 3 4 5 6 7                              <- state inputs shape\n        # A A A B B B C C C D D D E E E F F F G G G  <- inputs shape\n        # The data is truncated and split across devices as follows:\n        # |---| seq len = 3\n        # |---------------------------------| seq batch size = 6 seqs\n        # |----------------| per device batch size = 9 tuples\n\n        if len(state_inputs) > 0:\n            smallest_array = state_inputs[0]\n            seq_len = len(inputs[0]) // len(state_inputs[0])\n            self._loaded_max_seq_len = seq_len\n        else:\n            smallest_array = inputs[0]\n            self._loaded_max_seq_len = 1\n\n        sequences_per_minibatch = (\n            self.max_per_device_batch_size // self._loaded_max_seq_len * len(\n                self.devices))\n        if sequences_per_minibatch < 1:\n            logger.warn(\n                (\"Target minibatch size is {}, however the rollout sequence \"\n                 \"length is {}, hence the minibatch size will be raised to \"\n                 \"{}.\").format(self.max_per_device_batch_size,\n                               self._loaded_max_seq_len,\n                               self._loaded_max_seq_len * len(self.devices)))\n            sequences_per_minibatch = 1\n\n        if len(smallest_array) < sequences_per_minibatch:\n            # Dynamically shrink the batch size if insufficient data\n            sequences_per_minibatch = make_divisible_by(\n                len(smallest_array), len(self.devices))\n\n        if log_once(\"data_slicing\"):\n            logger.info(\n                (\"Divided {} rollout sequences, each of length {}, among \"\n                 \"{} devices.\").format(\n                     len(smallest_array), self._loaded_max_seq_len,\n                     len(self.devices)))\n\n        if sequences_per_minibatch < len(self.devices):\n            raise ValueError(\n                \"Must load at least 1 tuple sequence per device. Try \"\n                \"increasing `sgd_minibatch_size` or reducing `max_seq_len` \"\n                \"to ensure that at least one sequence fits per device.\")\n        self._loaded_per_device_batch_size = (sequences_per_minibatch // len(\n            self.devices) * self._loaded_max_seq_len)\n\n        if len(state_inputs) > 0:\n            # First truncate the RNN state arrays to the sequences_per_minib.\n            state_inputs = [\n                make_divisible_by(arr, sequences_per_minibatch)\n                for arr in state_inputs\n            ]\n            # Then truncate the data inputs to match\n            inputs = [arr[:len(state_inputs[0]) * seq_len] for arr in inputs]\n            assert len(state_inputs[0]) * seq_len == len(inputs[0]), \\\n                (len(state_inputs[0]), sequences_per_minibatch, seq_len,\n                 len(inputs[0]))\n            for ph, arr in zip(self.loss_inputs, inputs + state_inputs):\n                feed_dict[ph] = arr\n            truncated_len = len(inputs[0])\n        else:\n            for ph, arr in zip(self.loss_inputs, inputs + state_inputs):\n                truncated_arr = make_divisible_by(arr, sequences_per_minibatch)\n                feed_dict[ph] = truncated_arr\n                truncated_len = len(truncated_arr)\n\n        sess.run([t.init_op for t in self._towers], feed_dict=feed_dict)\n\n        self.num_tuples_loaded = truncated_len\n        tuples_per_device = truncated_len // len(self.devices)\n        assert tuples_per_device > 0, \"No data loaded?\"\n        assert tuples_per_device % self._loaded_per_device_batch_size == 0\n        return tuples_per_device", "code_tokens": ["def", "load_data", "(", "self", ",", "sess", ",", "inputs", ",", "state_inputs", ")", ":", "if", "log_once", "(", "\"load_data\"", ")", ":", "logger", ".", "info", "(", "\"Training on concatenated sample batches:\\n\\n{}\\n\"", ".", "format", "(", "summarize", "(", "{", "\"placeholders\"", ":", "self", ".", "loss_inputs", ",", "\"inputs\"", ":", "inputs", ",", "\"state_inputs\"", ":", "state_inputs", "}", ")", ")", ")", "feed_dict", "=", "{", "}", "assert", "len", "(", "self", ".", "loss_inputs", ")", "==", "len", "(", "inputs", "+", "state_inputs", ")", ",", "(", "self", ".", "loss_inputs", ",", "inputs", ",", "state_inputs", ")", "# Let's suppose we have the following input data, and 2 devices:", "# 1 2 3 4 5 6 7                              <- state inputs shape", "# A A A B B B C C C D D D E E E F F F G G G  <- inputs shape", "# The data is truncated and split across devices as follows:", "# |---| seq len = 3", "# |---------------------------------| seq batch size = 6 seqs", "# |----------------| per device batch size = 9 tuples", "if", "len", "(", "state_inputs", ")", ">", "0", ":", "smallest_array", "=", "state_inputs", "[", "0", "]", "seq_len", "=", "len", "(", "inputs", "[", "0", "]", ")", "//", "len", "(", "state_inputs", "[", "0", "]", ")", "self", ".", "_loaded_max_seq_len", "=", "seq_len", "else", ":", "smallest_array", "=", "inputs", "[", "0", "]", "self", ".", "_loaded_max_seq_len", "=", "1", "sequences_per_minibatch", "=", "(", "self", ".", "max_per_device_batch_size", "//", "self", ".", "_loaded_max_seq_len", "*", "len", "(", "self", ".", "devices", ")", ")", "if", "sequences_per_minibatch", "<", "1", ":", "logger", ".", "warn", "(", "(", "\"Target minibatch size is {}, however the rollout sequence \"", "\"length is {}, hence the minibatch size will be raised to \"", "\"{}.\"", ")", ".", "format", "(", "self", ".", "max_per_device_batch_size", ",", "self", ".", "_loaded_max_seq_len", ",", "self", ".", "_loaded_max_seq_len", "*", "len", "(", "self", ".", "devices", ")", ")", ")", "sequences_per_minibatch", "=", "1", "if", "len", "(", "smallest_array", ")", "<", "sequences_per_minibatch", ":", "# Dynamically shrink the batch size if insufficient data", "sequences_per_minibatch", "=", "make_divisible_by", "(", "len", "(", "smallest_array", ")", ",", "len", "(", "self", ".", "devices", ")", ")", "if", "log_once", "(", "\"data_slicing\"", ")", ":", "logger", ".", "info", "(", "(", "\"Divided {} rollout sequences, each of length {}, among \"", "\"{} devices.\"", ")", ".", "format", "(", "len", "(", "smallest_array", ")", ",", "self", ".", "_loaded_max_seq_len", ",", "len", "(", "self", ".", "devices", ")", ")", ")", "if", "sequences_per_minibatch", "<", "len", "(", "self", ".", "devices", ")", ":", "raise", "ValueError", "(", "\"Must load at least 1 tuple sequence per device. Try \"", "\"increasing `sgd_minibatch_size` or reducing `max_seq_len` \"", "\"to ensure that at least one sequence fits per device.\"", ")", "self", ".", "_loaded_per_device_batch_size", "=", "(", "sequences_per_minibatch", "//", "len", "(", "self", ".", "devices", ")", "*", "self", ".", "_loaded_max_seq_len", ")", "if", "len", "(", "state_inputs", ")", ">", "0", ":", "# First truncate the RNN state arrays to the sequences_per_minib.", "state_inputs", "=", "[", "make_divisible_by", "(", "arr", ",", "sequences_per_minibatch", ")", "for", "arr", "in", "state_inputs", "]", "# Then truncate the data inputs to match", "inputs", "=", "[", "arr", "[", ":", "len", "(", "state_inputs", "[", "0", "]", ")", "*", "seq_len", "]", "for", "arr", "in", "inputs", "]", "assert", "len", "(", "state_inputs", "[", "0", "]", ")", "*", "seq_len", "==", "len", "(", "inputs", "[", "0", "]", ")", ",", "(", "len", "(", "state_inputs", "[", "0", "]", ")", ",", "sequences_per_minibatch", ",", "seq_len", ",", "len", "(", "inputs", "[", "0", "]", ")", ")", "for", "ph", ",", "arr", "in", "zip", "(", "self", ".", "loss_inputs", ",", "inputs", "+", "state_inputs", ")", ":", "feed_dict", "[", "ph", "]", "=", "arr", "truncated_len", "=", "len", "(", "inputs", "[", "0", "]", ")", "else", ":", "for", "ph", ",", "arr", "in", "zip", "(", "self", ".", "loss_inputs", ",", "inputs", "+", "state_inputs", ")", ":", "truncated_arr", "=", "make_divisible_by", "(", "arr", ",", "sequences_per_minibatch", ")", "feed_dict", "[", "ph", "]", "=", "truncated_arr", "truncated_len", "=", "len", "(", "truncated_arr", ")", "sess", ".", "run", "(", "[", "t", ".", "init_op", "for", "t", "in", "self", ".", "_towers", "]", ",", "feed_dict", "=", "feed_dict", ")", "self", ".", "num_tuples_loaded", "=", "truncated_len", "tuples_per_device", "=", "truncated_len", "//", "len", "(", "self", ".", "devices", ")", "assert", "tuples_per_device", ">", "0", ",", "\"No data loaded?\"", "assert", "tuples_per_device", "%", "self", ".", "_loaded_per_device_batch_size", "==", "0", "return", "tuples_per_device"], "original_string": "def load_data(self, sess, inputs, state_inputs):\n        \"\"\"Bulk loads the specified inputs into device memory.\n\n        The shape of the inputs must conform to the shapes of the input\n        placeholders this optimizer was constructed with.\n\n        The data is split equally across all the devices. If the data is not\n        evenly divisible by the batch size, excess data will be discarded.\n\n        Args:\n            sess: TensorFlow session.\n            inputs: List of arrays matching the input placeholders, of shape\n                [BATCH_SIZE, ...].\n            state_inputs: List of RNN input arrays. These arrays have size\n                [BATCH_SIZE / MAX_SEQ_LEN, ...].\n\n        Returns:\n            The number of tuples loaded per device.\n        \"\"\"\n\n        if log_once(\"load_data\"):\n            logger.info(\n                \"Training on concatenated sample batches:\\n\\n{}\\n\".format(\n                    summarize({\n                        \"placeholders\": self.loss_inputs,\n                        \"inputs\": inputs,\n                        \"state_inputs\": state_inputs\n                    })))\n\n        feed_dict = {}\n        assert len(self.loss_inputs) == len(inputs + state_inputs), \\\n            (self.loss_inputs, inputs, state_inputs)\n\n        # Let's suppose we have the following input data, and 2 devices:\n        # 1 2 3 4 5 6 7                              <- state inputs shape\n        # A A A B B B C C C D D D E E E F F F G G G  <- inputs shape\n        # The data is truncated and split across devices as follows:\n        # |---| seq len = 3\n        # |---------------------------------| seq batch size = 6 seqs\n        # |----------------| per device batch size = 9 tuples\n\n        if len(state_inputs) > 0:\n            smallest_array = state_inputs[0]\n            seq_len = len(inputs[0]) // len(state_inputs[0])\n            self._loaded_max_seq_len = seq_len\n        else:\n            smallest_array = inputs[0]\n            self._loaded_max_seq_len = 1\n\n        sequences_per_minibatch = (\n            self.max_per_device_batch_size // self._loaded_max_seq_len * len(\n                self.devices))\n        if sequences_per_minibatch < 1:\n            logger.warn(\n                (\"Target minibatch size is {}, however the rollout sequence \"\n                 \"length is {}, hence the minibatch size will be raised to \"\n                 \"{}.\").format(self.max_per_device_batch_size,\n                               self._loaded_max_seq_len,\n                               self._loaded_max_seq_len * len(self.devices)))\n            sequences_per_minibatch = 1\n\n        if len(smallest_array) < sequences_per_minibatch:\n            # Dynamically shrink the batch size if insufficient data\n            sequences_per_minibatch = make_divisible_by(\n                len(smallest_array), len(self.devices))\n\n        if log_once(\"data_slicing\"):\n            logger.info(\n                (\"Divided {} rollout sequences, each of length {}, among \"\n                 \"{} devices.\").format(\n                     len(smallest_array), self._loaded_max_seq_len,\n                     len(self.devices)))\n\n        if sequences_per_minibatch < len(self.devices):\n            raise ValueError(\n                \"Must load at least 1 tuple sequence per device. Try \"\n                \"increasing `sgd_minibatch_size` or reducing `max_seq_len` \"\n                \"to ensure that at least one sequence fits per device.\")\n        self._loaded_per_device_batch_size = (sequences_per_minibatch // len(\n            self.devices) * self._loaded_max_seq_len)\n\n        if len(state_inputs) > 0:\n            # First truncate the RNN state arrays to the sequences_per_minib.\n            state_inputs = [\n                make_divisible_by(arr, sequences_per_minibatch)\n                for arr in state_inputs\n            ]\n            # Then truncate the data inputs to match\n            inputs = [arr[:len(state_inputs[0]) * seq_len] for arr in inputs]\n            assert len(state_inputs[0]) * seq_len == len(inputs[0]), \\\n                (len(state_inputs[0]), sequences_per_minibatch, seq_len,\n                 len(inputs[0]))\n            for ph, arr in zip(self.loss_inputs, inputs + state_inputs):\n                feed_dict[ph] = arr\n            truncated_len = len(inputs[0])\n        else:\n            for ph, arr in zip(self.loss_inputs, inputs + state_inputs):\n                truncated_arr = make_divisible_by(arr, sequences_per_minibatch)\n                feed_dict[ph] = truncated_arr\n                truncated_len = len(truncated_arr)\n\n        sess.run([t.init_op for t in self._towers], feed_dict=feed_dict)\n\n        self.num_tuples_loaded = truncated_len\n        tuples_per_device = truncated_len // len(self.devices)\n        assert tuples_per_device > 0, \"No data loaded?\"\n        assert tuples_per_device % self._loaded_per_device_batch_size == 0\n        return tuples_per_device"}, {"code": "def cyclegan_upsample(net, num_outputs, stride, method=\"conv2d_transpose\"):\n  \"\"\"Upsamples the given inputs.\n\n  Args:\n    net: A Tensor of size [batch_size, height, width, filters].\n    num_outputs: The number of output filters.\n    stride: A list of 2 scalars or a 1x2 Tensor indicating the scale,\n      relative to the inputs, of the output dimensions. For example, if kernel\n      size is [2, 3], then the output height and width will be twice and three\n      times the input size.\n    method: The upsampling method: 'nn_upsample_conv',\n      'bilinear_upsample_conv', or 'conv2d_transpose'.\n\n  Returns:\n    A Tensor which was upsampled using the specified method.\n\n  Raises:\n    ValueError: if `method` is not recognized.\n  \"\"\"\n\n  with tf.variable_scope(\"upconv\"):\n    net_shape = tf.shape(net)\n    height = net_shape[1]\n    width = net_shape[2]\n\n    # Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a\n    # 3x3 \"valid\" convolution produce an output with the same dimension as the\n    # input.\n    spatial_pad_1 = np.array([[0, 0], [1, 1], [1, 1], [0, 0]])\n\n    if method == \"nn_upsample_conv\":\n      net = tf.image.resize_nearest_neighbor(\n          net, [stride[0] * height, stride[1] * width])\n      net = tf.pad(net, spatial_pad_1, \"REFLECT\")\n      net = layers().Conv2D(\n          num_outputs, (3, 3), activation=tf.nn.relu)(net)\n    elif method == \"bilinear_upsample_conv\":\n      net = tf.image.resize_bilinear(net,\n                                     [stride[0] * height, stride[1] * width])\n      net = tf.pad(net, spatial_pad_1, \"REFLECT\")\n      net = layers().Conv2D(\n          num_outputs, (3, 3), activation=tf.nn.relu)(net)\n    elif method == \"conv2d_transpose\":\n      # This corrects 1 pixel offset for images with even width and height.\n      # conv2d is left aligned and conv2d_transpose is right aligned for even\n      # sized images (while doing \"SAME\" padding).\n      # Note: This doesn\"t reflect actual model in paper.\n      net = layers().Conv2DTranspose(\n          num_outputs, (3, 3), strides=stride, activation=tf.nn.relu)(net)\n      net = net[:, 1:, 1:, :]\n    else:\n      raise ValueError(\"Unknown method: [%s]\" % method)\n\n    return net", "code_tokens": ["def", "cyclegan_upsample", "(", "net", ",", "num_outputs", ",", "stride", ",", "method", "=", "\"conv2d_transpose\"", ")", ":", "with", "tf", ".", "variable_scope", "(", "\"upconv\"", ")", ":", "net_shape", "=", "tf", ".", "shape", "(", "net", ")", "height", "=", "net_shape", "[", "1", "]", "width", "=", "net_shape", "[", "2", "]", "# Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a", "# 3x3 \"valid\" convolution produce an output with the same dimension as the", "# input.", "spatial_pad_1", "=", "np", ".", "array", "(", "[", "[", "0", ",", "0", "]", ",", "[", "1", ",", "1", "]", ",", "[", "1", ",", "1", "]", ",", "[", "0", ",", "0", "]", "]", ")", "if", "method", "==", "\"nn_upsample_conv\"", ":", "net", "=", "tf", ".", "image", ".", "resize_nearest_neighbor", "(", "net", ",", "[", "stride", "[", "0", "]", "*", "height", ",", "stride", "[", "1", "]", "*", "width", "]", ")", "net", "=", "tf", ".", "pad", "(", "net", ",", "spatial_pad_1", ",", "\"REFLECT\"", ")", "net", "=", "layers", "(", ")", ".", "Conv2D", "(", "num_outputs", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "(", "net", ")", "elif", "method", "==", "\"bilinear_upsample_conv\"", ":", "net", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "net", ",", "[", "stride", "[", "0", "]", "*", "height", ",", "stride", "[", "1", "]", "*", "width", "]", ")", "net", "=", "tf", ".", "pad", "(", "net", ",", "spatial_pad_1", ",", "\"REFLECT\"", ")", "net", "=", "layers", "(", ")", ".", "Conv2D", "(", "num_outputs", ",", "(", "3", ",", "3", ")", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "(", "net", ")", "elif", "method", "==", "\"conv2d_transpose\"", ":", "# This corrects 1 pixel offset for images with even width and height.", "# conv2d is left aligned and conv2d_transpose is right aligned for even", "# sized images (while doing \"SAME\" padding).", "# Note: This doesn\"t reflect actual model in paper.", "net", "=", "layers", "(", ")", ".", "Conv2DTranspose", "(", "num_outputs", ",", "(", "3", ",", "3", ")", ",", "strides", "=", "stride", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "(", "net", ")", "net", "=", "net", "[", ":", ",", "1", ":", ",", "1", ":", ",", ":", "]", "else", ":", "raise", "ValueError", "(", "\"Unknown method: [%s]\"", "%", "method", ")", "return", "net"], "original_string": "def cyclegan_upsample(net, num_outputs, stride, method=\"conv2d_transpose\"):\n  \"\"\"Upsamples the given inputs.\n\n  Args:\n    net: A Tensor of size [batch_size, height, width, filters].\n    num_outputs: The number of output filters.\n    stride: A list of 2 scalars or a 1x2 Tensor indicating the scale,\n      relative to the inputs, of the output dimensions. For example, if kernel\n      size is [2, 3], then the output height and width will be twice and three\n      times the input size.\n    method: The upsampling method: 'nn_upsample_conv',\n      'bilinear_upsample_conv', or 'conv2d_transpose'.\n\n  Returns:\n    A Tensor which was upsampled using the specified method.\n\n  Raises:\n    ValueError: if `method` is not recognized.\n  \"\"\"\n\n  with tf.variable_scope(\"upconv\"):\n    net_shape = tf.shape(net)\n    height = net_shape[1]\n    width = net_shape[2]\n\n    # Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a\n    # 3x3 \"valid\" convolution produce an output with the same dimension as the\n    # input.\n    spatial_pad_1 = np.array([[0, 0], [1, 1], [1, 1], [0, 0]])\n\n    if method == \"nn_upsample_conv\":\n      net = tf.image.resize_nearest_neighbor(\n          net, [stride[0] * height, stride[1] * width])\n      net = tf.pad(net, spatial_pad_1, \"REFLECT\")\n      net = layers().Conv2D(\n          num_outputs, (3, 3), activation=tf.nn.relu)(net)\n    elif method == \"bilinear_upsample_conv\":\n      net = tf.image.resize_bilinear(net,\n                                     [stride[0] * height, stride[1] * width])\n      net = tf.pad(net, spatial_pad_1, \"REFLECT\")\n      net = layers().Conv2D(\n          num_outputs, (3, 3), activation=tf.nn.relu)(net)\n    elif method == \"conv2d_transpose\":\n      # This corrects 1 pixel offset for images with even width and height.\n      # conv2d is left aligned and conv2d_transpose is right aligned for even\n      # sized images (while doing \"SAME\" padding).\n      # Note: This doesn\"t reflect actual model in paper.\n      net = layers().Conv2DTranspose(\n          num_outputs, (3, 3), strides=stride, activation=tf.nn.relu)(net)\n      net = net[:, 1:, 1:, :]\n    else:\n      raise ValueError(\"Unknown method: [%s]\" % method)\n\n    return net"}, {"code": "def find_elt(modvars, keyword, match_last=False):\n    \"Attempt to resolve keywords such as Learner.lr_find. `match_last` starts matching from last component.\"\n    keyword = strip_fastai(keyword)\n    if keyword in modvars: return modvars[keyword]\n    comps = keyword.split('.')\n    comp_elt = modvars.get(comps[0])\n    if hasattr(comp_elt, '__dict__'): return find_elt(comp_elt.__dict__, '.'.join(comps[1:]), match_last=match_last)", "code_tokens": ["def", "find_elt", "(", "modvars", ",", "keyword", ",", "match_last", "=", "False", ")", ":", "keyword", "=", "strip_fastai", "(", "keyword", ")", "if", "keyword", "in", "modvars", ":", "return", "modvars", "[", "keyword", "]", "comps", "=", "keyword", ".", "split", "(", "'.'", ")", "comp_elt", "=", "modvars", ".", "get", "(", "comps", "[", "0", "]", ")", "if", "hasattr", "(", "comp_elt", ",", "'__dict__'", ")", ":", "return", "find_elt", "(", "comp_elt", ".", "__dict__", ",", "'.'", ".", "join", "(", "comps", "[", "1", ":", "]", ")", ",", "match_last", "=", "match_last", ")"], "original_string": "def find_elt(modvars, keyword, match_last=False):\n    \"Attempt to resolve keywords such as Learner.lr_find. `match_last` starts matching from last component.\"\n    keyword = strip_fastai(keyword)\n    if keyword in modvars: return modvars[keyword]\n    comps = keyword.split('.')\n    comp_elt = modvars.get(comps[0])\n    if hasattr(comp_elt, '__dict__'): return find_elt(comp_elt.__dict__, '.'.join(comps[1:]), match_last=match_last)"}, {"code": "def _consolidate(blocks):\n    \"\"\"\n    Merge blocks having same dtype, exclude non-consolidating blocks\n    \"\"\"\n\n    # sort by _can_consolidate, dtype\n    gkey = lambda x: x._consolidate_key\n    grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)\n\n    new_blocks = []\n    for (_can_consolidate, dtype), group_blocks in grouper:\n        merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n                                      _can_consolidate=_can_consolidate)\n        new_blocks = _extend_blocks(merged_blocks, new_blocks)\n    return new_blocks", "code_tokens": ["def", "_consolidate", "(", "blocks", ")", ":", "# sort by _can_consolidate, dtype", "gkey", "=", "lambda", "x", ":", "x", ".", "_consolidate_key", "grouper", "=", "itertools", ".", "groupby", "(", "sorted", "(", "blocks", ",", "key", "=", "gkey", ")", ",", "gkey", ")", "new_blocks", "=", "[", "]", "for", "(", "_can_consolidate", ",", "dtype", ")", ",", "group_blocks", "in", "grouper", ":", "merged_blocks", "=", "_merge_blocks", "(", "list", "(", "group_blocks", ")", ",", "dtype", "=", "dtype", ",", "_can_consolidate", "=", "_can_consolidate", ")", "new_blocks", "=", "_extend_blocks", "(", "merged_blocks", ",", "new_blocks", ")", "return", "new_blocks"], "original_string": "def _consolidate(blocks):\n    \"\"\"\n    Merge blocks having same dtype, exclude non-consolidating blocks\n    \"\"\"\n\n    # sort by _can_consolidate, dtype\n    gkey = lambda x: x._consolidate_key\n    grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)\n\n    new_blocks = []\n    for (_can_consolidate, dtype), group_blocks in grouper:\n        merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n                                      _can_consolidate=_can_consolidate)\n        new_blocks = _extend_blocks(merged_blocks, new_blocks)\n    return new_blocks"}, {"code": "def _tokenize_mteval_v14_intl(segment):\n    r\"\"\"Tokenize a string following following the international tokenizer in mteval-v14a.pl.\n    See https://github.com/moses-smt/mosesdecoder/\"\n           \"blob/master/scripts/generic/mteval-v14.pl#L954-L983\n\n    Parameters\n    ----------\n    segment: str\n        A string to be tokenized\n\n    Returns\n    -------\n    The tokenized string\n    \"\"\"\n    segment = segment.rstrip()\n    segment = unicodeRegex.nondigit_punct_re.sub(r'\\1 \\2 ', segment)\n    segment = unicodeRegex.punct_nondigit_re.sub(r' \\1 \\2', segment)\n    segment = unicodeRegex.symbol_re.sub(r' \\1 ', segment)\n    return segment.strip()", "code_tokens": ["def", "_tokenize_mteval_v14_intl", "(", "segment", ")", ":", "segment", "=", "segment", ".", "rstrip", "(", ")", "segment", "=", "unicodeRegex", ".", "nondigit_punct_re", ".", "sub", "(", "r'\\1 \\2 '", ",", "segment", ")", "segment", "=", "unicodeRegex", ".", "punct_nondigit_re", ".", "sub", "(", "r' \\1 \\2'", ",", "segment", ")", "segment", "=", "unicodeRegex", ".", "symbol_re", ".", "sub", "(", "r' \\1 '", ",", "segment", ")", "return", "segment", ".", "strip", "(", ")"], "original_string": "def _tokenize_mteval_v14_intl(segment):\n    r\"\"\"Tokenize a string following following the international tokenizer in mteval-v14a.pl.\n    See https://github.com/moses-smt/mosesdecoder/\"\n           \"blob/master/scripts/generic/mteval-v14.pl#L954-L983\n\n    Parameters\n    ----------\n    segment: str\n        A string to be tokenized\n\n    Returns\n    -------\n    The tokenized string\n    \"\"\"\n    segment = segment.rstrip()\n    segment = unicodeRegex.nondigit_punct_re.sub(r'\\1 \\2 ', segment)\n    segment = unicodeRegex.punct_nondigit_re.sub(r' \\1 \\2', segment)\n    segment = unicodeRegex.symbol_re.sub(r' \\1 ', segment)\n    return segment.strip()"}, {"code": "def rounding_sequence_accuracy(predictions,\n                               labels,\n                               weights_fn=common_layers.weights_nonzero):\n  \"\"\"Sequence accuracy for L1/L2 losses: round down the predictions to ints.\"\"\"\n  outputs = tf.squeeze(tf.to_int32(predictions), axis=-1)\n  weights = weights_fn(labels)\n  labels = tf.to_int32(labels)\n  not_correct = tf.to_float(tf.not_equal(outputs, labels)) * weights\n  axis = list(range(1, len(outputs.get_shape())))\n  correct_seq = 1.0 - tf.minimum(1.0, tf.reduce_sum(not_correct, axis=axis))\n  return correct_seq, tf.constant(1.0)", "code_tokens": ["def", "rounding_sequence_accuracy", "(", "predictions", ",", "labels", ",", "weights_fn", "=", "common_layers", ".", "weights_nonzero", ")", ":", "outputs", "=", "tf", ".", "squeeze", "(", "tf", ".", "to_int32", "(", "predictions", ")", ",", "axis", "=", "-", "1", ")", "weights", "=", "weights_fn", "(", "labels", ")", "labels", "=", "tf", ".", "to_int32", "(", "labels", ")", "not_correct", "=", "tf", ".", "to_float", "(", "tf", ".", "not_equal", "(", "outputs", ",", "labels", ")", ")", "*", "weights", "axis", "=", "list", "(", "range", "(", "1", ",", "len", "(", "outputs", ".", "get_shape", "(", ")", ")", ")", ")", "correct_seq", "=", "1.0", "-", "tf", ".", "minimum", "(", "1.0", ",", "tf", ".", "reduce_sum", "(", "not_correct", ",", "axis", "=", "axis", ")", ")", "return", "correct_seq", ",", "tf", ".", "constant", "(", "1.0", ")"], "original_string": "def rounding_sequence_accuracy(predictions,\n                               labels,\n                               weights_fn=common_layers.weights_nonzero):\n  \"\"\"Sequence accuracy for L1/L2 losses: round down the predictions to ints.\"\"\"\n  outputs = tf.squeeze(tf.to_int32(predictions), axis=-1)\n  weights = weights_fn(labels)\n  labels = tf.to_int32(labels)\n  not_correct = tf.to_float(tf.not_equal(outputs, labels)) * weights\n  axis = list(range(1, len(outputs.get_shape())))\n  correct_seq = 1.0 - tf.minimum(1.0, tf.reduce_sum(not_correct, axis=axis))\n  return correct_seq, tf.constant(1.0)"}, {"code": "def wait_for_crm_operation(operation):\n    \"\"\"Poll for cloud resource manager operation until finished.\"\"\"\n    logger.info(\"wait_for_crm_operation: \"\n                \"Waiting for operation {} to finish...\".format(operation))\n\n    for _ in range(MAX_POLLS):\n        result = crm.operations().get(name=operation[\"name\"]).execute()\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n\n        if \"done\" in result and result[\"done\"]:\n            logger.info(\"wait_for_crm_operation: Operation done.\")\n            break\n\n        time.sleep(POLL_INTERVAL)\n\n    return result", "code_tokens": ["def", "wait_for_crm_operation", "(", "operation", ")", ":", "logger", ".", "info", "(", "\"wait_for_crm_operation: \"", "\"Waiting for operation {} to finish...\"", ".", "format", "(", "operation", ")", ")", "for", "_", "in", "range", "(", "MAX_POLLS", ")", ":", "result", "=", "crm", ".", "operations", "(", ")", ".", "get", "(", "name", "=", "operation", "[", "\"name\"", "]", ")", ".", "execute", "(", ")", "if", "\"error\"", "in", "result", ":", "raise", "Exception", "(", "result", "[", "\"error\"", "]", ")", "if", "\"done\"", "in", "result", "and", "result", "[", "\"done\"", "]", ":", "logger", ".", "info", "(", "\"wait_for_crm_operation: Operation done.\"", ")", "break", "time", ".", "sleep", "(", "POLL_INTERVAL", ")", "return", "result"], "original_string": "def wait_for_crm_operation(operation):\n    \"\"\"Poll for cloud resource manager operation until finished.\"\"\"\n    logger.info(\"wait_for_crm_operation: \"\n                \"Waiting for operation {} to finish...\".format(operation))\n\n    for _ in range(MAX_POLLS):\n        result = crm.operations().get(name=operation[\"name\"]).execute()\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n\n        if \"done\" in result and result[\"done\"]:\n            logger.info(\"wait_for_crm_operation: Operation done.\")\n            break\n\n        time.sleep(POLL_INTERVAL)\n\n    return result"}, {"code": "def highlight(self, *fields, **kwargs):\n        \"\"\"\n        Request highlighting of some fields. All keyword arguments passed in will be\n        used as parameters for all the fields in the ``fields`` parameter. Example::\n\n            Search().highlight('title', 'body', fragment_size=50)\n\n        will produce the equivalent of::\n\n            {\n                \"highlight\": {\n                    \"fields\": {\n                        \"body\": {\"fragment_size\": 50},\n                        \"title\": {\"fragment_size\": 50}\n                    }\n                }\n            }\n\n        If you want to have different options for different fields you can call ``highlight`` twice::\n\n            Search().highlight('title', fragment_size=50).highlight('body', fragment_size=100)\n\n        which will produce::\n\n            {\n                \"highlight\": {\n                    \"fields\": {\n                        \"body\": {\"fragment_size\": 100},\n                        \"title\": {\"fragment_size\": 50}\n                    }\n                }\n            }\n\n        \"\"\"\n        s = self._clone()\n        for f in fields:\n            s._highlight[f] = kwargs\n        return s", "code_tokens": ["def", "highlight", "(", "self", ",", "*", "fields", ",", "*", "*", "kwargs", ")", ":", "s", "=", "self", ".", "_clone", "(", ")", "for", "f", "in", "fields", ":", "s", ".", "_highlight", "[", "f", "]", "=", "kwargs", "return", "s"], "original_string": "def highlight(self, *fields, **kwargs):\n        \"\"\"\n        Request highlighting of some fields. All keyword arguments passed in will be\n        used as parameters for all the fields in the ``fields`` parameter. Example::\n\n            Search().highlight('title', 'body', fragment_size=50)\n\n        will produce the equivalent of::\n\n            {\n                \"highlight\": {\n                    \"fields\": {\n                        \"body\": {\"fragment_size\": 50},\n                        \"title\": {\"fragment_size\": 50}\n                    }\n                }\n            }\n\n        If you want to have different options for different fields you can call ``highlight`` twice::\n\n            Search().highlight('title', fragment_size=50).highlight('body', fragment_size=100)\n\n        which will produce::\n\n            {\n                \"highlight\": {\n                    \"fields\": {\n                        \"body\": {\"fragment_size\": 100},\n                        \"title\": {\"fragment_size\": 50}\n                    }\n                }\n            }\n\n        \"\"\"\n        s = self._clone()\n        for f in fields:\n            s._highlight[f] = kwargs\n        return s"}, {"code": "def find_user(name, api_key=None):\n    '''\n    Find a user by name and return it.\n\n    :param name:        The user name.\n    :param api_key:     The Slack admin api key.\n    :return:            The user object.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' slack.find_user name=\"ThomasHatch\"\n\n        salt '*' slack.find_user name=\"ThomasHatch\" api_key=peWcBiMOS9HrZG15peWcBiMOS9HrZG15\n    '''\n    if not api_key:\n        api_key = _get_api_key()\n\n    ret = list_users(api_key)\n    if ret['res']:\n        users = ret['message']\n        if users:\n            for user in range(0, len(users)):\n                if users[user]['name'] == name:\n                    return users[user]\n    return False", "code_tokens": ["def", "find_user", "(", "name", ",", "api_key", "=", "None", ")", ":", "if", "not", "api_key", ":", "api_key", "=", "_get_api_key", "(", ")", "ret", "=", "list_users", "(", "api_key", ")", "if", "ret", "[", "'res'", "]", ":", "users", "=", "ret", "[", "'message'", "]", "if", "users", ":", "for", "user", "in", "range", "(", "0", ",", "len", "(", "users", ")", ")", ":", "if", "users", "[", "user", "]", "[", "'name'", "]", "==", "name", ":", "return", "users", "[", "user", "]", "return", "False"], "original_string": "def find_user(name, api_key=None):\n    '''\n    Find a user by name and return it.\n\n    :param name:        The user name.\n    :param api_key:     The Slack admin api key.\n    :return:            The user object.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' slack.find_user name=\"ThomasHatch\"\n\n        salt '*' slack.find_user name=\"ThomasHatch\" api_key=peWcBiMOS9HrZG15peWcBiMOS9HrZG15\n    '''\n    if not api_key:\n        api_key = _get_api_key()\n\n    ret = list_users(api_key)\n    if ret['res']:\n        users = ret['message']\n        if users:\n            for user in range(0, len(users)):\n                if users[user]['name'] == name:\n                    return users[user]\n    return False"}, {"code": "def delete_namespaced_pod_disruption_budget(self, name, namespace, **kwargs):\n        \"\"\"\n        delete a PodDisruptionBudget\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_namespaced_pod_disruption_budget(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodDisruptionBudget (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param V1DeleteOptions body:\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_pod_disruption_budget_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.delete_namespaced_pod_disruption_budget_with_http_info(name, namespace, **kwargs)\n            return data", "code_tokens": ["def", "delete_namespaced_pod_disruption_budget", "(", "self", ",", "name", ",", "namespace", ",", "*", "*", "kwargs", ")", ":", "kwargs", "[", "'_return_http_data_only'", "]", "=", "True", "if", "kwargs", ".", "get", "(", "'async_req'", ")", ":", "return", "self", ".", "delete_namespaced_pod_disruption_budget_with_http_info", "(", "name", ",", "namespace", ",", "*", "*", "kwargs", ")", "else", ":", "(", "data", ")", "=", "self", ".", "delete_namespaced_pod_disruption_budget_with_http_info", "(", "name", ",", "namespace", ",", "*", "*", "kwargs", ")", "return", "data"], "original_string": "def delete_namespaced_pod_disruption_budget(self, name, namespace, **kwargs):\n        \"\"\"\n        delete a PodDisruptionBudget\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_namespaced_pod_disruption_budget(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PodDisruptionBudget (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param V1DeleteOptions body:\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.\n        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \\\"orphan\\\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.\n        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.\n        :return: V1Status\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_namespaced_pod_disruption_budget_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.delete_namespaced_pod_disruption_budget_with_http_info(name, namespace, **kwargs)\n            return data"}, {"code": "def _valid_choices(cla55: type) -> Dict[str, str]:\n    \"\"\"\n    Return a mapping {registered_name -> subclass_name}\n    for the registered subclasses of `cla55`.\n    \"\"\"\n    valid_choices: Dict[str, str] = {}\n\n    if cla55 not in Registrable._registry:\n        raise ValueError(f\"{cla55} is not a known Registrable class\")\n\n    for name, subclass in Registrable._registry[cla55].items():\n        # These wrapper classes need special treatment\n        if isinstance(subclass, (_Seq2SeqWrapper, _Seq2VecWrapper)):\n            subclass = subclass._module_class\n\n        valid_choices[name] = full_name(subclass)\n\n    return valid_choices", "code_tokens": ["def", "_valid_choices", "(", "cla55", ":", "type", ")", "->", "Dict", "[", "str", ",", "str", "]", ":", "valid_choices", ":", "Dict", "[", "str", ",", "str", "]", "=", "{", "}", "if", "cla55", "not", "in", "Registrable", ".", "_registry", ":", "raise", "ValueError", "(", "f\"{cla55} is not a known Registrable class\"", ")", "for", "name", ",", "subclass", "in", "Registrable", ".", "_registry", "[", "cla55", "]", ".", "items", "(", ")", ":", "# These wrapper classes need special treatment", "if", "isinstance", "(", "subclass", ",", "(", "_Seq2SeqWrapper", ",", "_Seq2VecWrapper", ")", ")", ":", "subclass", "=", "subclass", ".", "_module_class", "valid_choices", "[", "name", "]", "=", "full_name", "(", "subclass", ")", "return", "valid_choices"], "original_string": "def _valid_choices(cla55: type) -> Dict[str, str]:\n    \"\"\"\n    Return a mapping {registered_name -> subclass_name}\n    for the registered subclasses of `cla55`.\n    \"\"\"\n    valid_choices: Dict[str, str] = {}\n\n    if cla55 not in Registrable._registry:\n        raise ValueError(f\"{cla55} is not a known Registrable class\")\n\n    for name, subclass in Registrable._registry[cla55].items():\n        # These wrapper classes need special treatment\n        if isinstance(subclass, (_Seq2SeqWrapper, _Seq2VecWrapper)):\n            subclass = subclass._module_class\n\n        valid_choices[name] = full_name(subclass)\n\n    return valid_choices"}, {"code": "def best_match(\n            self, req, working_set, installer=None, replace_conflicting=False):\n        \"\"\"Find distribution best matching `req` and usable on `working_set`\n\n        This calls the ``find(req)`` method of the `working_set` to see if a\n        suitable distribution is already active.  (This may raise\n        ``VersionConflict`` if an unsuitable version of the project is already\n        active in the specified `working_set`.)  If a suitable distribution\n        isn't active, this method returns the newest distribution in the\n        environment that meets the ``Requirement`` in `req`.  If no suitable\n        distribution is found, and `installer` is supplied, then the result of\n        calling the environment's ``obtain(req, installer)`` method will be\n        returned.\n        \"\"\"\n        try:\n            dist = working_set.find(req)\n        except VersionConflict:\n            if not replace_conflicting:\n                raise\n            dist = None\n        if dist is not None:\n            return dist\n        for dist in self[req.key]:\n            if dist in req:\n                return dist\n        # try to download/install\n        return self.obtain(req, installer)", "code_tokens": ["def", "best_match", "(", "self", ",", "req", ",", "working_set", ",", "installer", "=", "None", ",", "replace_conflicting", "=", "False", ")", ":", "try", ":", "dist", "=", "working_set", ".", "find", "(", "req", ")", "except", "VersionConflict", ":", "if", "not", "replace_conflicting", ":", "raise", "dist", "=", "None", "if", "dist", "is", "not", "None", ":", "return", "dist", "for", "dist", "in", "self", "[", "req", ".", "key", "]", ":", "if", "dist", "in", "req", ":", "return", "dist", "# try to download/install", "return", "self", ".", "obtain", "(", "req", ",", "installer", ")"], "original_string": "def best_match(\n            self, req, working_set, installer=None, replace_conflicting=False):\n        \"\"\"Find distribution best matching `req` and usable on `working_set`\n\n        This calls the ``find(req)`` method of the `working_set` to see if a\n        suitable distribution is already active.  (This may raise\n        ``VersionConflict`` if an unsuitable version of the project is already\n        active in the specified `working_set`.)  If a suitable distribution\n        isn't active, this method returns the newest distribution in the\n        environment that meets the ``Requirement`` in `req`.  If no suitable\n        distribution is found, and `installer` is supplied, then the result of\n        calling the environment's ``obtain(req, installer)`` method will be\n        returned.\n        \"\"\"\n        try:\n            dist = working_set.find(req)\n        except VersionConflict:\n            if not replace_conflicting:\n                raise\n            dist = None\n        if dist is not None:\n            return dist\n        for dist in self[req.key]:\n            if dist in req:\n                return dist\n        # try to download/install\n        return self.obtain(req, installer)"}, {"code": "def remove(name=None, pkgs=None, **kwargs):\n    '''\n    Remove a package and all its dependencies which are not in use by other\n    packages.\n\n    name\n        The name of the package to be deleted.\n\n\n    Multiple Package Options:\n\n    pkgs\n        A list of packages to delete. Must be passed as a python list. The\n        ``name`` parameter will be ignored if this option is passed.\n\n    .. versionadded:: 0.16.0\n\n\n    Returns a dict containing the changes.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.remove <package name>\n        salt '*' pkg.remove <package1>,<package2>,<package3>\n        salt '*' pkg.remove pkgs='[\"foo\", \"bar\"]'\n    '''\n    try:\n        pkg_params = __salt__['pkg_resource.parse_targets'](name, pkgs)[0]\n    except MinionError as exc:\n        raise CommandExecutionError(exc)\n\n    old = list_pkgs()\n    targets = [x for x in pkg_params if x in old]\n    if not targets:\n        return {}\n    cmd = '/opt/csw/bin/pkgutil -yr {0}'.format(' '.join(targets))\n    __salt__['cmd.run_all'](cmd)\n    __context__.pop('pkg.list_pkgs', None)\n    new = list_pkgs()\n    return salt.utils.data.compare_dicts(old, new)", "code_tokens": ["def", "remove", "(", "name", "=", "None", ",", "pkgs", "=", "None", ",", "*", "*", "kwargs", ")", ":", "try", ":", "pkg_params", "=", "__salt__", "[", "'pkg_resource.parse_targets'", "]", "(", "name", ",", "pkgs", ")", "[", "0", "]", "except", "MinionError", "as", "exc", ":", "raise", "CommandExecutionError", "(", "exc", ")", "old", "=", "list_pkgs", "(", ")", "targets", "=", "[", "x", "for", "x", "in", "pkg_params", "if", "x", "in", "old", "]", "if", "not", "targets", ":", "return", "{", "}", "cmd", "=", "'/opt/csw/bin/pkgutil -yr {0}'", ".", "format", "(", "' '", ".", "join", "(", "targets", ")", ")", "__salt__", "[", "'cmd.run_all'", "]", "(", "cmd", ")", "__context__", ".", "pop", "(", "'pkg.list_pkgs'", ",", "None", ")", "new", "=", "list_pkgs", "(", ")", "return", "salt", ".", "utils", ".", "data", ".", "compare_dicts", "(", "old", ",", "new", ")"], "original_string": "def remove(name=None, pkgs=None, **kwargs):\n    '''\n    Remove a package and all its dependencies which are not in use by other\n    packages.\n\n    name\n        The name of the package to be deleted.\n\n\n    Multiple Package Options:\n\n    pkgs\n        A list of packages to delete. Must be passed as a python list. The\n        ``name`` parameter will be ignored if this option is passed.\n\n    .. versionadded:: 0.16.0\n\n\n    Returns a dict containing the changes.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.remove <package name>\n        salt '*' pkg.remove <package1>,<package2>,<package3>\n        salt '*' pkg.remove pkgs='[\"foo\", \"bar\"]'\n    '''\n    try:\n        pkg_params = __salt__['pkg_resource.parse_targets'](name, pkgs)[0]\n    except MinionError as exc:\n        raise CommandExecutionError(exc)\n\n    old = list_pkgs()\n    targets = [x for x in pkg_params if x in old]\n    if not targets:\n        return {}\n    cmd = '/opt/csw/bin/pkgutil -yr {0}'.format(' '.join(targets))\n    __salt__['cmd.run_all'](cmd)\n    __context__.pop('pkg.list_pkgs', None)\n    new = list_pkgs()\n    return salt.utils.data.compare_dicts(old, new)"}, {"code": "def urlencoded_processor(entity):\n    '''\n    Accept x-www-form-urlencoded data (run through CherryPy's formatter)\n    and reformat it into a Low State data structure.\n\n    Since we can't easily represent complicated data structures with\n    key-value pairs, any more complicated requirements (e.g. compound\n    commands) must instead be delivered via JSON or YAML.\n\n    For example::\n\n    .. code-block:: bash\n\n        curl -si localhost:8000 -d client=local -d tgt='*' \\\\\n                -d fun='test.kwarg' -d arg='one=1' -d arg='two=2'\n\n    :param entity: raw POST data\n    '''\n    # First call out to CherryPy's default processor\n    cherrypy._cpreqbody.process_urlencoded(entity)\n    cherrypy._cpreqbody.process_urlencoded(entity)\n    cherrypy.serving.request.unserialized_data = entity.params\n    cherrypy.serving.request.raw_body = ''", "code_tokens": ["def", "urlencoded_processor", "(", "entity", ")", ":", "# First call out to CherryPy's default processor", "cherrypy", ".", "_cpreqbody", ".", "process_urlencoded", "(", "entity", ")", "cherrypy", ".", "_cpreqbody", ".", "process_urlencoded", "(", "entity", ")", "cherrypy", ".", "serving", ".", "request", ".", "unserialized_data", "=", "entity", ".", "params", "cherrypy", ".", "serving", ".", "request", ".", "raw_body", "=", "''"], "original_string": "def urlencoded_processor(entity):\n    '''\n    Accept x-www-form-urlencoded data (run through CherryPy's formatter)\n    and reformat it into a Low State data structure.\n\n    Since we can't easily represent complicated data structures with\n    key-value pairs, any more complicated requirements (e.g. compound\n    commands) must instead be delivered via JSON or YAML.\n\n    For example::\n\n    .. code-block:: bash\n\n        curl -si localhost:8000 -d client=local -d tgt='*' \\\\\n                -d fun='test.kwarg' -d arg='one=1' -d arg='two=2'\n\n    :param entity: raw POST data\n    '''\n    # First call out to CherryPy's default processor\n    cherrypy._cpreqbody.process_urlencoded(entity)\n    cherrypy._cpreqbody.process_urlencoded(entity)\n    cherrypy.serving.request.unserialized_data = entity.params\n    cherrypy.serving.request.raw_body = ''"}, {"code": "def get_loc(self, key, method=None):\n        \"\"\"\n        Get location for a label or a tuple of labels as an integer, slice or\n        boolean mask.\n\n        Parameters\n        ----------\n        key : label or tuple of labels (one for each level)\n        method : None\n\n        Returns\n        -------\n        loc : int, slice object or boolean mask\n            If the key is past the lexsort depth, the return may be a\n            boolean mask array, otherwise it is always a slice or int.\n\n        Examples\n        ---------\n        >>> mi = pd.MultiIndex.from_arrays([list('abb'), list('def')])\n\n        >>> mi.get_loc('b')\n        slice(1, 3, None)\n\n        >>> mi.get_loc(('b', 'e'))\n        1\n\n        Notes\n        ------\n        The key cannot be a slice, list of same-level labels, a boolean mask,\n        or a sequence of such. If you want to use those, use\n        :meth:`MultiIndex.get_locs` instead.\n\n        See Also\n        --------\n        Index.get_loc : The get_loc method for (single-level) index.\n        MultiIndex.slice_locs : Get slice location given start label(s) and\n                                end label(s).\n        MultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                              sequence of such.\n        \"\"\"\n        if method is not None:\n            raise NotImplementedError('only the default get_loc method is '\n                                      'currently supported for MultiIndex')\n\n        def _maybe_to_slice(loc):\n            \"\"\"convert integer indexer to boolean mask or slice if possible\"\"\"\n            if not isinstance(loc, np.ndarray) or loc.dtype != 'int64':\n                return loc\n\n            loc = lib.maybe_indices_to_slice(loc, len(self))\n            if isinstance(loc, slice):\n                return loc\n\n            mask = np.empty(len(self), dtype='bool')\n            mask.fill(False)\n            mask[loc] = True\n            return mask\n\n        if not isinstance(key, tuple):\n            loc = self._get_level_indexer(key, level=0)\n            return _maybe_to_slice(loc)\n\n        keylen = len(key)\n        if self.nlevels < keylen:\n            raise KeyError('Key length ({0}) exceeds index depth ({1})'\n                           ''.format(keylen, self.nlevels))\n\n        if keylen == self.nlevels and self.is_unique:\n            return self._engine.get_loc(key)\n\n        # -- partial selection or non-unique index\n        # break the key into 2 parts based on the lexsort_depth of the index;\n        # the first part returns a continuous slice of the index; the 2nd part\n        # needs linear search within the slice\n        i = self.lexsort_depth\n        lead_key, follow_key = key[:i], key[i:]\n        start, stop = (self.slice_locs(lead_key, lead_key)\n                       if lead_key else (0, len(self)))\n\n        if start == stop:\n            raise KeyError(key)\n\n        if not follow_key:\n            return slice(start, stop)\n\n        warnings.warn('indexing past lexsort depth may impact performance.',\n                      PerformanceWarning, stacklevel=10)\n\n        loc = np.arange(start, stop, dtype='int64')\n\n        for i, k in enumerate(follow_key, len(lead_key)):\n            mask = self.codes[i][loc] == self.levels[i].get_loc(k)\n            if not mask.all():\n                loc = loc[mask]\n            if not len(loc):\n                raise KeyError(key)\n\n        return (_maybe_to_slice(loc) if len(loc) != stop - start else\n                slice(start, stop))", "code_tokens": ["def", "get_loc", "(", "self", ",", "key", ",", "method", "=", "None", ")", ":", "if", "method", "is", "not", "None", ":", "raise", "NotImplementedError", "(", "'only the default get_loc method is '", "'currently supported for MultiIndex'", ")", "def", "_maybe_to_slice", "(", "loc", ")", ":", "\"\"\"convert integer indexer to boolean mask or slice if possible\"\"\"", "if", "not", "isinstance", "(", "loc", ",", "np", ".", "ndarray", ")", "or", "loc", ".", "dtype", "!=", "'int64'", ":", "return", "loc", "loc", "=", "lib", ".", "maybe_indices_to_slice", "(", "loc", ",", "len", "(", "self", ")", ")", "if", "isinstance", "(", "loc", ",", "slice", ")", ":", "return", "loc", "mask", "=", "np", ".", "empty", "(", "len", "(", "self", ")", ",", "dtype", "=", "'bool'", ")", "mask", ".", "fill", "(", "False", ")", "mask", "[", "loc", "]", "=", "True", "return", "mask", "if", "not", "isinstance", "(", "key", ",", "tuple", ")", ":", "loc", "=", "self", ".", "_get_level_indexer", "(", "key", ",", "level", "=", "0", ")", "return", "_maybe_to_slice", "(", "loc", ")", "keylen", "=", "len", "(", "key", ")", "if", "self", ".", "nlevels", "<", "keylen", ":", "raise", "KeyError", "(", "'Key length ({0}) exceeds index depth ({1})'", "''", ".", "format", "(", "keylen", ",", "self", ".", "nlevels", ")", ")", "if", "keylen", "==", "self", ".", "nlevels", "and", "self", ".", "is_unique", ":", "return", "self", ".", "_engine", ".", "get_loc", "(", "key", ")", "# -- partial selection or non-unique index", "# break the key into 2 parts based on the lexsort_depth of the index;", "# the first part returns a continuous slice of the index; the 2nd part", "# needs linear search within the slice", "i", "=", "self", ".", "lexsort_depth", "lead_key", ",", "follow_key", "=", "key", "[", ":", "i", "]", ",", "key", "[", "i", ":", "]", "start", ",", "stop", "=", "(", "self", ".", "slice_locs", "(", "lead_key", ",", "lead_key", ")", "if", "lead_key", "else", "(", "0", ",", "len", "(", "self", ")", ")", ")", "if", "start", "==", "stop", ":", "raise", "KeyError", "(", "key", ")", "if", "not", "follow_key", ":", "return", "slice", "(", "start", ",", "stop", ")", "warnings", ".", "warn", "(", "'indexing past lexsort depth may impact performance.'", ",", "PerformanceWarning", ",", "stacklevel", "=", "10", ")", "loc", "=", "np", ".", "arange", "(", "start", ",", "stop", ",", "dtype", "=", "'int64'", ")", "for", "i", ",", "k", "in", "enumerate", "(", "follow_key", ",", "len", "(", "lead_key", ")", ")", ":", "mask", "=", "self", ".", "codes", "[", "i", "]", "[", "loc", "]", "==", "self", ".", "levels", "[", "i", "]", ".", "get_loc", "(", "k", ")", "if", "not", "mask", ".", "all", "(", ")", ":", "loc", "=", "loc", "[", "mask", "]", "if", "not", "len", "(", "loc", ")", ":", "raise", "KeyError", "(", "key", ")", "return", "(", "_maybe_to_slice", "(", "loc", ")", "if", "len", "(", "loc", ")", "!=", "stop", "-", "start", "else", "slice", "(", "start", ",", "stop", ")", ")"], "original_string": "def get_loc(self, key, method=None):\n        \"\"\"\n        Get location for a label or a tuple of labels as an integer, slice or\n        boolean mask.\n\n        Parameters\n        ----------\n        key : label or tuple of labels (one for each level)\n        method : None\n\n        Returns\n        -------\n        loc : int, slice object or boolean mask\n            If the key is past the lexsort depth, the return may be a\n            boolean mask array, otherwise it is always a slice or int.\n\n        Examples\n        ---------\n        >>> mi = pd.MultiIndex.from_arrays([list('abb'), list('def')])\n\n        >>> mi.get_loc('b')\n        slice(1, 3, None)\n\n        >>> mi.get_loc(('b', 'e'))\n        1\n\n        Notes\n        ------\n        The key cannot be a slice, list of same-level labels, a boolean mask,\n        or a sequence of such. If you want to use those, use\n        :meth:`MultiIndex.get_locs` instead.\n\n        See Also\n        --------\n        Index.get_loc : The get_loc method for (single-level) index.\n        MultiIndex.slice_locs : Get slice location given start label(s) and\n                                end label(s).\n        MultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                              sequence of such.\n        \"\"\"\n        if method is not None:\n            raise NotImplementedError('only the default get_loc method is '\n                                      'currently supported for MultiIndex')\n\n        def _maybe_to_slice(loc):\n            \"\"\"convert integer indexer to boolean mask or slice if possible\"\"\"\n            if not isinstance(loc, np.ndarray) or loc.dtype != 'int64':\n                return loc\n\n            loc = lib.maybe_indices_to_slice(loc, len(self))\n            if isinstance(loc, slice):\n                return loc\n\n            mask = np.empty(len(self), dtype='bool')\n            mask.fill(False)\n            mask[loc] = True\n            return mask\n\n        if not isinstance(key, tuple):\n            loc = self._get_level_indexer(key, level=0)\n            return _maybe_to_slice(loc)\n\n        keylen = len(key)\n        if self.nlevels < keylen:\n            raise KeyError('Key length ({0}) exceeds index depth ({1})'\n                           ''.format(keylen, self.nlevels))\n\n        if keylen == self.nlevels and self.is_unique:\n            return self._engine.get_loc(key)\n\n        # -- partial selection or non-unique index\n        # break the key into 2 parts based on the lexsort_depth of the index;\n        # the first part returns a continuous slice of the index; the 2nd part\n        # needs linear search within the slice\n        i = self.lexsort_depth\n        lead_key, follow_key = key[:i], key[i:]\n        start, stop = (self.slice_locs(lead_key, lead_key)\n                       if lead_key else (0, len(self)))\n\n        if start == stop:\n            raise KeyError(key)\n\n        if not follow_key:\n            return slice(start, stop)\n\n        warnings.warn('indexing past lexsort depth may impact performance.',\n                      PerformanceWarning, stacklevel=10)\n\n        loc = np.arange(start, stop, dtype='int64')\n\n        for i, k in enumerate(follow_key, len(lead_key)):\n            mask = self.codes[i][loc] == self.levels[i].get_loc(k)\n            if not mask.all():\n                loc = loc[mask]\n            if not len(loc):\n                raise KeyError(key)\n\n        return (_maybe_to_slice(loc) if len(loc) != stop - start else\n                slice(start, stop))"}, {"code": "def user_update(user_id=None, name=None, email=None, enabled=None,\n                tenant=None, profile=None, project=None, description=None, **connection_args):\n    '''\n    Update a user's information (keystone user-update)\n    The following fields may be updated: name, email, enabled, tenant.\n    Because the name is one of the fields, a valid user id is required.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_update user_id=c965f79c4f864eaaa9c3b41904e67082 name=newname\n        salt '*' keystone.user_update c965f79c4f864eaaa9c3b41904e67082 name=newname email=newemail@domain.com\n    '''\n    kstone = auth(profile, **connection_args)\n    if not user_id:\n        for user in kstone.users.list():\n            if user.name == name:\n                user_id = user.id\n                break\n        if not user_id:\n            return {'Error': 'Unable to resolve user id'}\n    user = kstone.users.get(user_id)\n    # Keep previous settings if not updating them\n    if not name:\n        name = user.name\n    if not email:\n        email = user.email\n    if enabled is None:\n        enabled = user.enabled\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        if description is None:\n            description = getattr(user, 'description', None)\n        else:\n            description = six.text_type(description)\n\n        project_id = None\n        if project:\n            for proj in kstone.projects.list():\n                if proj.name == project:\n                    project_id = proj.id\n                    break\n        if not project_id:\n            project_id = getattr(user, 'project_id', None)\n\n        kstone.users.update(user=user_id, name=name, email=email, enabled=enabled, description=description,\n                            project_id=project_id)\n    else:\n        kstone.users.update(user=user_id, name=name, email=email, enabled=enabled)\n\n        tenant_id = None\n        if tenant:\n            for tnt in kstone.tenants.list():\n                if tnt.name == tenant:\n                    tenant_id = tnt.id\n                    break\n            if tenant_id:\n                kstone.users.update_tenant(user_id, tenant_id)\n\n    ret = 'Info updated for user ID {0}'.format(user_id)\n    return ret", "code_tokens": ["def", "user_update", "(", "user_id", "=", "None", ",", "name", "=", "None", ",", "email", "=", "None", ",", "enabled", "=", "None", ",", "tenant", "=", "None", ",", "profile", "=", "None", ",", "project", "=", "None", ",", "description", "=", "None", ",", "*", "*", "connection_args", ")", ":", "kstone", "=", "auth", "(", "profile", ",", "*", "*", "connection_args", ")", "if", "not", "user_id", ":", "for", "user", "in", "kstone", ".", "users", ".", "list", "(", ")", ":", "if", "user", ".", "name", "==", "name", ":", "user_id", "=", "user", ".", "id", "break", "if", "not", "user_id", ":", "return", "{", "'Error'", ":", "'Unable to resolve user id'", "}", "user", "=", "kstone", ".", "users", ".", "get", "(", "user_id", ")", "# Keep previous settings if not updating them", "if", "not", "name", ":", "name", "=", "user", ".", "name", "if", "not", "email", ":", "email", "=", "user", ".", "email", "if", "enabled", "is", "None", ":", "enabled", "=", "user", ".", "enabled", "if", "_OS_IDENTITY_API_VERSION", ">", "2", ":", "if", "description", "is", "None", ":", "description", "=", "getattr", "(", "user", ",", "'description'", ",", "None", ")", "else", ":", "description", "=", "six", ".", "text_type", "(", "description", ")", "project_id", "=", "None", "if", "project", ":", "for", "proj", "in", "kstone", ".", "projects", ".", "list", "(", ")", ":", "if", "proj", ".", "name", "==", "project", ":", "project_id", "=", "proj", ".", "id", "break", "if", "not", "project_id", ":", "project_id", "=", "getattr", "(", "user", ",", "'project_id'", ",", "None", ")", "kstone", ".", "users", ".", "update", "(", "user", "=", "user_id", ",", "name", "=", "name", ",", "email", "=", "email", ",", "enabled", "=", "enabled", ",", "description", "=", "description", ",", "project_id", "=", "project_id", ")", "else", ":", "kstone", ".", "users", ".", "update", "(", "user", "=", "user_id", ",", "name", "=", "name", ",", "email", "=", "email", ",", "enabled", "=", "enabled", ")", "tenant_id", "=", "None", "if", "tenant", ":", "for", "tnt", "in", "kstone", ".", "tenants", ".", "list", "(", ")", ":", "if", "tnt", ".", "name", "==", "tenant", ":", "tenant_id", "=", "tnt", ".", "id", "break", "if", "tenant_id", ":", "kstone", ".", "users", ".", "update_tenant", "(", "user_id", ",", "tenant_id", ")", "ret", "=", "'Info updated for user ID {0}'", ".", "format", "(", "user_id", ")", "return", "ret"], "original_string": "def user_update(user_id=None, name=None, email=None, enabled=None,\n                tenant=None, profile=None, project=None, description=None, **connection_args):\n    '''\n    Update a user's information (keystone user-update)\n    The following fields may be updated: name, email, enabled, tenant.\n    Because the name is one of the fields, a valid user id is required.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_update user_id=c965f79c4f864eaaa9c3b41904e67082 name=newname\n        salt '*' keystone.user_update c965f79c4f864eaaa9c3b41904e67082 name=newname email=newemail@domain.com\n    '''\n    kstone = auth(profile, **connection_args)\n    if not user_id:\n        for user in kstone.users.list():\n            if user.name == name:\n                user_id = user.id\n                break\n        if not user_id:\n            return {'Error': 'Unable to resolve user id'}\n    user = kstone.users.get(user_id)\n    # Keep previous settings if not updating them\n    if not name:\n        name = user.name\n    if not email:\n        email = user.email\n    if enabled is None:\n        enabled = user.enabled\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        if description is None:\n            description = getattr(user, 'description', None)\n        else:\n            description = six.text_type(description)\n\n        project_id = None\n        if project:\n            for proj in kstone.projects.list():\n                if proj.name == project:\n                    project_id = proj.id\n                    break\n        if not project_id:\n            project_id = getattr(user, 'project_id', None)\n\n        kstone.users.update(user=user_id, name=name, email=email, enabled=enabled, description=description,\n                            project_id=project_id)\n    else:\n        kstone.users.update(user=user_id, name=name, email=email, enabled=enabled)\n\n        tenant_id = None\n        if tenant:\n            for tnt in kstone.tenants.list():\n                if tnt.name == tenant:\n                    tenant_id = tnt.id\n                    break\n            if tenant_id:\n                kstone.users.update_tenant(user_id, tenant_id)\n\n    ret = 'Info updated for user ID {0}'.format(user_id)\n    return ret"}, {"code": "def _machinectl(cmd,\n                output_loglevel='debug',\n                ignore_retcode=False,\n                use_vt=False):\n    '''\n    Helper function to run machinectl\n    '''\n    prefix = 'machinectl --no-legend --no-pager'\n    return __salt__['cmd.run_all']('{0} {1}'.format(prefix, cmd),\n                                   output_loglevel=output_loglevel,\n                                   ignore_retcode=ignore_retcode,\n                                   use_vt=use_vt)", "code_tokens": ["def", "_machinectl", "(", "cmd", ",", "output_loglevel", "=", "'debug'", ",", "ignore_retcode", "=", "False", ",", "use_vt", "=", "False", ")", ":", "prefix", "=", "'machinectl --no-legend --no-pager'", "return", "__salt__", "[", "'cmd.run_all'", "]", "(", "'{0} {1}'", ".", "format", "(", "prefix", ",", "cmd", ")", ",", "output_loglevel", "=", "output_loglevel", ",", "ignore_retcode", "=", "ignore_retcode", ",", "use_vt", "=", "use_vt", ")"], "original_string": "def _machinectl(cmd,\n                output_loglevel='debug',\n                ignore_retcode=False,\n                use_vt=False):\n    '''\n    Helper function to run machinectl\n    '''\n    prefix = 'machinectl --no-legend --no-pager'\n    return __salt__['cmd.run_all']('{0} {1}'.format(prefix, cmd),\n                                   output_loglevel=output_loglevel,\n                                   ignore_retcode=ignore_retcode,\n                                   use_vt=use_vt)"}, {"code": "def play_env_problem_randomly(env_problem,\n                              num_steps):\n  \"\"\"Plays the env problem by randomly sampling actions for `num_steps`.\"\"\"\n  # Reset all environments.\n  env_problem.reset()\n\n  # Play all environments, sampling random actions each time.\n  for _ in range(num_steps):\n    # Sample batch_size actions from the action space and stack them.\n    actions = np.stack([env_problem.action_space.sample() for _ in range(\n        env_problem.batch_size)])\n\n    # Execute actions, observations are stored in `env_problem`.\n    _, _, dones, _ = env_problem.step(actions)\n\n    # Get the indices where we are done and reset those.\n    env_problem.reset(indices=done_indices(dones))", "code_tokens": ["def", "play_env_problem_randomly", "(", "env_problem", ",", "num_steps", ")", ":", "# Reset all environments.", "env_problem", ".", "reset", "(", ")", "# Play all environments, sampling random actions each time.", "for", "_", "in", "range", "(", "num_steps", ")", ":", "# Sample batch_size actions from the action space and stack them.", "actions", "=", "np", ".", "stack", "(", "[", "env_problem", ".", "action_space", ".", "sample", "(", ")", "for", "_", "in", "range", "(", "env_problem", ".", "batch_size", ")", "]", ")", "# Execute actions, observations are stored in `env_problem`.", "_", ",", "_", ",", "dones", ",", "_", "=", "env_problem", ".", "step", "(", "actions", ")", "# Get the indices where we are done and reset those.", "env_problem", ".", "reset", "(", "indices", "=", "done_indices", "(", "dones", ")", ")"], "original_string": "def play_env_problem_randomly(env_problem,\n                              num_steps):\n  \"\"\"Plays the env problem by randomly sampling actions for `num_steps`.\"\"\"\n  # Reset all environments.\n  env_problem.reset()\n\n  # Play all environments, sampling random actions each time.\n  for _ in range(num_steps):\n    # Sample batch_size actions from the action space and stack them.\n    actions = np.stack([env_problem.action_space.sample() for _ in range(\n        env_problem.batch_size)])\n\n    # Execute actions, observations are stored in `env_problem`.\n    _, _, dones, _ = env_problem.step(actions)\n\n    # Get the indices where we are done and reset those.\n    env_problem.reset(indices=done_indices(dones))"}, {"code": "def set_wu_settings(level=None,\n                    recommended=None,\n                    featured=None,\n                    elevated=None,\n                    msupdate=None,\n                    day=None,\n                    time=None):\n    '''\n    Change Windows Update settings. If no parameters are passed, the current\n    value will be returned.\n\n    Supported:\n        - Windows Vista / Server 2008\n        - Windows 7 / Server 2008R2\n        - Windows 8 / Server 2012\n        - Windows 8.1 / Server 2012R2\n\n    .. note:\n        Microsoft began using the Unified Update Platform (UUP) starting with\n        Windows 10 / Server 2016. The Windows Update settings have changed and\n        the ability to 'Save' Windows Update settings has been removed. Windows\n        Update settings are read-only. See MSDN documentation:\n        https://msdn.microsoft.com/en-us/library/aa385829(v=vs.85).aspx\n\n    Args:\n\n        level (int):\n            Number from 1 to 4 indicating the update level:\n\n            1. Never check for updates\n            2. Check for updates but let me choose whether to download and install them\n            3. Download updates but let me choose whether to install them\n            4. Install updates automatically\n\n        recommended (bool):\n            Boolean value that indicates whether to include optional or\n            recommended updates when a search for updates and installation of\n            updates is performed.\n\n        featured (bool):\n            Boolean value that indicates whether to display notifications for\n            featured updates.\n\n        elevated (bool):\n            Boolean value that indicates whether non-administrators can perform\n            some update-related actions without administrator approval.\n\n        msupdate (bool):\n            Boolean value that indicates whether to turn on Microsoft Update for\n            other Microsoft products\n\n        day (str):\n            Days of the week on which Automatic Updates installs or uninstalls\n            updates. Accepted values:\n\n            - Everyday\n            - Monday\n            - Tuesday\n            - Wednesday\n            - Thursday\n            - Friday\n            - Saturday\n\n        time (str):\n            Time at which Automatic Updates installs or uninstalls updates. Must\n            be in the ##:## 24hr format, eg. 3:00 PM would be 15:00. Must be in\n            1 hour increments.\n\n    Returns:\n\n        dict: Returns a dictionary containing the results.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' win_wua.set_wu_settings level=4 recommended=True featured=False\n    '''\n    # The AutomaticUpdateSettings.Save() method used in this function does not\n    # work on Windows 10 / Server 2016. It is called in throughout this function\n    # like this:\n    #\n    # with salt.utils.winapi.Com():\n    #     obj_au = win32com.client.Dispatch('Microsoft.Update.AutoUpdate')\n    #     obj_au_settings = obj_au.Settings\n    #     obj_au_settings.Save()\n    #\n    # The `Save()` method reports success but doesn't actually change anything.\n    # Windows Update settings are read-only in Windows 10 / Server 2016. There's\n    # a little blurb on MSDN that mentions this, but gives no alternative for\n    # changing these settings in Windows 10 / Server 2016.\n    #\n    # https://msdn.microsoft.com/en-us/library/aa385829(v=vs.85).aspx\n    #\n    # Apparently the Windows Update framework in Windows Vista - Windows 8.1 has\n    # been changed quite a bit in Windows 10 / Server 2016. It is now called the\n    # Unified Update Platform (UUP). I haven't found an API or a Powershell\n    # commandlet for working with the the UUP. Perhaps there will be something\n    # forthcoming. The `win_lgpo` module might be an option for changing the\n    # Windows Update settings using local group policy.\n    ret = {'Success': True}\n\n    # Initialize the PyCom system\n    with salt.utils.winapi.Com():\n\n        # Create an AutoUpdate object\n        obj_au = win32com.client.Dispatch('Microsoft.Update.AutoUpdate')\n\n    # Create an AutoUpdate Settings Object\n    obj_au_settings = obj_au.Settings\n\n    # Only change the setting if it's passed\n    if level is not None:\n        obj_au_settings.NotificationLevel = int(level)\n        result = obj_au_settings.Save()\n        if result is None:\n            ret['Level'] = level\n        else:\n            ret['Comment'] = \"Settings failed to save. Check permissions.\"\n            ret['Success'] = False\n\n    if recommended is not None:\n        obj_au_settings.IncludeRecommendedUpdates = recommended\n        result = obj_au_settings.Save()\n        if result is None:\n            ret['Recommended'] = recommended\n        else:\n            ret['Comment'] = \"Settings failed to save. Check permissions.\"\n            ret['Success'] = False\n\n    if featured is not None:\n        obj_au_settings.FeaturedUpdatesEnabled = featured\n        result = obj_au_settings.Save()\n        if result is None:\n            ret['Featured'] = featured\n        else:\n            ret['Comment'] = \"Settings failed to save. Check permissions.\"\n            ret['Success'] = False\n\n    if elevated is not None:\n        obj_au_settings.NonAdministratorsElevated = elevated\n        result = obj_au_settings.Save()\n        if result is None:\n            ret['Elevated'] = elevated\n        else:\n            ret['Comment'] = \"Settings failed to save. Check permissions.\"\n            ret['Success'] = False\n\n    if day is not None:\n        # Check that day is valid\n        days = {'Everyday': 0,\n                'Sunday': 1,\n                'Monday': 2,\n                'Tuesday': 3,\n                'Wednesday': 4,\n                'Thursday': 5,\n                'Friday': 6,\n                'Saturday': 7}\n        if day not in days:\n            ret['Comment'] = \"Day needs to be one of the following: Everyday,\" \\\n                             \"Monday, Tuesday, Wednesday, Thursday, Friday, \" \\\n                             \"Saturday\"\n            ret['Success'] = False\n        else:\n            # Set the numeric equivalent for the day setting\n            obj_au_settings.ScheduledInstallationDay = days[day]\n            result = obj_au_settings.Save()\n            if result is None:\n                ret['Day'] = day\n            else:\n                ret['Comment'] = \"Settings failed to save. Check permissions.\"\n                ret['Success'] = False\n\n    if time is not None:\n        # Check for time as a string: if the time is not quoted, yaml will\n        # treat it as an integer\n        if not isinstance(time, six.string_types):\n            ret['Comment'] = \"Time argument needs to be a string; it may need to\"\\\n                             \"be quoted. Passed {0}. Time not set.\".format(time)\n            ret['Success'] = False\n        # Check for colon in the time\n        elif ':' not in time:\n            ret['Comment'] = \"Time argument needs to be in 00:00 format.\" \\\n                             \" Passed {0}. Time not set.\".format(time)\n            ret['Success'] = False\n        else:\n            # Split the time by :\n            t = time.split(\":\")\n            # We only need the hours value\n            obj_au_settings.FeaturedUpdatesEnabled = t[0]\n            result = obj_au_settings.Save()\n            if result is None:\n                ret['Time'] = time\n            else:\n                ret['Comment'] = \"Settings failed to save. Check permissions.\"\n                ret['Success'] = False\n\n    if msupdate is not None:\n        # Microsoft Update requires special handling\n        # First load the MS Update Service Manager\n        with salt.utils.winapi.Com():\n            obj_sm = win32com.client.Dispatch('Microsoft.Update.ServiceManager')\n\n        # Give it a bogus name\n        obj_sm.ClientApplicationID = \"My App\"\n\n        if msupdate:\n            # msupdate is true, so add it to the services\n            try:\n                obj_sm.AddService2('7971f918-a847-4430-9279-4a52d1efe18d', 7, '')\n                ret['msupdate'] = msupdate\n            except Exception as error:\n                hr, msg, exc, arg = error.args  # pylint: disable=W0633\n                # Consider checking for -2147024891 (0x80070005) Access Denied\n                ret['Comment'] = \"Failed with failure code: {0}\".format(exc[5])\n                ret['Success'] = False\n        else:\n            # msupdate is false, so remove it from the services\n            # check to see if the update is there or the RemoveService function\n            # will fail\n            if _get_msupdate_status():\n                # Service found, remove the service\n                try:\n                    obj_sm.RemoveService('7971f918-a847-4430-9279-4a52d1efe18d')\n                    ret['msupdate'] = msupdate\n                except Exception as error:\n                    hr, msg, exc, arg = error.args  # pylint: disable=W0633\n                    # Consider checking for the following\n                    # -2147024891 (0x80070005) Access Denied\n                    # -2145091564 (0x80248014) Service Not Found (shouldn't get\n                    # this with the check for _get_msupdate_status above\n                    ret['Comment'] = \"Failed with failure code: {0}\".format(exc[5])\n                    ret['Success'] = False\n            else:\n                ret['msupdate'] = msupdate\n\n    ret['Reboot'] = get_needs_reboot()\n\n    return ret", "code_tokens": ["def", "set_wu_settings", "(", "level", "=", "None", ",", "recommended", "=", "None", ",", "featured", "=", "None", ",", "elevated", "=", "None", ",", "msupdate", "=", "None", ",", "day", "=", "None", ",", "time", "=", "None", ")", ":", "# The AutomaticUpdateSettings.Save() method used in this function does not", "# work on Windows 10 / Server 2016. It is called in throughout this function", "# like this:", "#", "# with salt.utils.winapi.Com():", "#     obj_au = win32com.client.Dispatch('Microsoft.Update.AutoUpdate')", "#     obj_au_settings = obj_au.Settings", "#     obj_au_settings.Save()", "#", "# The `Save()` method reports success but doesn't actually change anything.", "# Windows Update settings are read-only in Windows 10 / Server 2016. There's", "# a little blurb on MSDN that mentions this, but gives no alternative for", "# changing these settings in Windows 10 / Server 2016.", "#", "# https://msdn.microsoft.com/en-us/library/aa385829(v=vs.85).aspx", "#", "# Apparently the Windows Update framework in Windows Vista - Windows 8.1 has", "# been changed quite a bit in Windows 10 / Server 2016. It is now called the", "# Unified Update Platform (UUP). I haven't found an API or a Powershell", "# commandlet for working with the the UUP. Perhaps there will be something", "# forthcoming. The `win_lgpo` module might be an option for changing the", "# Windows Update settings using local group policy.", "ret", "=", "{", "'Success'", ":", "True", "}", "# Initialize the PyCom system", "with", "salt", ".", "utils", ".", "winapi", ".", "Com", "(", ")", ":", "# Create an AutoUpdate object", "obj_au", "=", "win32com", ".", "client", ".", "Dispatch", "(", "'Microsoft.Update.AutoUpdate'", ")", "# Create an AutoUpdate Settings Object", "obj_au_settings", "=", "obj_au", ".", "Settings", "# Only change the setting if it's passed", "if", "level", "is", "not", "None", ":", "obj_au_settings", ".", "NotificationLevel", "=", "int", "(", "level", ")", "result", "=", "obj_au_settings", ".", "Save", "(", ")", "if", "result", "is", "None", ":", "ret", "[", "'Level'", "]", "=", "level", "else", ":", "ret", "[", "'Comment'", "]", "=", "\"Settings failed to save. Check permissions.\"", "ret", "[", "'Success'", "]", "=", "False", "if", "recommended", "is", "not", "None", ":", "obj_au_settings", ".", "IncludeRecommendedUpdates", "=", "recommended", "result", "=", "obj_au_settings", ".", "Save", "(", ")", "if", "result", "is", "None", ":", "ret", "[", "'Recommended'", "]", "=", "recommended", "else", ":", "ret", "[", "'Comment'", "]", "=", "\"Settings failed to save. Check permissions.\"", "ret", "[", "'Success'", "]", "=", "False", "if", "featured", "is", "not", "None", ":", "obj_au_settings", ".", "FeaturedUpdatesEnabled", "=", "featured", "result", "=", "obj_au_settings", ".", "Save", "(", ")", "if", "result", "is", "None", ":", "ret", "[", "'Featured'", "]", "=", "featured", "else", ":", "ret", "[", "'Comment'", "]", "=", "\"Settings failed to save. Check permissions.\"", "ret", "[", "'Success'", "]", "=", "False", "if", "elevated", "is", "not", "None", ":", "obj_au_settings", ".", "NonAdministratorsElevated", "=", "elevated", "result", "=", "obj_au_settings", ".", "Save", "(", ")", "if", "result", "is", "None", ":", "ret", "[", "'Elevated'", "]", "=", "elevated", "else", ":", "ret", "[", "'Comment'", "]", "=", "\"Settings failed to save. Check permissions.\"", "ret", "[", "'Success'", "]", "=", "False", "if", "day", "is", "not", "None", ":", "# Check that day is valid", "days", "=", "{", "'Everyday'", ":", "0", ",", "'Sunday'", ":", "1", ",", "'Monday'", ":", "2", ",", "'Tuesday'", ":", "3", ",", "'Wednesday'", ":", "4", ",", "'Thursday'", ":", "5", ",", "'Friday'", ":", "6", ",", "'Saturday'", ":", "7", "}", "if", "day", "not", "in", "days", ":", "ret", "[", "'Comment'", "]", "=", "\"Day needs to be one of the following: Everyday,\"", "\"Monday, Tuesday, Wednesday, Thursday, Friday, \"", "\"Saturday\"", "ret", "[", "'Success'", "]", "=", "False", "else", ":", "# Set the numeric equivalent for the day setting", "obj_au_settings", ".", "ScheduledInstallationDay", "=", "days", "[", "day", "]", "result", "=", "obj_au_settings", ".", "Save", "(", ")", "if", "result", "is", "None", ":", "ret", "[", "'Day'", "]", "=", "day", "else", ":", "ret", "[", "'Comment'", "]", "=", "\"Settings failed to save. Check permissions.\"", "ret", "[", "'Success'", "]", "=", "False", "if", "time", "is", "not", "None", ":", "# Check for time as a string: if the time is not quoted, yaml will", "# treat it as an integer", "if", "not", "isinstance", "(", "time", ",", "six", ".", "string_types", ")", ":", "ret", "[", "'Comment'", "]", "=", "\"Time argument needs to be a string; it may need to\"", "\"be quoted. Passed {0}. Time not set.\"", ".", "format", "(", "time", ")", "ret", "[", "'Success'", "]", "=", "False", "# Check for colon in the time", "elif", "':'", "not", "in", "time", ":", "ret", "[", "'Comment'", "]", "=", "\"Time argument needs to be in 00:00 format.\"", "\" Passed {0}. Time not set.\"", ".", "format", "(", "time", ")", "ret", "[", "'Success'", "]", "=", "False", "else", ":", "# Split the time by :", "t", "=", "time", ".", "split", "(", "\":\"", ")", "# We only need the hours value", "obj_au_settings", ".", "FeaturedUpdatesEnabled", "=", "t", "[", "0", "]", "result", "=", "obj_au_settings", ".", "Save", "(", ")", "if", "result", "is", "None", ":", "ret", "[", "'Time'", "]", "=", "time", "else", ":", "ret", "[", "'Comment'", "]", "=", "\"Settings failed to save. Check permissions.\"", "ret", "[", "'Success'", "]", "=", "False", "if", "msupdate", "is", "not", "None", ":", "# Microsoft Update requires special handling", "# First load the MS Update Service Manager", "with", "salt", ".", "utils", ".", "winapi", ".", "Com", "(", ")", ":", "obj_sm", "=", "win32com", ".", "client", ".", "Dispatch", "(", "'Microsoft.Update.ServiceManager'", ")", "# Give it a bogus name", "obj_sm", ".", "ClientApplicationID", "=", "\"My App\"", "if", "msupdate", ":", "# msupdate is true, so add it to the services", "try", ":", "obj_sm", ".", "AddService2", "(", "'7971f918-a847-4430-9279-4a52d1efe18d'", ",", "7", ",", "''", ")", "ret", "[", "'msupdate'", "]", "=", "msupdate", "except", "Exception", "as", "error", ":", "hr", ",", "msg", ",", "exc", ",", "arg", "=", "error", ".", "args", "# pylint: disable=W0633", "# Consider checking for -2147024891 (0x80070005) Access Denied", "ret", "[", "'Comment'", "]", "=", "\"Failed with failure code: {0}\"", ".", "format", "(", "exc", "[", "5", "]", ")", "ret", "[", "'Success'", "]", "=", "False", "else", ":", "# msupdate is false, so remove it from the services", "# check to see if the update is there or the RemoveService function", "# will fail", "if", "_get_msupdate_status", "(", ")", ":", "# Service found, remove the service", "try", ":", "obj_sm", ".", "RemoveService", "(", "'7971f918-a847-4430-9279-4a52d1efe18d'", ")", "ret", "[", "'msupdate'", "]", "=", "msupdate", "except", "Exception", "as", "error", ":", "hr", ",", "msg", ",", "exc", ",", "arg", "=", "error", ".", "args", "# pylint: disable=W0633", "# Consider checking for the following", "# -2147024891 (0x80070005) Access Denied", "# -2145091564 (0x80248014) Service Not Found (shouldn't get", "# this with the check for _get_msupdate_status above", "ret", "[", "'Comment'", "]", "=", "\"Failed with failure code: {0}\"", ".", "format", "(", "exc", "[", "5", "]", ")", "ret", "[", "'Success'", "]", "=", "False", "else", ":", "ret", "[", "'msupdate'", "]", "=", "msupdate", "ret", "[", "'Reboot'", "]", "=", "get_needs_reboot", "(", ")", "return", "ret"], "original_string": "def set_wu_settings(level=None,\n                    recommended=None,\n                    featured=None,\n                    elevated=None,\n                    msupdate=None,\n                    day=None,\n                    time=None):\n    '''\n    Change Windows Update settings. If no parameters are passed, the current\n    value will be returned.\n\n    Supported:\n        - Windows Vista / Server 2008\n        - Windows 7 / Server 2008R2\n        - Windows 8 / Server 2012\n        - Windows 8.1 / Server 2012R2\n\n    .. note:\n        Microsoft began using the Unified Update Platform (UUP) starting with\n        Windows 10 / Server 2016. The Windows Update settings have changed and\n        the ability to 'Save' Windows Update settings has been removed. Windows\n        Update settings are read-only. See MSDN documentation:\n        https://msdn.microsoft.com/en-us/library/aa385829(v=vs.85).aspx\n\n    Args:\n\n        level (int):\n            Number from 1 to 4 indicating the update level:\n\n            1. Never check for updates\n            2. Check for updates but let me choose whether to download and install them\n            3. Download updates but let me choose whether to install them\n            4. Install updates automatically\n\n        recommended (bool):\n            Boolean value that indicates whether to include optional or\n            recommended updates when a search for updates and installation of\n            updates is performed.\n\n        featured (bool):\n            Boolean value that indicates whether to display notifications for\n            featured updates.\n\n        elevated (bool):\n            Boolean value that indicates whether non-administrators can perform\n            some update-related actions without administrator approval.\n\n        msupdate (bool):\n            Boolean value that indicates whether to turn on Microsoft Update for\n            other Microsoft products\n\n        day (str):\n            Days of the week on which Automatic Updates installs or uninstalls\n            updates. Accepted values:\n\n            - Everyday\n            - Monday\n            - Tuesday\n            - Wednesday\n            - Thursday\n            - Friday\n            - Saturday\n\n        time (str):\n            Time at which Automatic Updates installs or uninstalls updates. Must\n            be in the ##:## 24hr format, eg. 3:00 PM would be 15:00. Must be in\n            1 hour increments.\n\n    Returns:\n\n        dict: Returns a dictionary containing the results.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' win_wua.set_wu_settings level=4 recommended=True featured=False\n    '''\n    # The AutomaticUpdateSettings.Save() method used in this function does not\n    # work on Windows 10 / Server 2016. It is called in throughout this function\n    # like this:\n    #\n    # with salt.utils.winapi.Com():\n    #     obj_au = win32com.client.Dispatch('Microsoft.Update.AutoUpdate')\n    #     obj_au_settings = obj_au.Settings\n    #     obj_au_settings.Save()\n    #\n    # The `Save()` method reports success but doesn't actually change anything.\n    # Windows Update settings are read-only in Windows 10 / Server 2016. There's\n    # a little blurb on MSDN that mentions this, but gives no alternative for\n    # changing these settings in Windows 10 / Server 2016.\n    #\n    # https://msdn.microsoft.com/en-us/library/aa385829(v=vs.85).aspx\n    #\n    # Apparently the Windows Update framework in Windows Vista - Windows 8.1 has\n    # been changed quite a bit in Windows 10 / Server 2016. It is now called the\n    # Unified Update Platform (UUP). I haven't found an API or a Powershell\n    # commandlet for working with the the UUP. Perhaps there will be something\n    # forthcoming. The `win_lgpo` module might be an option for changing the\n    # Windows Update settings using local group policy.\n    ret = {'Success': True}\n\n    # Initialize the PyCom system\n    with salt.utils.winapi.Com():\n\n        # Create an AutoUpdate object\n        obj_au = win32com.client.Dispatch('Microsoft.Update.AutoUpdate')\n\n    # Create an AutoUpdate Settings Object\n    obj_au_settings = obj_au.Settings\n\n    # Only change the setting if it's passed\n    if level is not None:\n        obj_au_settings.NotificationLevel = int(level)\n        result = obj_au_settings.Save()\n        if result is None:\n            ret['Level'] = level\n        else:\n            ret['Comment'] = \"Settings failed to save. Check permissions.\"\n            ret['Success'] = False\n\n    if recommended is not None:\n        obj_au_settings.IncludeRecommendedUpdates = recommended\n        result = obj_au_settings.Save()\n        if result is None:\n            ret['Recommended'] = recommended\n        else:\n            ret['Comment'] = \"Settings failed to save. Check permissions.\"\n            ret['Success'] = False\n\n    if featured is not None:\n        obj_au_settings.FeaturedUpdatesEnabled = featured\n        result = obj_au_settings.Save()\n        if result is None:\n            ret['Featured'] = featured\n        else:\n            ret['Comment'] = \"Settings failed to save. Check permissions.\"\n            ret['Success'] = False\n\n    if elevated is not None:\n        obj_au_settings.NonAdministratorsElevated = elevated\n        result = obj_au_settings.Save()\n        if result is None:\n            ret['Elevated'] = elevated\n        else:\n            ret['Comment'] = \"Settings failed to save. Check permissions.\"\n            ret['Success'] = False\n\n    if day is not None:\n        # Check that day is valid\n        days = {'Everyday': 0,\n                'Sunday': 1,\n                'Monday': 2,\n                'Tuesday': 3,\n                'Wednesday': 4,\n                'Thursday': 5,\n                'Friday': 6,\n                'Saturday': 7}\n        if day not in days:\n            ret['Comment'] = \"Day needs to be one of the following: Everyday,\" \\\n                             \"Monday, Tuesday, Wednesday, Thursday, Friday, \" \\\n                             \"Saturday\"\n            ret['Success'] = False\n        else:\n            # Set the numeric equivalent for the day setting\n            obj_au_settings.ScheduledInstallationDay = days[day]\n            result = obj_au_settings.Save()\n            if result is None:\n                ret['Day'] = day\n            else:\n                ret['Comment'] = \"Settings failed to save. Check permissions.\"\n                ret['Success'] = False\n\n    if time is not None:\n        # Check for time as a string: if the time is not quoted, yaml will\n        # treat it as an integer\n        if not isinstance(time, six.string_types):\n            ret['Comment'] = \"Time argument needs to be a string; it may need to\"\\\n                             \"be quoted. Passed {0}. Time not set.\".format(time)\n            ret['Success'] = False\n        # Check for colon in the time\n        elif ':' not in time:\n            ret['Comment'] = \"Time argument needs to be in 00:00 format.\" \\\n                             \" Passed {0}. Time not set.\".format(time)\n            ret['Success'] = False\n        else:\n            # Split the time by :\n            t = time.split(\":\")\n            # We only need the hours value\n            obj_au_settings.FeaturedUpdatesEnabled = t[0]\n            result = obj_au_settings.Save()\n            if result is None:\n                ret['Time'] = time\n            else:\n                ret['Comment'] = \"Settings failed to save. Check permissions.\"\n                ret['Success'] = False\n\n    if msupdate is not None:\n        # Microsoft Update requires special handling\n        # First load the MS Update Service Manager\n        with salt.utils.winapi.Com():\n            obj_sm = win32com.client.Dispatch('Microsoft.Update.ServiceManager')\n\n        # Give it a bogus name\n        obj_sm.ClientApplicationID = \"My App\"\n\n        if msupdate:\n            # msupdate is true, so add it to the services\n            try:\n                obj_sm.AddService2('7971f918-a847-4430-9279-4a52d1efe18d', 7, '')\n                ret['msupdate'] = msupdate\n            except Exception as error:\n                hr, msg, exc, arg = error.args  # pylint: disable=W0633\n                # Consider checking for -2147024891 (0x80070005) Access Denied\n                ret['Comment'] = \"Failed with failure code: {0}\".format(exc[5])\n                ret['Success'] = False\n        else:\n            # msupdate is false, so remove it from the services\n            # check to see if the update is there or the RemoveService function\n            # will fail\n            if _get_msupdate_status():\n                # Service found, remove the service\n                try:\n                    obj_sm.RemoveService('7971f918-a847-4430-9279-4a52d1efe18d')\n                    ret['msupdate'] = msupdate\n                except Exception as error:\n                    hr, msg, exc, arg = error.args  # pylint: disable=W0633\n                    # Consider checking for the following\n                    # -2147024891 (0x80070005) Access Denied\n                    # -2145091564 (0x80248014) Service Not Found (shouldn't get\n                    # this with the check for _get_msupdate_status above\n                    ret['Comment'] = \"Failed with failure code: {0}\".format(exc[5])\n                    ret['Success'] = False\n            else:\n                ret['msupdate'] = msupdate\n\n    ret['Reboot'] = get_needs_reboot()\n\n    return ret"}]