[{"code": "def calculate_3D_elastic_energy(self, film, match, elasticity_tensor=None,\n                                    include_strain=False):\n        \"\"\"\n        Calculates the multi-plane elastic energy. Returns 999 if no elastic\n        tensor was given on init\n\n        Args:\n            film(Structure): conventional standard structure for the film\n            match(dictionary) : match dictionary from substrate analyzer\n            elasticity_tensor(ElasticTensor): elasticity tensor for the film\n            include_strain(bool): include strain in the output or not; changes\n             return from just the energy to a tuple with the energy and strain\n             in voigt notation\n        \"\"\"\n        if elasticity_tensor is None:\n            return 9999\n\n        # Get the appropriate surface structure\n        struc = SlabGenerator(self.film, match['film_miller'], 20, 15,\n                              primitive=False).get_slab().oriented_unit_cell\n\n        # Generate 3D lattice vectors for film super lattice\n        film_matrix = list(match['film_sl_vecs'])\n        film_matrix.append(np.cross(film_matrix[0], film_matrix[1]))\n\n        # Generate 3D lattice vectors for substrate super lattice\n        # Out of plane substrate super lattice has to be same length as\n        # Film out of plane vector to ensure no extra deformation in that\n        # direction\n        substrate_matrix = list(match['sub_sl_vecs'])\n        temp_sub = np.cross(substrate_matrix[0], substrate_matrix[1])\n        temp_sub = temp_sub * fast_norm(film_matrix[2]) / fast_norm(temp_sub)\n        substrate_matrix.append(temp_sub)\n\n        transform_matrix = np.transpose(np.linalg.solve(film_matrix,\n                                                        substrate_matrix))\n\n        dfm = Deformation(transform_matrix)\n\n        strain = dfm.green_lagrange_strain.convert_to_ieee(struc, initial_fit=False)\n\n        energy_density = elasticity_tensor.energy_density(\n            strain)\n\n        if include_strain:\n            return (film.volume * energy_density / len(film.sites), strain.von_mises_strain)\n        else:\n            return film.volume * energy_density / len(film.sites)", "code_tokens": ["def", "calculate_3D_elastic_energy", "(", "self", ",", "film", ",", "match", ",", "elasticity_tensor", "=", "None", ",", "include_strain", "=", "False", ")", ":", "if", "elasticity_tensor", "is", "None", ":", "return", "9999", "# Get the appropriate surface structure", "struc", "=", "SlabGenerator", "(", "self", ".", "film", ",", "match", "[", "'film_miller'", "]", ",", "20", ",", "15", ",", "primitive", "=", "False", ")", ".", "get_slab", "(", ")", ".", "oriented_unit_cell", "# Generate 3D lattice vectors for film super lattice", "film_matrix", "=", "list", "(", "match", "[", "'film_sl_vecs'", "]", ")", "film_matrix", ".", "append", "(", "np", ".", "cross", "(", "film_matrix", "[", "0", "]", ",", "film_matrix", "[", "1", "]", ")", ")", "# Generate 3D lattice vectors for substrate super lattice", "# Out of plane substrate super lattice has to be same length as", "# Film out of plane vector to ensure no extra deformation in that", "# direction", "substrate_matrix", "=", "list", "(", "match", "[", "'sub_sl_vecs'", "]", ")", "temp_sub", "=", "np", ".", "cross", "(", "substrate_matrix", "[", "0", "]", ",", "substrate_matrix", "[", "1", "]", ")", "temp_sub", "=", "temp_sub", "*", "fast_norm", "(", "film_matrix", "[", "2", "]", ")", "/", "fast_norm", "(", "temp_sub", ")", "substrate_matrix", ".", "append", "(", "temp_sub", ")", "transform_matrix", "=", "np", ".", "transpose", "(", "np", ".", "linalg", ".", "solve", "(", "film_matrix", ",", "substrate_matrix", ")", ")", "dfm", "=", "Deformation", "(", "transform_matrix", ")", "strain", "=", "dfm", ".", "green_lagrange_strain", ".", "convert_to_ieee", "(", "struc", ",", "initial_fit", "=", "False", ")", "energy_density", "=", "elasticity_tensor", ".", "energy_density", "(", "strain", ")", "if", "include_strain", ":", "return", "(", "film", ".", "volume", "*", "energy_density", "/", "len", "(", "film", ".", "sites", ")", ",", "strain", ".", "von_mises_strain", ")", "else", ":", "return", "film", ".", "volume", "*", "energy_density", "/", "len", "(", "film", ".", "sites", ")"], "original_string": "def calculate_3D_elastic_energy(self, film, match, elasticity_tensor=None,\n                                    include_strain=False):\n        \"\"\"\n        Calculates the multi-plane elastic energy. Returns 999 if no elastic\n        tensor was given on init\n\n        Args:\n            film(Structure): conventional standard structure for the film\n            match(dictionary) : match dictionary from substrate analyzer\n            elasticity_tensor(ElasticTensor): elasticity tensor for the film\n            include_strain(bool): include strain in the output or not; changes\n             return from just the energy to a tuple with the energy and strain\n             in voigt notation\n        \"\"\"\n        if elasticity_tensor is None:\n            return 9999\n\n        # Get the appropriate surface structure\n        struc = SlabGenerator(self.film, match['film_miller'], 20, 15,\n                              primitive=False).get_slab().oriented_unit_cell\n\n        # Generate 3D lattice vectors for film super lattice\n        film_matrix = list(match['film_sl_vecs'])\n        film_matrix.append(np.cross(film_matrix[0], film_matrix[1]))\n\n        # Generate 3D lattice vectors for substrate super lattice\n        # Out of plane substrate super lattice has to be same length as\n        # Film out of plane vector to ensure no extra deformation in that\n        # direction\n        substrate_matrix = list(match['sub_sl_vecs'])\n        temp_sub = np.cross(substrate_matrix[0], substrate_matrix[1])\n        temp_sub = temp_sub * fast_norm(film_matrix[2]) / fast_norm(temp_sub)\n        substrate_matrix.append(temp_sub)\n\n        transform_matrix = np.transpose(np.linalg.solve(film_matrix,\n                                                        substrate_matrix))\n\n        dfm = Deformation(transform_matrix)\n\n        strain = dfm.green_lagrange_strain.convert_to_ieee(struc, initial_fit=False)\n\n        energy_density = elasticity_tensor.energy_density(\n            strain)\n\n        if include_strain:\n            return (film.volume * energy_density / len(film.sites), strain.von_mises_strain)\n        else:\n            return film.volume * energy_density / len(film.sites)"}, {"code": "def _get_list_related_field(\n        self, field, filter_rel_field, page=None, page_size=None\n    ):\n        \"\"\"\n            Return a list of values for a related field\n\n        :param field: Marshmallow field\n        :param filter_rel_field: Filters for the related field\n        :param page: The page index\n        :param page_size: The page size\n        :return: (int, list) total record count and list of dict with id and value\n        \"\"\"\n        ret = list()\n        if isinstance(field, Related) or isinstance(field, RelatedList):\n            datamodel = self.datamodel.get_related_interface(field.name)\n            filters = datamodel.get_filters(datamodel.get_search_columns_list())\n            page, page_size = self._sanitize_page_args(page, page_size)\n            order_field = self.order_rel_fields.get(field.name)\n            if order_field:\n                order_column, order_direction = order_field\n            else:\n                order_column, order_direction = \"\", \"\"\n            if filter_rel_field:\n                filters = filters.add_filter_list(filter_rel_field)\n            count, values = datamodel.query(\n                filters, order_column, order_direction, page=page, page_size=page_size\n            )\n            for value in values:\n                ret.append({\"id\": datamodel.get_pk_value(value), \"value\": str(value)})\n        return count, ret", "code_tokens": ["def", "_get_list_related_field", "(", "self", ",", "field", ",", "filter_rel_field", ",", "page", "=", "None", ",", "page_size", "=", "None", ")", ":", "ret", "=", "list", "(", ")", "if", "isinstance", "(", "field", ",", "Related", ")", "or", "isinstance", "(", "field", ",", "RelatedList", ")", ":", "datamodel", "=", "self", ".", "datamodel", ".", "get_related_interface", "(", "field", ".", "name", ")", "filters", "=", "datamodel", ".", "get_filters", "(", "datamodel", ".", "get_search_columns_list", "(", ")", ")", "page", ",", "page_size", "=", "self", ".", "_sanitize_page_args", "(", "page", ",", "page_size", ")", "order_field", "=", "self", ".", "order_rel_fields", ".", "get", "(", "field", ".", "name", ")", "if", "order_field", ":", "order_column", ",", "order_direction", "=", "order_field", "else", ":", "order_column", ",", "order_direction", "=", "\"\"", ",", "\"\"", "if", "filter_rel_field", ":", "filters", "=", "filters", ".", "add_filter_list", "(", "filter_rel_field", ")", "count", ",", "values", "=", "datamodel", ".", "query", "(", "filters", ",", "order_column", ",", "order_direction", ",", "page", "=", "page", ",", "page_size", "=", "page_size", ")", "for", "value", "in", "values", ":", "ret", ".", "append", "(", "{", "\"id\"", ":", "datamodel", ".", "get_pk_value", "(", "value", ")", ",", "\"value\"", ":", "str", "(", "value", ")", "}", ")", "return", "count", ",", "ret"], "original_string": "def _get_list_related_field(\n        self, field, filter_rel_field, page=None, page_size=None\n    ):\n        \"\"\"\n            Return a list of values for a related field\n\n        :param field: Marshmallow field\n        :param filter_rel_field: Filters for the related field\n        :param page: The page index\n        :param page_size: The page size\n        :return: (int, list) total record count and list of dict with id and value\n        \"\"\"\n        ret = list()\n        if isinstance(field, Related) or isinstance(field, RelatedList):\n            datamodel = self.datamodel.get_related_interface(field.name)\n            filters = datamodel.get_filters(datamodel.get_search_columns_list())\n            page, page_size = self._sanitize_page_args(page, page_size)\n            order_field = self.order_rel_fields.get(field.name)\n            if order_field:\n                order_column, order_direction = order_field\n            else:\n                order_column, order_direction = \"\", \"\"\n            if filter_rel_field:\n                filters = filters.add_filter_list(filter_rel_field)\n            count, values = datamodel.query(\n                filters, order_column, order_direction, page=page, page_size=page_size\n            )\n            for value in values:\n                ret.append({\"id\": datamodel.get_pk_value(value), \"value\": str(value)})\n        return count, ret"}, {"code": "def AddFile(self, filepath):\n    \"\"\"Adds a file path as a source.\n\n    Args:\n      filepath: a string representing a path to the file.\n\n    Returns:\n      True if the file is not an already existing source.\n    \"\"\"\n    if filepath not in self._files:\n      self._files.add(filepath)\n      return True\n    return False", "code_tokens": ["def", "AddFile", "(", "self", ",", "filepath", ")", ":", "if", "filepath", "not", "in", "self", ".", "_files", ":", "self", ".", "_files", ".", "add", "(", "filepath", ")", "return", "True", "return", "False"], "original_string": "def AddFile(self, filepath):\n    \"\"\"Adds a file path as a source.\n\n    Args:\n      filepath: a string representing a path to the file.\n\n    Returns:\n      True if the file is not an already existing source.\n    \"\"\"\n    if filepath not in self._files:\n      self._files.add(filepath)\n      return True\n    return False"}, {"code": "def _update_rs_from_primary(\n        sds,\n        replica_set_name,\n        server_description,\n        max_set_version,\n        max_election_id):\n    \"\"\"Update topology description from a primary's ismaster response.\n\n    Pass in a dict of ServerDescriptions, current replica set name, the\n    ServerDescription we are processing, and the TopologyDescription's\n    max_set_version and max_election_id if any.\n\n    Returns (new topology type, new replica_set_name, new max_set_version,\n    new max_election_id).\n    \"\"\"\n    if replica_set_name is None:\n        replica_set_name = server_description.replica_set_name\n\n    elif replica_set_name != server_description.replica_set_name:\n        # We found a primary but it doesn't have the replica_set_name\n        # provided by the user.\n        sds.pop(server_description.address)\n        return (_check_has_primary(sds),\n                replica_set_name,\n                max_set_version,\n                max_election_id)\n\n    max_election_tuple = max_set_version, max_election_id\n    if None not in server_description.election_tuple:\n        if (None not in max_election_tuple and\n                max_election_tuple > server_description.election_tuple):\n\n            # Stale primary, set to type Unknown.\n            address = server_description.address\n            sds[address] = ServerDescription(address)\n            return (_check_has_primary(sds),\n                    replica_set_name,\n                    max_set_version,\n                    max_election_id)\n\n        max_election_id = server_description.election_id\n\n    if (server_description.set_version is not None and\n        (max_set_version is None or\n            server_description.set_version > max_set_version)):\n\n        max_set_version = server_description.set_version\n\n    # We've heard from the primary. Is it the same primary as before?\n    for server in sds.values():\n        if (server.server_type is SERVER_TYPE.RSPrimary\n                and server.address != server_description.address):\n\n            # Reset old primary's type to Unknown.\n            sds[server.address] = ServerDescription(server.address)\n\n            # There can be only one prior primary.\n            break\n\n    # Discover new hosts from this primary's response.\n    for new_address in server_description.all_hosts:\n        if new_address not in sds:\n            sds[new_address] = ServerDescription(new_address)\n\n    # Remove hosts not in the response.\n    for addr in set(sds) - server_description.all_hosts:\n        sds.pop(addr)\n\n    # If the host list differs from the seed list, we may not have a primary\n    # after all.\n    return (_check_has_primary(sds),\n            replica_set_name,\n            max_set_version,\n            max_election_id)", "code_tokens": ["def", "_update_rs_from_primary", "(", "sds", ",", "replica_set_name", ",", "server_description", ",", "max_set_version", ",", "max_election_id", ")", ":", "if", "replica_set_name", "is", "None", ":", "replica_set_name", "=", "server_description", ".", "replica_set_name", "elif", "replica_set_name", "!=", "server_description", ".", "replica_set_name", ":", "# We found a primary but it doesn't have the replica_set_name", "# provided by the user.", "sds", ".", "pop", "(", "server_description", ".", "address", ")", "return", "(", "_check_has_primary", "(", "sds", ")", ",", "replica_set_name", ",", "max_set_version", ",", "max_election_id", ")", "max_election_tuple", "=", "max_set_version", ",", "max_election_id", "if", "None", "not", "in", "server_description", ".", "election_tuple", ":", "if", "(", "None", "not", "in", "max_election_tuple", "and", "max_election_tuple", ">", "server_description", ".", "election_tuple", ")", ":", "# Stale primary, set to type Unknown.", "address", "=", "server_description", ".", "address", "sds", "[", "address", "]", "=", "ServerDescription", "(", "address", ")", "return", "(", "_check_has_primary", "(", "sds", ")", ",", "replica_set_name", ",", "max_set_version", ",", "max_election_id", ")", "max_election_id", "=", "server_description", ".", "election_id", "if", "(", "server_description", ".", "set_version", "is", "not", "None", "and", "(", "max_set_version", "is", "None", "or", "server_description", ".", "set_version", ">", "max_set_version", ")", ")", ":", "max_set_version", "=", "server_description", ".", "set_version", "# We've heard from the primary. Is it the same primary as before?", "for", "server", "in", "sds", ".", "values", "(", ")", ":", "if", "(", "server", ".", "server_type", "is", "SERVER_TYPE", ".", "RSPrimary", "and", "server", ".", "address", "!=", "server_description", ".", "address", ")", ":", "# Reset old primary's type to Unknown.", "sds", "[", "server", ".", "address", "]", "=", "ServerDescription", "(", "server", ".", "address", ")", "# There can be only one prior primary.", "break", "# Discover new hosts from this primary's response.", "for", "new_address", "in", "server_description", ".", "all_hosts", ":", "if", "new_address", "not", "in", "sds", ":", "sds", "[", "new_address", "]", "=", "ServerDescription", "(", "new_address", ")", "# Remove hosts not in the response.", "for", "addr", "in", "set", "(", "sds", ")", "-", "server_description", ".", "all_hosts", ":", "sds", ".", "pop", "(", "addr", ")", "# If the host list differs from the seed list, we may not have a primary", "# after all.", "return", "(", "_check_has_primary", "(", "sds", ")", ",", "replica_set_name", ",", "max_set_version", ",", "max_election_id", ")"], "original_string": "def _update_rs_from_primary(\n        sds,\n        replica_set_name,\n        server_description,\n        max_set_version,\n        max_election_id):\n    \"\"\"Update topology description from a primary's ismaster response.\n\n    Pass in a dict of ServerDescriptions, current replica set name, the\n    ServerDescription we are processing, and the TopologyDescription's\n    max_set_version and max_election_id if any.\n\n    Returns (new topology type, new replica_set_name, new max_set_version,\n    new max_election_id).\n    \"\"\"\n    if replica_set_name is None:\n        replica_set_name = server_description.replica_set_name\n\n    elif replica_set_name != server_description.replica_set_name:\n        # We found a primary but it doesn't have the replica_set_name\n        # provided by the user.\n        sds.pop(server_description.address)\n        return (_check_has_primary(sds),\n                replica_set_name,\n                max_set_version,\n                max_election_id)\n\n    max_election_tuple = max_set_version, max_election_id\n    if None not in server_description.election_tuple:\n        if (None not in max_election_tuple and\n                max_election_tuple > server_description.election_tuple):\n\n            # Stale primary, set to type Unknown.\n            address = server_description.address\n            sds[address] = ServerDescription(address)\n            return (_check_has_primary(sds),\n                    replica_set_name,\n                    max_set_version,\n                    max_election_id)\n\n        max_election_id = server_description.election_id\n\n    if (server_description.set_version is not None and\n        (max_set_version is None or\n            server_description.set_version > max_set_version)):\n\n        max_set_version = server_description.set_version\n\n    # We've heard from the primary. Is it the same primary as before?\n    for server in sds.values():\n        if (server.server_type is SERVER_TYPE.RSPrimary\n                and server.address != server_description.address):\n\n            # Reset old primary's type to Unknown.\n            sds[server.address] = ServerDescription(server.address)\n\n            # There can be only one prior primary.\n            break\n\n    # Discover new hosts from this primary's response.\n    for new_address in server_description.all_hosts:\n        if new_address not in sds:\n            sds[new_address] = ServerDescription(new_address)\n\n    # Remove hosts not in the response.\n    for addr in set(sds) - server_description.all_hosts:\n        sds.pop(addr)\n\n    # If the host list differs from the seed list, we may not have a primary\n    # after all.\n    return (_check_has_primary(sds),\n            replica_set_name,\n            max_set_version,\n            max_election_id)"}, {"code": "def get_config_path(module_id: str = None, ext: str = 'yaml') -> Path:\n    \"\"\"\n    Get path for configuration file. Defaulted to\n    ``~/.ehforwarderbot/profiles/profile_name/channel_id/config.yaml``.\n    \n    This method creates the queried path if not existing. The config file will\n    not be created, however.\n    \n    Args:\n        module_id (str): Module ID.\n        ext (Optional[Str]): Extension name of the config file.\n            Defaulted to ``\"yaml\"``.\n\n    Returns:\n        The path to the configuration file.\n    \"\"\"\n    if module_id:\n        config_path = get_data_path(module_id)\n    else:\n        profile = coordinator.profile\n        config_path = get_base_path() / 'profiles' / profile\n    if not config_path.exists():\n        config_path.mkdir(parents=True)\n    return config_path / \"config.{}\".format(ext)", "code_tokens": ["def", "get_config_path", "(", "module_id", ":", "str", "=", "None", ",", "ext", ":", "str", "=", "'yaml'", ")", "->", "Path", ":", "if", "module_id", ":", "config_path", "=", "get_data_path", "(", "module_id", ")", "else", ":", "profile", "=", "coordinator", ".", "profile", "config_path", "=", "get_base_path", "(", ")", "/", "'profiles'", "/", "profile", "if", "not", "config_path", ".", "exists", "(", ")", ":", "config_path", ".", "mkdir", "(", "parents", "=", "True", ")", "return", "config_path", "/", "\"config.{}\"", ".", "format", "(", "ext", ")"], "original_string": "def get_config_path(module_id: str = None, ext: str = 'yaml') -> Path:\n    \"\"\"\n    Get path for configuration file. Defaulted to\n    ``~/.ehforwarderbot/profiles/profile_name/channel_id/config.yaml``.\n    \n    This method creates the queried path if not existing. The config file will\n    not be created, however.\n    \n    Args:\n        module_id (str): Module ID.\n        ext (Optional[Str]): Extension name of the config file.\n            Defaulted to ``\"yaml\"``.\n\n    Returns:\n        The path to the configuration file.\n    \"\"\"\n    if module_id:\n        config_path = get_data_path(module_id)\n    else:\n        profile = coordinator.profile\n        config_path = get_base_path() / 'profiles' / profile\n    if not config_path.exists():\n        config_path.mkdir(parents=True)\n    return config_path / \"config.{}\".format(ext)"}, {"code": "def stop(self):\n        \"\"\"Use this method to manually stop the Client.\n        Requires no parameters.\n\n        Raises:\n            ``ConnectionError`` in case you try to stop an already stopped Client.\n        \"\"\"\n        if not self.is_started:\n            raise ConnectionError(\"Client is already stopped\")\n\n        if self.takeout_id:\n            self.send(functions.account.FinishTakeoutSession())\n            log.warning(\"Takeout session {} finished\".format(self.takeout_id))\n\n        Syncer.remove(self)\n        self.dispatcher.stop()\n\n        for _ in range(self.DOWNLOAD_WORKERS):\n            self.download_queue.put(None)\n\n        for i in self.download_workers_list:\n            i.join()\n\n        self.download_workers_list.clear()\n\n        for _ in range(self.UPDATES_WORKERS):\n            self.updates_queue.put(None)\n\n        for i in self.updates_workers_list:\n            i.join()\n\n        self.updates_workers_list.clear()\n\n        for i in self.media_sessions.values():\n            i.stop()\n\n        self.media_sessions.clear()\n\n        self.is_started = False\n        self.session.stop()\n\n        return self", "code_tokens": ["def", "stop", "(", "self", ")", ":", "if", "not", "self", ".", "is_started", ":", "raise", "ConnectionError", "(", "\"Client is already stopped\"", ")", "if", "self", ".", "takeout_id", ":", "self", ".", "send", "(", "functions", ".", "account", ".", "FinishTakeoutSession", "(", ")", ")", "log", ".", "warning", "(", "\"Takeout session {} finished\"", ".", "format", "(", "self", ".", "takeout_id", ")", ")", "Syncer", ".", "remove", "(", "self", ")", "self", ".", "dispatcher", ".", "stop", "(", ")", "for", "_", "in", "range", "(", "self", ".", "DOWNLOAD_WORKERS", ")", ":", "self", ".", "download_queue", ".", "put", "(", "None", ")", "for", "i", "in", "self", ".", "download_workers_list", ":", "i", ".", "join", "(", ")", "self", ".", "download_workers_list", ".", "clear", "(", ")", "for", "_", "in", "range", "(", "self", ".", "UPDATES_WORKERS", ")", ":", "self", ".", "updates_queue", ".", "put", "(", "None", ")", "for", "i", "in", "self", ".", "updates_workers_list", ":", "i", ".", "join", "(", ")", "self", ".", "updates_workers_list", ".", "clear", "(", ")", "for", "i", "in", "self", ".", "media_sessions", ".", "values", "(", ")", ":", "i", ".", "stop", "(", ")", "self", ".", "media_sessions", ".", "clear", "(", ")", "self", ".", "is_started", "=", "False", "self", ".", "session", ".", "stop", "(", ")", "return", "self"], "original_string": "def stop(self):\n        \"\"\"Use this method to manually stop the Client.\n        Requires no parameters.\n\n        Raises:\n            ``ConnectionError`` in case you try to stop an already stopped Client.\n        \"\"\"\n        if not self.is_started:\n            raise ConnectionError(\"Client is already stopped\")\n\n        if self.takeout_id:\n            self.send(functions.account.FinishTakeoutSession())\n            log.warning(\"Takeout session {} finished\".format(self.takeout_id))\n\n        Syncer.remove(self)\n        self.dispatcher.stop()\n\n        for _ in range(self.DOWNLOAD_WORKERS):\n            self.download_queue.put(None)\n\n        for i in self.download_workers_list:\n            i.join()\n\n        self.download_workers_list.clear()\n\n        for _ in range(self.UPDATES_WORKERS):\n            self.updates_queue.put(None)\n\n        for i in self.updates_workers_list:\n            i.join()\n\n        self.updates_workers_list.clear()\n\n        for i in self.media_sessions.values():\n            i.stop()\n\n        self.media_sessions.clear()\n\n        self.is_started = False\n        self.session.stop()\n\n        return self"}, {"code": "def gen_random_string(str_len):\n    \"\"\" generate random string with specified length\n    \"\"\"\n    return ''.join(\n        random.choice(string.ascii_letters + string.digits) for _ in range(str_len))", "code_tokens": ["def", "gen_random_string", "(", "str_len", ")", ":", "return", "''", ".", "join", "(", "random", ".", "choice", "(", "string", ".", "ascii_letters", "+", "string", ".", "digits", ")", "for", "_", "in", "range", "(", "str_len", ")", ")"], "original_string": "def gen_random_string(str_len):\n    \"\"\" generate random string with specified length\n    \"\"\"\n    return ''.join(\n        random.choice(string.ascii_letters + string.digits) for _ in range(str_len))"}, {"code": "def mount(dmg):\n    '''\n    Attempt to mount a dmg file to a temporary location and return the\n    location of the pkg file inside\n\n    Args:\n        dmg (str): The location of the dmg file to mount\n\n    Returns:\n        tuple: Tuple containing the results of the command along with the mount\n               point\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' macpackage.mount /tmp/software.dmg\n    '''\n\n    temp_dir = __salt__['temp.dir'](prefix='dmg-')\n\n    cmd = 'hdiutil attach -readonly -nobrowse -mountpoint {0} \"{1}\"'.format(temp_dir, dmg)\n\n    return __salt__['cmd.run'](cmd), temp_dir", "code_tokens": ["def", "mount", "(", "dmg", ")", ":", "temp_dir", "=", "__salt__", "[", "'temp.dir'", "]", "(", "prefix", "=", "'dmg-'", ")", "cmd", "=", "'hdiutil attach -readonly -nobrowse -mountpoint {0} \"{1}\"'", ".", "format", "(", "temp_dir", ",", "dmg", ")", "return", "__salt__", "[", "'cmd.run'", "]", "(", "cmd", ")", ",", "temp_dir"], "original_string": "def mount(dmg):\n    '''\n    Attempt to mount a dmg file to a temporary location and return the\n    location of the pkg file inside\n\n    Args:\n        dmg (str): The location of the dmg file to mount\n\n    Returns:\n        tuple: Tuple containing the results of the command along with the mount\n               point\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' macpackage.mount /tmp/software.dmg\n    '''\n\n    temp_dir = __salt__['temp.dir'](prefix='dmg-')\n\n    cmd = 'hdiutil attach -readonly -nobrowse -mountpoint {0} \"{1}\"'.format(temp_dir, dmg)\n\n    return __salt__['cmd.run'](cmd), temp_dir"}, {"code": "def Set(self,\n          subject,\n          attribute,\n          value,\n          timestamp=None,\n          replace=True,\n          sync=True):\n    \"\"\"Set the value into the data store.\"\"\"\n    subject = utils.SmartUnicode(subject)\n\n    _ = sync\n    attribute = utils.SmartUnicode(attribute)\n\n    if timestamp is None or timestamp == self.NEWEST_TIMESTAMP:\n      timestamp = time.time() * 1000000\n\n    if subject not in self.subjects:\n      self.subjects[subject] = {}\n\n    if replace or attribute not in self.subjects[subject]:\n      self.subjects[subject][attribute] = []\n\n    encoded_value = self._value_converter.Encode(attribute, value)\n    self.subjects[subject][attribute].append([encoded_value, int(timestamp)])\n    self.subjects[subject][attribute].sort(key=lambda x: x[1])", "code_tokens": ["def", "Set", "(", "self", ",", "subject", ",", "attribute", ",", "value", ",", "timestamp", "=", "None", ",", "replace", "=", "True", ",", "sync", "=", "True", ")", ":", "subject", "=", "utils", ".", "SmartUnicode", "(", "subject", ")", "_", "=", "sync", "attribute", "=", "utils", ".", "SmartUnicode", "(", "attribute", ")", "if", "timestamp", "is", "None", "or", "timestamp", "==", "self", ".", "NEWEST_TIMESTAMP", ":", "timestamp", "=", "time", ".", "time", "(", ")", "*", "1000000", "if", "subject", "not", "in", "self", ".", "subjects", ":", "self", ".", "subjects", "[", "subject", "]", "=", "{", "}", "if", "replace", "or", "attribute", "not", "in", "self", ".", "subjects", "[", "subject", "]", ":", "self", ".", "subjects", "[", "subject", "]", "[", "attribute", "]", "=", "[", "]", "encoded_value", "=", "self", ".", "_value_converter", ".", "Encode", "(", "attribute", ",", "value", ")", "self", ".", "subjects", "[", "subject", "]", "[", "attribute", "]", ".", "append", "(", "[", "encoded_value", ",", "int", "(", "timestamp", ")", "]", ")", "self", ".", "subjects", "[", "subject", "]", "[", "attribute", "]", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")"], "original_string": "def Set(self,\n          subject,\n          attribute,\n          value,\n          timestamp=None,\n          replace=True,\n          sync=True):\n    \"\"\"Set the value into the data store.\"\"\"\n    subject = utils.SmartUnicode(subject)\n\n    _ = sync\n    attribute = utils.SmartUnicode(attribute)\n\n    if timestamp is None or timestamp == self.NEWEST_TIMESTAMP:\n      timestamp = time.time() * 1000000\n\n    if subject not in self.subjects:\n      self.subjects[subject] = {}\n\n    if replace or attribute not in self.subjects[subject]:\n      self.subjects[subject][attribute] = []\n\n    encoded_value = self._value_converter.Encode(attribute, value)\n    self.subjects[subject][attribute].append([encoded_value, int(timestamp)])\n    self.subjects[subject][attribute].sort(key=lambda x: x[1])"}, {"code": "def send(self, data):\n        \"\"\"\n        :param data:\n        :type data: bytearray | bytes\n        :return:\n        :rtype:\n        \"\"\"\n        data = bytes(data) if type(data) is not bytes else data\n        self._wa_noiseprotocol.send(data)", "code_tokens": ["def", "send", "(", "self", ",", "data", ")", ":", "data", "=", "bytes", "(", "data", ")", "if", "type", "(", "data", ")", "is", "not", "bytes", "else", "data", "self", ".", "_wa_noiseprotocol", ".", "send", "(", "data", ")"], "original_string": "def send(self, data):\n        \"\"\"\n        :param data:\n        :type data: bytearray | bytes\n        :return:\n        :rtype:\n        \"\"\"\n        data = bytes(data) if type(data) is not bytes else data\n        self._wa_noiseprotocol.send(data)"}, {"code": "def msg_curse(self, args=None, max_width=None):\n        \"\"\"Return the dict to display in the curse interface.\"\"\"\n        # Init the return message\n        ret = []\n\n        # Only process if stats exist, not empty (issue #871) and plugin not disabled\n        if not self.stats or (self.stats == {}) or self.is_disable():\n            return ret\n\n        # Build the string message\n        # Header\n        msg = '{:8}'.format('LOAD')\n        ret.append(self.curse_add_line(msg, \"TITLE\"))\n        # Core number\n        if 'cpucore' in self.stats and self.stats['cpucore'] > 0:\n            msg = '{}-core'.format(int(self.stats['cpucore']))\n            ret.append(self.curse_add_line(msg))\n        # New line\n        ret.append(self.curse_new_line())\n        # 1min load\n        msg = '{:8}'.format('1 min:')\n        ret.append(self.curse_add_line(msg))\n        msg = '{:>6.2f}'.format(self.stats['min1'])\n        ret.append(self.curse_add_line(msg))\n        # New line\n        ret.append(self.curse_new_line())\n        # 5min load\n        msg = '{:8}'.format('5 min:')\n        ret.append(self.curse_add_line(msg))\n        msg = '{:>6.2f}'.format(self.stats['min5'])\n        ret.append(self.curse_add_line(\n            msg, self.get_views(key='min5', option='decoration')))\n        # New line\n        ret.append(self.curse_new_line())\n        # 15min load\n        msg = '{:8}'.format('15 min:')\n        ret.append(self.curse_add_line(msg))\n        msg = '{:>6.2f}'.format(self.stats['min15'])\n        ret.append(self.curse_add_line(\n            msg, self.get_views(key='min15', option='decoration')))\n\n        return ret", "code_tokens": ["def", "msg_curse", "(", "self", ",", "args", "=", "None", ",", "max_width", "=", "None", ")", ":", "# Init the return message", "ret", "=", "[", "]", "# Only process if stats exist, not empty (issue #871) and plugin not disabled", "if", "not", "self", ".", "stats", "or", "(", "self", ".", "stats", "==", "{", "}", ")", "or", "self", ".", "is_disable", "(", ")", ":", "return", "ret", "# Build the string message", "# Header", "msg", "=", "'{:8}'", ".", "format", "(", "'LOAD'", ")", "ret", ".", "append", "(", "self", ".", "curse_add_line", "(", "msg", ",", "\"TITLE\"", ")", ")", "# Core number", "if", "'cpucore'", "in", "self", ".", "stats", "and", "self", ".", "stats", "[", "'cpucore'", "]", ">", "0", ":", "msg", "=", "'{}-core'", ".", "format", "(", "int", "(", "self", ".", "stats", "[", "'cpucore'", "]", ")", ")", "ret", ".", "append", "(", "self", ".", "curse_add_line", "(", "msg", ")", ")", "# New line", "ret", ".", "append", "(", "self", ".", "curse_new_line", "(", ")", ")", "# 1min load", "msg", "=", "'{:8}'", ".", "format", "(", "'1 min:'", ")", "ret", ".", "append", "(", "self", ".", "curse_add_line", "(", "msg", ")", ")", "msg", "=", "'{:>6.2f}'", ".", "format", "(", "self", ".", "stats", "[", "'min1'", "]", ")", "ret", ".", "append", "(", "self", ".", "curse_add_line", "(", "msg", ")", ")", "# New line", "ret", ".", "append", "(", "self", ".", "curse_new_line", "(", ")", ")", "# 5min load", "msg", "=", "'{:8}'", ".", "format", "(", "'5 min:'", ")", "ret", ".", "append", "(", "self", ".", "curse_add_line", "(", "msg", ")", ")", "msg", "=", "'{:>6.2f}'", ".", "format", "(", "self", ".", "stats", "[", "'min5'", "]", ")", "ret", ".", "append", "(", "self", ".", "curse_add_line", "(", "msg", ",", "self", ".", "get_views", "(", "key", "=", "'min5'", ",", "option", "=", "'decoration'", ")", ")", ")", "# New line", "ret", ".", "append", "(", "self", ".", "curse_new_line", "(", ")", ")", "# 15min load", "msg", "=", "'{:8}'", ".", "format", "(", "'15 min:'", ")", "ret", ".", "append", "(", "self", ".", "curse_add_line", "(", "msg", ")", ")", "msg", "=", "'{:>6.2f}'", ".", "format", "(", "self", ".", "stats", "[", "'min15'", "]", ")", "ret", ".", "append", "(", "self", ".", "curse_add_line", "(", "msg", ",", "self", ".", "get_views", "(", "key", "=", "'min15'", ",", "option", "=", "'decoration'", ")", ")", ")", "return", "ret"], "original_string": "def msg_curse(self, args=None, max_width=None):\n        \"\"\"Return the dict to display in the curse interface.\"\"\"\n        # Init the return message\n        ret = []\n\n        # Only process if stats exist, not empty (issue #871) and plugin not disabled\n        if not self.stats or (self.stats == {}) or self.is_disable():\n            return ret\n\n        # Build the string message\n        # Header\n        msg = '{:8}'.format('LOAD')\n        ret.append(self.curse_add_line(msg, \"TITLE\"))\n        # Core number\n        if 'cpucore' in self.stats and self.stats['cpucore'] > 0:\n            msg = '{}-core'.format(int(self.stats['cpucore']))\n            ret.append(self.curse_add_line(msg))\n        # New line\n        ret.append(self.curse_new_line())\n        # 1min load\n        msg = '{:8}'.format('1 min:')\n        ret.append(self.curse_add_line(msg))\n        msg = '{:>6.2f}'.format(self.stats['min1'])\n        ret.append(self.curse_add_line(msg))\n        # New line\n        ret.append(self.curse_new_line())\n        # 5min load\n        msg = '{:8}'.format('5 min:')\n        ret.append(self.curse_add_line(msg))\n        msg = '{:>6.2f}'.format(self.stats['min5'])\n        ret.append(self.curse_add_line(\n            msg, self.get_views(key='min5', option='decoration')))\n        # New line\n        ret.append(self.curse_new_line())\n        # 15min load\n        msg = '{:8}'.format('15 min:')\n        ret.append(self.curse_add_line(msg))\n        msg = '{:>6.2f}'.format(self.stats['min15'])\n        ret.append(self.curse_add_line(\n            msg, self.get_views(key='min15', option='decoration')))\n\n        return ret"}, {"code": "def Run(self, args):\n    \"\"\"Fingerprint a file.\"\"\"\n    with vfs.VFSOpen(\n        args.pathspec, progress_callback=self.Progress) as file_obj:\n      fingerprinter = Fingerprinter(self.Progress, file_obj)\n      response = rdf_client_action.FingerprintResponse()\n      response.pathspec = file_obj.pathspec\n      if args.tuples:\n        tuples = args.tuples\n      else:\n        # There are none selected -- we will cover everything\n        tuples = list()\n        for k in self._fingerprint_types:\n          tuples.append(rdf_client_action.FingerprintTuple(fp_type=k))\n\n      for finger in tuples:\n        hashers = [self._hash_types[h] for h in finger.hashers] or None\n        if finger.fp_type in self._fingerprint_types:\n          invoke = self._fingerprint_types[finger.fp_type]\n          res = invoke(fingerprinter, hashers)\n          if res:\n            response.matching_types.append(finger.fp_type)\n        else:\n          raise RuntimeError(\n              \"Encountered unknown fingerprint type. %s\" % finger.fp_type)\n\n      # Structure of the results is a list of dicts, each containing the\n      # name of the hashing method, hashes for enabled hash algorithms,\n      # and auxilliary data where present (e.g. signature blobs).\n      # Also see Fingerprint:HashIt()\n      response.results = fingerprinter.HashIt()\n\n      # We now return data in a more structured form.\n      for result in response.results:\n        if result.GetItem(\"name\") == \"generic\":\n          for hash_type in [\"md5\", \"sha1\", \"sha256\"]:\n            value = result.GetItem(hash_type)\n            if value is not None:\n              setattr(response.hash, hash_type, value)\n\n        if result[\"name\"] == \"pecoff\":\n          for hash_type in [\"md5\", \"sha1\", \"sha256\"]:\n            value = result.GetItem(hash_type)\n            if value:\n              setattr(response.hash, \"pecoff_\" + hash_type, value)\n\n          signed_data = result.GetItem(\"SignedData\", [])\n          for data in signed_data:\n            response.hash.signed_data.Append(\n                revision=data[0], cert_type=data[1], certificate=data[2])\n\n      self.SendReply(response)", "code_tokens": ["def", "Run", "(", "self", ",", "args", ")", ":", "with", "vfs", ".", "VFSOpen", "(", "args", ".", "pathspec", ",", "progress_callback", "=", "self", ".", "Progress", ")", "as", "file_obj", ":", "fingerprinter", "=", "Fingerprinter", "(", "self", ".", "Progress", ",", "file_obj", ")", "response", "=", "rdf_client_action", ".", "FingerprintResponse", "(", ")", "response", ".", "pathspec", "=", "file_obj", ".", "pathspec", "if", "args", ".", "tuples", ":", "tuples", "=", "args", ".", "tuples", "else", ":", "# There are none selected -- we will cover everything", "tuples", "=", "list", "(", ")", "for", "k", "in", "self", ".", "_fingerprint_types", ":", "tuples", ".", "append", "(", "rdf_client_action", ".", "FingerprintTuple", "(", "fp_type", "=", "k", ")", ")", "for", "finger", "in", "tuples", ":", "hashers", "=", "[", "self", ".", "_hash_types", "[", "h", "]", "for", "h", "in", "finger", ".", "hashers", "]", "or", "None", "if", "finger", ".", "fp_type", "in", "self", ".", "_fingerprint_types", ":", "invoke", "=", "self", ".", "_fingerprint_types", "[", "finger", ".", "fp_type", "]", "res", "=", "invoke", "(", "fingerprinter", ",", "hashers", ")", "if", "res", ":", "response", ".", "matching_types", ".", "append", "(", "finger", ".", "fp_type", ")", "else", ":", "raise", "RuntimeError", "(", "\"Encountered unknown fingerprint type. %s\"", "%", "finger", ".", "fp_type", ")", "# Structure of the results is a list of dicts, each containing the", "# name of the hashing method, hashes for enabled hash algorithms,", "# and auxilliary data where present (e.g. signature blobs).", "# Also see Fingerprint:HashIt()", "response", ".", "results", "=", "fingerprinter", ".", "HashIt", "(", ")", "# We now return data in a more structured form.", "for", "result", "in", "response", ".", "results", ":", "if", "result", ".", "GetItem", "(", "\"name\"", ")", "==", "\"generic\"", ":", "for", "hash_type", "in", "[", "\"md5\"", ",", "\"sha1\"", ",", "\"sha256\"", "]", ":", "value", "=", "result", ".", "GetItem", "(", "hash_type", ")", "if", "value", "is", "not", "None", ":", "setattr", "(", "response", ".", "hash", ",", "hash_type", ",", "value", ")", "if", "result", "[", "\"name\"", "]", "==", "\"pecoff\"", ":", "for", "hash_type", "in", "[", "\"md5\"", ",", "\"sha1\"", ",", "\"sha256\"", "]", ":", "value", "=", "result", ".", "GetItem", "(", "hash_type", ")", "if", "value", ":", "setattr", "(", "response", ".", "hash", ",", "\"pecoff_\"", "+", "hash_type", ",", "value", ")", "signed_data", "=", "result", ".", "GetItem", "(", "\"SignedData\"", ",", "[", "]", ")", "for", "data", "in", "signed_data", ":", "response", ".", "hash", ".", "signed_data", ".", "Append", "(", "revision", "=", "data", "[", "0", "]", ",", "cert_type", "=", "data", "[", "1", "]", ",", "certificate", "=", "data", "[", "2", "]", ")", "self", ".", "SendReply", "(", "response", ")"], "original_string": "def Run(self, args):\n    \"\"\"Fingerprint a file.\"\"\"\n    with vfs.VFSOpen(\n        args.pathspec, progress_callback=self.Progress) as file_obj:\n      fingerprinter = Fingerprinter(self.Progress, file_obj)\n      response = rdf_client_action.FingerprintResponse()\n      response.pathspec = file_obj.pathspec\n      if args.tuples:\n        tuples = args.tuples\n      else:\n        # There are none selected -- we will cover everything\n        tuples = list()\n        for k in self._fingerprint_types:\n          tuples.append(rdf_client_action.FingerprintTuple(fp_type=k))\n\n      for finger in tuples:\n        hashers = [self._hash_types[h] for h in finger.hashers] or None\n        if finger.fp_type in self._fingerprint_types:\n          invoke = self._fingerprint_types[finger.fp_type]\n          res = invoke(fingerprinter, hashers)\n          if res:\n            response.matching_types.append(finger.fp_type)\n        else:\n          raise RuntimeError(\n              \"Encountered unknown fingerprint type. %s\" % finger.fp_type)\n\n      # Structure of the results is a list of dicts, each containing the\n      # name of the hashing method, hashes for enabled hash algorithms,\n      # and auxilliary data where present (e.g. signature blobs).\n      # Also see Fingerprint:HashIt()\n      response.results = fingerprinter.HashIt()\n\n      # We now return data in a more structured form.\n      for result in response.results:\n        if result.GetItem(\"name\") == \"generic\":\n          for hash_type in [\"md5\", \"sha1\", \"sha256\"]:\n            value = result.GetItem(hash_type)\n            if value is not None:\n              setattr(response.hash, hash_type, value)\n\n        if result[\"name\"] == \"pecoff\":\n          for hash_type in [\"md5\", \"sha1\", \"sha256\"]:\n            value = result.GetItem(hash_type)\n            if value:\n              setattr(response.hash, \"pecoff_\" + hash_type, value)\n\n          signed_data = result.GetItem(\"SignedData\", [])\n          for data in signed_data:\n            response.hash.signed_data.Append(\n                revision=data[0], cert_type=data[1], certificate=data[2])\n\n      self.SendReply(response)"}, {"code": "def process_raw_data(self, fname, max_size):\n        \"\"\"\n        Loads data from the input file.\n\n        :param fname: input file name\n        :param max_size: loads at most 'max_size' samples from the input file,\n            if None loads the entire dataset\n        \"\"\"\n        logging.info(f'Processing data from {fname}')\n        data = []\n        with open(fname) as dfile:\n            for idx, line in enumerate(dfile):\n                if max_size and idx == max_size:\n                    break\n                data.append(line)\n        return data", "code_tokens": ["def", "process_raw_data", "(", "self", ",", "fname", ",", "max_size", ")", ":", "logging", ".", "info", "(", "f'Processing data from {fname}'", ")", "data", "=", "[", "]", "with", "open", "(", "fname", ")", "as", "dfile", ":", "for", "idx", ",", "line", "in", "enumerate", "(", "dfile", ")", ":", "if", "max_size", "and", "idx", "==", "max_size", ":", "break", "data", ".", "append", "(", "line", ")", "return", "data"], "original_string": "def process_raw_data(self, fname, max_size):\n        \"\"\"\n        Loads data from the input file.\n\n        :param fname: input file name\n        :param max_size: loads at most 'max_size' samples from the input file,\n            if None loads the entire dataset\n        \"\"\"\n        logging.info(f'Processing data from {fname}')\n        data = []\n        with open(fname) as dfile:\n            for idx, line in enumerate(dfile):\n                if max_size and idx == max_size:\n                    break\n                data.append(line)\n        return data"}, {"code": "def auth_decrypt(self, A, C, seq_num):\n        \"\"\"\n        Decrypt the data and verify the authentication code (in this order).\n        Note that TLS 1.3 is not supposed to use any additional data A.\n        If the verification fails, an AEADTagError is raised. It is the user's\n        responsibility to catch it if deemed useful. If we lack the key, we\n        raise a CipherError which contains the encrypted input.\n        \"\"\"\n        C, mac = C[:-self.tag_len], C[-self.tag_len:]\n        if False in six.itervalues(self.ready):\n            raise CipherError(C, mac)\n\n        if hasattr(self, \"pc_cls\"):\n            self._cipher.mode._initialization_vector = self._get_nonce(seq_num)\n            self._cipher.mode._tag = mac\n            decryptor = self._cipher.decryptor()\n            decryptor.authenticate_additional_data(A)\n            P = decryptor.update(C)\n            try:\n                decryptor.finalize()\n            except InvalidTag:\n                raise AEADTagError(P, mac)\n        else:\n            try:\n                if (conf.crypto_valid_advanced and\n                        isinstance(self._cipher, AESCCM)):\n                    P = self._cipher.decrypt(self._get_nonce(seq_num), C + mac, A,  # noqa: E501\n                                             tag_length=self.tag_len)\n                else:\n                    if (conf.crypto_valid_advanced and\n                            isinstance(self, Cipher_CHACHA20_POLY1305)):\n                        A += struct.pack(\"!H\", len(C))\n                    P = self._cipher.decrypt(self._get_nonce(seq_num), C + mac, A)  # noqa: E501\n            except InvalidTag:\n                raise AEADTagError(\"<unauthenticated data>\", mac)\n        return P, mac", "code_tokens": ["def", "auth_decrypt", "(", "self", ",", "A", ",", "C", ",", "seq_num", ")", ":", "C", ",", "mac", "=", "C", "[", ":", "-", "self", ".", "tag_len", "]", ",", "C", "[", "-", "self", ".", "tag_len", ":", "]", "if", "False", "in", "six", ".", "itervalues", "(", "self", ".", "ready", ")", ":", "raise", "CipherError", "(", "C", ",", "mac", ")", "if", "hasattr", "(", "self", ",", "\"pc_cls\"", ")", ":", "self", ".", "_cipher", ".", "mode", ".", "_initialization_vector", "=", "self", ".", "_get_nonce", "(", "seq_num", ")", "self", ".", "_cipher", ".", "mode", ".", "_tag", "=", "mac", "decryptor", "=", "self", ".", "_cipher", ".", "decryptor", "(", ")", "decryptor", ".", "authenticate_additional_data", "(", "A", ")", "P", "=", "decryptor", ".", "update", "(", "C", ")", "try", ":", "decryptor", ".", "finalize", "(", ")", "except", "InvalidTag", ":", "raise", "AEADTagError", "(", "P", ",", "mac", ")", "else", ":", "try", ":", "if", "(", "conf", ".", "crypto_valid_advanced", "and", "isinstance", "(", "self", ".", "_cipher", ",", "AESCCM", ")", ")", ":", "P", "=", "self", ".", "_cipher", ".", "decrypt", "(", "self", ".", "_get_nonce", "(", "seq_num", ")", ",", "C", "+", "mac", ",", "A", ",", "# noqa: E501", "tag_length", "=", "self", ".", "tag_len", ")", "else", ":", "if", "(", "conf", ".", "crypto_valid_advanced", "and", "isinstance", "(", "self", ",", "Cipher_CHACHA20_POLY1305", ")", ")", ":", "A", "+=", "struct", ".", "pack", "(", "\"!H\"", ",", "len", "(", "C", ")", ")", "P", "=", "self", ".", "_cipher", ".", "decrypt", "(", "self", ".", "_get_nonce", "(", "seq_num", ")", ",", "C", "+", "mac", ",", "A", ")", "# noqa: E501", "except", "InvalidTag", ":", "raise", "AEADTagError", "(", "\"<unauthenticated data>\"", ",", "mac", ")", "return", "P", ",", "mac"], "original_string": "def auth_decrypt(self, A, C, seq_num):\n        \"\"\"\n        Decrypt the data and verify the authentication code (in this order).\n        Note that TLS 1.3 is not supposed to use any additional data A.\n        If the verification fails, an AEADTagError is raised. It is the user's\n        responsibility to catch it if deemed useful. If we lack the key, we\n        raise a CipherError which contains the encrypted input.\n        \"\"\"\n        C, mac = C[:-self.tag_len], C[-self.tag_len:]\n        if False in six.itervalues(self.ready):\n            raise CipherError(C, mac)\n\n        if hasattr(self, \"pc_cls\"):\n            self._cipher.mode._initialization_vector = self._get_nonce(seq_num)\n            self._cipher.mode._tag = mac\n            decryptor = self._cipher.decryptor()\n            decryptor.authenticate_additional_data(A)\n            P = decryptor.update(C)\n            try:\n                decryptor.finalize()\n            except InvalidTag:\n                raise AEADTagError(P, mac)\n        else:\n            try:\n                if (conf.crypto_valid_advanced and\n                        isinstance(self._cipher, AESCCM)):\n                    P = self._cipher.decrypt(self._get_nonce(seq_num), C + mac, A,  # noqa: E501\n                                             tag_length=self.tag_len)\n                else:\n                    if (conf.crypto_valid_advanced and\n                            isinstance(self, Cipher_CHACHA20_POLY1305)):\n                        A += struct.pack(\"!H\", len(C))\n                    P = self._cipher.decrypt(self._get_nonce(seq_num), C + mac, A)  # noqa: E501\n            except InvalidTag:\n                raise AEADTagError(\"<unauthenticated data>\", mac)\n        return P, mac"}, {"code": "def _set_cluster(self):\n        \"\"\"\n        Compute and set the cluster of atoms as a Molecule object. The siteato\n        coordinates are translated such that the absorbing atom(aka central\n        atom) is at the origin.\n\n        Returns:\n            Molecule\n        \"\"\"\n        center = self.struct[self.center_index].coords\n        sphere = self.struct.get_neighbors(self.struct[self.center_index], self.radius)\n\n        symbols = [self.absorbing_atom]\n        coords = [[0, 0, 0]]\n        for i, site_dist in enumerate(sphere):\n            site_symbol = re.sub(r\"[^aA-zZ]+\", \"\", site_dist[0].species_string)\n            symbols.append(site_symbol)\n            coords.append(site_dist[0].coords - center)\n        return Molecule(symbols, coords)", "code_tokens": ["def", "_set_cluster", "(", "self", ")", ":", "center", "=", "self", ".", "struct", "[", "self", ".", "center_index", "]", ".", "coords", "sphere", "=", "self", ".", "struct", ".", "get_neighbors", "(", "self", ".", "struct", "[", "self", ".", "center_index", "]", ",", "self", ".", "radius", ")", "symbols", "=", "[", "self", ".", "absorbing_atom", "]", "coords", "=", "[", "[", "0", ",", "0", ",", "0", "]", "]", "for", "i", ",", "site_dist", "in", "enumerate", "(", "sphere", ")", ":", "site_symbol", "=", "re", ".", "sub", "(", "r\"[^aA-zZ]+\"", ",", "\"\"", ",", "site_dist", "[", "0", "]", ".", "species_string", ")", "symbols", ".", "append", "(", "site_symbol", ")", "coords", ".", "append", "(", "site_dist", "[", "0", "]", ".", "coords", "-", "center", ")", "return", "Molecule", "(", "symbols", ",", "coords", ")"], "original_string": "def _set_cluster(self):\n        \"\"\"\n        Compute and set the cluster of atoms as a Molecule object. The siteato\n        coordinates are translated such that the absorbing atom(aka central\n        atom) is at the origin.\n\n        Returns:\n            Molecule\n        \"\"\"\n        center = self.struct[self.center_index].coords\n        sphere = self.struct.get_neighbors(self.struct[self.center_index], self.radius)\n\n        symbols = [self.absorbing_atom]\n        coords = [[0, 0, 0]]\n        for i, site_dist in enumerate(sphere):\n            site_symbol = re.sub(r\"[^aA-zZ]+\", \"\", site_dist[0].species_string)\n            symbols.append(site_symbol)\n            coords.append(site_dist[0].coords - center)\n        return Molecule(symbols, coords)"}, {"code": "def check_state(self, state):\n        \"\"\"\n        Check if the specific function is reached with certain arguments\n\n        :param angr.SimState state: The state to check\n        :return: True if the function is reached with certain arguments, False otherwise.\n        :rtype: bool\n        \"\"\"\n\n        if state.addr == self.function.addr:\n            arch = state.arch\n            if self._check_arguments(arch, state):\n                return True\n\n        return False", "code_tokens": ["def", "check_state", "(", "self", ",", "state", ")", ":", "if", "state", ".", "addr", "==", "self", ".", "function", ".", "addr", ":", "arch", "=", "state", ".", "arch", "if", "self", ".", "_check_arguments", "(", "arch", ",", "state", ")", ":", "return", "True", "return", "False"], "original_string": "def check_state(self, state):\n        \"\"\"\n        Check if the specific function is reached with certain arguments\n\n        :param angr.SimState state: The state to check\n        :return: True if the function is reached with certain arguments, False otherwise.\n        :rtype: bool\n        \"\"\"\n\n        if state.addr == self.function.addr:\n            arch = state.arch\n            if self._check_arguments(arch, state):\n                return True\n\n        return False"}, {"code": "def Kdiag(self, X):\n        \"\"\"I've used the fact that we call this method for K_ff when finding the covariance as a hack so\n        I know if I should return K_ff or K_xx. In this case we're returning K_ff!!\n        $K_{ff}^{post} = K_{ff} - K_{fx} K_{xx}^{-1} K_{xf}$\"\"\"\n        K_ff = np.zeros(X.shape[0])\n        for i,x in enumerate(X):\n            K_ff[i] = self.k_ff(x[0],x[0],self.lengthscale[0])\n        return K_ff * self.variances[0]", "code_tokens": ["def", "Kdiag", "(", "self", ",", "X", ")", ":", "K_ff", "=", "np", ".", "zeros", "(", "X", ".", "shape", "[", "0", "]", ")", "for", "i", ",", "x", "in", "enumerate", "(", "X", ")", ":", "K_ff", "[", "i", "]", "=", "self", ".", "k_ff", "(", "x", "[", "0", "]", ",", "x", "[", "0", "]", ",", "self", ".", "lengthscale", "[", "0", "]", ")", "return", "K_ff", "*", "self", ".", "variances", "[", "0", "]"], "original_string": "def Kdiag(self, X):\n        \"\"\"I've used the fact that we call this method for K_ff when finding the covariance as a hack so\n        I know if I should return K_ff or K_xx. In this case we're returning K_ff!!\n        $K_{ff}^{post} = K_{ff} - K_{fx} K_{xx}^{-1} K_{xf}$\"\"\"\n        K_ff = np.zeros(X.shape[0])\n        for i,x in enumerate(X):\n            K_ff[i] = self.k_ff(x[0],x[0],self.lengthscale[0])\n        return K_ff * self.variances[0]"}, {"code": "def InitFromFlowObject(self, fo):\n    \"\"\"Initialize from rdf_flow_objects.Flow corresponding to a failed flow.\"\"\"\n\n    self.client_id = fo.client_id\n    if fo.HasField(\"backtrace\"):\n      self.backtrace = fo.backtrace\n    self.log_message = fo.error_message\n    self.timestamp = fo.last_update_time\n\n    return self", "code_tokens": ["def", "InitFromFlowObject", "(", "self", ",", "fo", ")", ":", "self", ".", "client_id", "=", "fo", ".", "client_id", "if", "fo", ".", "HasField", "(", "\"backtrace\"", ")", ":", "self", ".", "backtrace", "=", "fo", ".", "backtrace", "self", ".", "log_message", "=", "fo", ".", "error_message", "self", ".", "timestamp", "=", "fo", ".", "last_update_time", "return", "self"], "original_string": "def InitFromFlowObject(self, fo):\n    \"\"\"Initialize from rdf_flow_objects.Flow corresponding to a failed flow.\"\"\"\n\n    self.client_id = fo.client_id\n    if fo.HasField(\"backtrace\"):\n      self.backtrace = fo.backtrace\n    self.log_message = fo.error_message\n    self.timestamp = fo.last_update_time\n\n    return self"}, {"code": "def _mdot_r(a, b):\n    \"\"\"Recursive helper for mdot\"\"\"\n    if type(a) == tuple:\n        if len(a) > 1:\n            a = mdot(*a)\n        else:\n            a = a[0]\n    if type(b) == tuple:\n        if len(b) > 1:\n            b = mdot(*b)\n        else:\n            b = b[0]\n    return np.dot(a, b)", "code_tokens": ["def", "_mdot_r", "(", "a", ",", "b", ")", ":", "if", "type", "(", "a", ")", "==", "tuple", ":", "if", "len", "(", "a", ")", ">", "1", ":", "a", "=", "mdot", "(", "*", "a", ")", "else", ":", "a", "=", "a", "[", "0", "]", "if", "type", "(", "b", ")", "==", "tuple", ":", "if", "len", "(", "b", ")", ">", "1", ":", "b", "=", "mdot", "(", "*", "b", ")", "else", ":", "b", "=", "b", "[", "0", "]", "return", "np", ".", "dot", "(", "a", ",", "b", ")"], "original_string": "def _mdot_r(a, b):\n    \"\"\"Recursive helper for mdot\"\"\"\n    if type(a) == tuple:\n        if len(a) > 1:\n            a = mdot(*a)\n        else:\n            a = a[0]\n    if type(b) == tuple:\n        if len(b) > 1:\n            b = mdot(*b)\n        else:\n            b = b[0]\n    return np.dot(a, b)"}, {"code": "def get(self):\n        \"\"\"\n        Get a JSON-ready representation of this FooterSettings.\n\n        :returns: This FooterSettings, ready for use in a request body.\n        :rtype: dict\n        \"\"\"\n        footer_settings = {}\n        if self.enable is not None:\n            footer_settings[\"enable\"] = self.enable\n\n        if self.text is not None:\n            footer_settings[\"text\"] = self.text.get()\n\n        if self.html is not None:\n            footer_settings[\"html\"] = self.html.get()\n        return footer_settings", "code_tokens": ["def", "get", "(", "self", ")", ":", "footer_settings", "=", "{", "}", "if", "self", ".", "enable", "is", "not", "None", ":", "footer_settings", "[", "\"enable\"", "]", "=", "self", ".", "enable", "if", "self", ".", "text", "is", "not", "None", ":", "footer_settings", "[", "\"text\"", "]", "=", "self", ".", "text", ".", "get", "(", ")", "if", "self", ".", "html", "is", "not", "None", ":", "footer_settings", "[", "\"html\"", "]", "=", "self", ".", "html", ".", "get", "(", ")", "return", "footer_settings"], "original_string": "def get(self):\n        \"\"\"\n        Get a JSON-ready representation of this FooterSettings.\n\n        :returns: This FooterSettings, ready for use in a request body.\n        :rtype: dict\n        \"\"\"\n        footer_settings = {}\n        if self.enable is not None:\n            footer_settings[\"enable\"] = self.enable\n\n        if self.text is not None:\n            footer_settings[\"text\"] = self.text.get()\n\n        if self.html is not None:\n            footer_settings[\"html\"] = self.html.get()\n        return footer_settings"}, {"code": "def _set_windows(self, ticks, bars):\n        \"\"\" be aware of default windows \"\"\"\n        self.tick_window = ticks\n        self.bar_window = bars", "code_tokens": ["def", "_set_windows", "(", "self", ",", "ticks", ",", "bars", ")", ":", "self", ".", "tick_window", "=", "ticks", "self", ".", "bar_window", "=", "bars"], "original_string": "def _set_windows(self, ticks, bars):\n        \"\"\" be aware of default windows \"\"\"\n        self.tick_window = ticks\n        self.bar_window = bars"}, {"code": "def get_messages(self, *args, **kwargs):\n        \"\"\"Return a get_content generator for inbox (messages only).\n\n        The additional parameters are passed directly into\n        :meth:`.get_content`. Note: the `url` parameter cannot be altered.\n\n        \"\"\"\n        return self.get_content(self.config['messages'], *args, **kwargs)", "code_tokens": ["def", "get_messages", "(", "self", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "return", "self", ".", "get_content", "(", "self", ".", "config", "[", "'messages'", "]", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "original_string": "def get_messages(self, *args, **kwargs):\n        \"\"\"Return a get_content generator for inbox (messages only).\n\n        The additional parameters are passed directly into\n        :meth:`.get_content`. Note: the `url` parameter cannot be altered.\n\n        \"\"\"\n        return self.get_content(self.config['messages'], *args, **kwargs)"}, {"code": "def pairs_to_dict(response, decode_keys=False):\n    \"Create a dict given a list of key/value pairs\"\n    if response is None:\n        return {}\n    if decode_keys:\n        # the iter form is faster, but I don't know how to make that work\n        # with a nativestr() map\n        return dict(izip(imap(nativestr, response[::2]), response[1::2]))\n    else:\n        it = iter(response)\n        return dict(izip(it, it))", "code_tokens": ["def", "pairs_to_dict", "(", "response", ",", "decode_keys", "=", "False", ")", ":", "if", "response", "is", "None", ":", "return", "{", "}", "if", "decode_keys", ":", "# the iter form is faster, but I don't know how to make that work", "# with a nativestr() map", "return", "dict", "(", "izip", "(", "imap", "(", "nativestr", ",", "response", "[", ":", ":", "2", "]", ")", ",", "response", "[", "1", ":", ":", "2", "]", ")", ")", "else", ":", "it", "=", "iter", "(", "response", ")", "return", "dict", "(", "izip", "(", "it", ",", "it", ")", ")"], "original_string": "def pairs_to_dict(response, decode_keys=False):\n    \"Create a dict given a list of key/value pairs\"\n    if response is None:\n        return {}\n    if decode_keys:\n        # the iter form is faster, but I don't know how to make that work\n        # with a nativestr() map\n        return dict(izip(imap(nativestr, response[::2]), response[1::2]))\n    else:\n        it = iter(response)\n        return dict(izip(it, it))"}, {"code": "def estimate_row_scales(\n            self,\n            X_centered,\n            column_scales):\n        \"\"\"\n        row_scale[i]**2 =\n        mean{j in observed[i, :]}{\n            (X[i, j] - row_center[i] - column_center[j]) ** 2\n            --------------------------------------------------\n                        column_scale[j] ** 2\n        }\n        \"\"\"\n        n_rows, n_cols = X_centered.shape\n        column_scales = np.asarray(column_scales)\n        if len(column_scales) != n_cols:\n            raise ValueError(\"Expected length %d but got shape %s\" % (\n                n_cols, column_scales))\n        row_variances = np.nanmean(\n            X_centered ** 2 / (column_scales ** 2).reshape((1, n_cols)),\n            axis=1)\n        row_variances[row_variances == 0] = 1.0\n        assert len(row_variances) == n_rows, \"%d != %d\" % (\n            len(row_variances),\n            n_rows)\n        return np.sqrt(row_variances)", "code_tokens": ["def", "estimate_row_scales", "(", "self", ",", "X_centered", ",", "column_scales", ")", ":", "n_rows", ",", "n_cols", "=", "X_centered", ".", "shape", "column_scales", "=", "np", ".", "asarray", "(", "column_scales", ")", "if", "len", "(", "column_scales", ")", "!=", "n_cols", ":", "raise", "ValueError", "(", "\"Expected length %d but got shape %s\"", "%", "(", "n_cols", ",", "column_scales", ")", ")", "row_variances", "=", "np", ".", "nanmean", "(", "X_centered", "**", "2", "/", "(", "column_scales", "**", "2", ")", ".", "reshape", "(", "(", "1", ",", "n_cols", ")", ")", ",", "axis", "=", "1", ")", "row_variances", "[", "row_variances", "==", "0", "]", "=", "1.0", "assert", "len", "(", "row_variances", ")", "==", "n_rows", ",", "\"%d != %d\"", "%", "(", "len", "(", "row_variances", ")", ",", "n_rows", ")", "return", "np", ".", "sqrt", "(", "row_variances", ")"], "original_string": "def estimate_row_scales(\n            self,\n            X_centered,\n            column_scales):\n        \"\"\"\n        row_scale[i]**2 =\n        mean{j in observed[i, :]}{\n            (X[i, j] - row_center[i] - column_center[j]) ** 2\n            --------------------------------------------------\n                        column_scale[j] ** 2\n        }\n        \"\"\"\n        n_rows, n_cols = X_centered.shape\n        column_scales = np.asarray(column_scales)\n        if len(column_scales) != n_cols:\n            raise ValueError(\"Expected length %d but got shape %s\" % (\n                n_cols, column_scales))\n        row_variances = np.nanmean(\n            X_centered ** 2 / (column_scales ** 2).reshape((1, n_cols)),\n            axis=1)\n        row_variances[row_variances == 0] = 1.0\n        assert len(row_variances) == n_rows, \"%d != %d\" % (\n            len(row_variances),\n            n_rows)\n        return np.sqrt(row_variances)"}, {"code": "def toTFExample(image, label):\n  \"\"\"Serializes an image/label as a TFExample byte string\"\"\"\n  example = tf.train.Example(\n    features=tf.train.Features(\n      feature={\n        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=label.astype(\"int64\"))),\n        'image': tf.train.Feature(int64_list=tf.train.Int64List(value=image.astype(\"int64\")))\n      }\n    )\n  )\n  return example.SerializeToString()", "code_tokens": ["def", "toTFExample", "(", "image", ",", "label", ")", ":", "example", "=", "tf", ".", "train", ".", "Example", "(", "features", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "{", "'label'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "label", ".", "astype", "(", "\"int64\"", ")", ")", ")", ",", "'image'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "image", ".", "astype", "(", "\"int64\"", ")", ")", ")", "}", ")", ")", "return", "example", ".", "SerializeToString", "(", ")"], "original_string": "def toTFExample(image, label):\n  \"\"\"Serializes an image/label as a TFExample byte string\"\"\"\n  example = tf.train.Example(\n    features=tf.train.Features(\n      feature={\n        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=label.astype(\"int64\"))),\n        'image': tf.train.Feature(int64_list=tf.train.Int64List(value=image.astype(\"int64\")))\n      }\n    )\n  )\n  return example.SerializeToString()"}, {"code": "def _bind_ldap(self, ldap, con, username, password):\n        \"\"\"\n            Private to bind/Authenticate a user.\n            If AUTH_LDAP_BIND_USER exists then it will bind first with it,\n            next will search the LDAP server using the username with UID\n            and try to bind to it (OpenLDAP).\n            If AUTH_LDAP_BIND_USER does not exit, will bind with username/password\n        \"\"\"\n        try:\n            if self.auth_ldap_bind_user:\n                self._bind_indirect_user(ldap, con)\n                user = self._search_ldap(ldap, con, username)\n                if user:\n                    log.debug(\"LDAP got User {0}\".format(user))\n                    # username = DN from search\n                    username = user[0][0]\n                else:\n                    return False\n            log.debug(\"LDAP bind with: {0} {1}\".format(username, \"XXXXXX\"))\n            if self.auth_ldap_username_format:\n                username = self.auth_ldap_username_format % username\n            if self.auth_ldap_append_domain:\n                username = username + \"@\" + self.auth_ldap_append_domain\n            con.bind_s(username, password)\n            log.debug(\"LDAP bind OK: {0}\".format(username))\n            return True\n        except ldap.INVALID_CREDENTIALS:\n            return False", "code_tokens": ["def", "_bind_ldap", "(", "self", ",", "ldap", ",", "con", ",", "username", ",", "password", ")", ":", "try", ":", "if", "self", ".", "auth_ldap_bind_user", ":", "self", ".", "_bind_indirect_user", "(", "ldap", ",", "con", ")", "user", "=", "self", ".", "_search_ldap", "(", "ldap", ",", "con", ",", "username", ")", "if", "user", ":", "log", ".", "debug", "(", "\"LDAP got User {0}\"", ".", "format", "(", "user", ")", ")", "# username = DN from search", "username", "=", "user", "[", "0", "]", "[", "0", "]", "else", ":", "return", "False", "log", ".", "debug", "(", "\"LDAP bind with: {0} {1}\"", ".", "format", "(", "username", ",", "\"XXXXXX\"", ")", ")", "if", "self", ".", "auth_ldap_username_format", ":", "username", "=", "self", ".", "auth_ldap_username_format", "%", "username", "if", "self", ".", "auth_ldap_append_domain", ":", "username", "=", "username", "+", "\"@\"", "+", "self", ".", "auth_ldap_append_domain", "con", ".", "bind_s", "(", "username", ",", "password", ")", "log", ".", "debug", "(", "\"LDAP bind OK: {0}\"", ".", "format", "(", "username", ")", ")", "return", "True", "except", "ldap", ".", "INVALID_CREDENTIALS", ":", "return", "False"], "original_string": "def _bind_ldap(self, ldap, con, username, password):\n        \"\"\"\n            Private to bind/Authenticate a user.\n            If AUTH_LDAP_BIND_USER exists then it will bind first with it,\n            next will search the LDAP server using the username with UID\n            and try to bind to it (OpenLDAP).\n            If AUTH_LDAP_BIND_USER does not exit, will bind with username/password\n        \"\"\"\n        try:\n            if self.auth_ldap_bind_user:\n                self._bind_indirect_user(ldap, con)\n                user = self._search_ldap(ldap, con, username)\n                if user:\n                    log.debug(\"LDAP got User {0}\".format(user))\n                    # username = DN from search\n                    username = user[0][0]\n                else:\n                    return False\n            log.debug(\"LDAP bind with: {0} {1}\".format(username, \"XXXXXX\"))\n            if self.auth_ldap_username_format:\n                username = self.auth_ldap_username_format % username\n            if self.auth_ldap_append_domain:\n                username = username + \"@\" + self.auth_ldap_append_domain\n            con.bind_s(username, password)\n            log.debug(\"LDAP bind OK: {0}\".format(username))\n            return True\n        except ldap.INVALID_CREDENTIALS:\n            return False"}, {"code": "def plot_gaussian(mean=0., variance=1.,\n                  ax=None,\n                  mean_line=False,\n                  xlim=None,\n                  ylim=None,\n                  xlabel=None,\n                  ylabel=None,\n                  label=None):\n    \"\"\"\n    DEPRECATED. Use plot_gaussian_pdf() instead. This is poorly named, as\n    there are multiple ways to plot a Gaussian.\n    \"\"\"\n\n    warnings.warn('This function is deprecated. It is poorly named. '\\\n                  'A Gaussian can be plotted as a PDF or CDF. This '\\\n                  'plots a PDF. Use plot_gaussian_pdf() instead,',\n                  DeprecationWarning)\n    return plot_gaussian_pdf(mean, variance, ax, mean_line, xlim, ylim, xlabel,\n                             ylabel, label)", "code_tokens": ["def", "plot_gaussian", "(", "mean", "=", "0.", ",", "variance", "=", "1.", ",", "ax", "=", "None", ",", "mean_line", "=", "False", ",", "xlim", "=", "None", ",", "ylim", "=", "None", ",", "xlabel", "=", "None", ",", "ylabel", "=", "None", ",", "label", "=", "None", ")", ":", "warnings", ".", "warn", "(", "'This function is deprecated. It is poorly named. '", "'A Gaussian can be plotted as a PDF or CDF. This '", "'plots a PDF. Use plot_gaussian_pdf() instead,'", ",", "DeprecationWarning", ")", "return", "plot_gaussian_pdf", "(", "mean", ",", "variance", ",", "ax", ",", "mean_line", ",", "xlim", ",", "ylim", ",", "xlabel", ",", "ylabel", ",", "label", ")"], "original_string": "def plot_gaussian(mean=0., variance=1.,\n                  ax=None,\n                  mean_line=False,\n                  xlim=None,\n                  ylim=None,\n                  xlabel=None,\n                  ylabel=None,\n                  label=None):\n    \"\"\"\n    DEPRECATED. Use plot_gaussian_pdf() instead. This is poorly named, as\n    there are multiple ways to plot a Gaussian.\n    \"\"\"\n\n    warnings.warn('This function is deprecated. It is poorly named. '\\\n                  'A Gaussian can be plotted as a PDF or CDF. This '\\\n                  'plots a PDF. Use plot_gaussian_pdf() instead,',\n                  DeprecationWarning)\n    return plot_gaussian_pdf(mean, variance, ax, mean_line, xlim, ylim, xlabel,\n                             ylabel, label)"}, {"code": "def expectation(self, prep_prog, operator_programs=None):\n        \"\"\"\n        Calculate the expectation value of operators given a state prepared by\n        prep_program.\n\n        :note: If the execution of ``quil_program`` is **non-deterministic**, i.e., if it includes\n            measurements and/or noisy quantum gates, then the final wavefunction from which the\n            expectation values are computed itself only represents a stochastically generated\n            sample. The expectations returned from *different* ``expectation`` calls *will then\n            generally be different*.\n\n        To measure the expectation of a PauliSum, you probably want to\n        do something like this::\n\n                progs, coefs = hamiltonian.get_programs()\n                expect_coeffs = np.array(cxn.expectation(prep_program, operator_programs=progs))\n                return np.real_if_close(np.dot(coefs, expect_coeffs))\n\n        :param Program prep_prog: Quil program for state preparation.\n        :param list operator_programs: A list of Programs, each specifying an operator whose expectation to compute.\n            Default is a list containing only the empty Program.\n        :return: Expectation values of the operators.\n        :rtype: List[float]\n        \"\"\"\n        # Developer note: This code is for backwards compatibility. It can't be replaced with\n        # ForestConnection._expectation because we've turned off the ability to set\n        # `needs_compilation` (that usually indicates the user is doing something iffy like\n        # using a noise model with this function)\n\n        if isinstance(operator_programs, Program):\n            warnings.warn(\n                \"You have provided a Program rather than a list of Programs. The results from expectation \"\n                \"will be line-wise expectation values of the operator_programs.\", SyntaxWarning)\n\n        payload = self._expectation_payload(prep_prog, operator_programs)\n        response = post_json(self.session, self.sync_endpoint + \"/qvm\", payload)\n        return response.json()", "code_tokens": ["def", "expectation", "(", "self", ",", "prep_prog", ",", "operator_programs", "=", "None", ")", ":", "# Developer note: This code is for backwards compatibility. It can't be replaced with", "# ForestConnection._expectation because we've turned off the ability to set", "# `needs_compilation` (that usually indicates the user is doing something iffy like", "# using a noise model with this function)", "if", "isinstance", "(", "operator_programs", ",", "Program", ")", ":", "warnings", ".", "warn", "(", "\"You have provided a Program rather than a list of Programs. The results from expectation \"", "\"will be line-wise expectation values of the operator_programs.\"", ",", "SyntaxWarning", ")", "payload", "=", "self", ".", "_expectation_payload", "(", "prep_prog", ",", "operator_programs", ")", "response", "=", "post_json", "(", "self", ".", "session", ",", "self", ".", "sync_endpoint", "+", "\"/qvm\"", ",", "payload", ")", "return", "response", ".", "json", "(", ")"], "original_string": "def expectation(self, prep_prog, operator_programs=None):\n        \"\"\"\n        Calculate the expectation value of operators given a state prepared by\n        prep_program.\n\n        :note: If the execution of ``quil_program`` is **non-deterministic**, i.e., if it includes\n            measurements and/or noisy quantum gates, then the final wavefunction from which the\n            expectation values are computed itself only represents a stochastically generated\n            sample. The expectations returned from *different* ``expectation`` calls *will then\n            generally be different*.\n\n        To measure the expectation of a PauliSum, you probably want to\n        do something like this::\n\n                progs, coefs = hamiltonian.get_programs()\n                expect_coeffs = np.array(cxn.expectation(prep_program, operator_programs=progs))\n                return np.real_if_close(np.dot(coefs, expect_coeffs))\n\n        :param Program prep_prog: Quil program for state preparation.\n        :param list operator_programs: A list of Programs, each specifying an operator whose expectation to compute.\n            Default is a list containing only the empty Program.\n        :return: Expectation values of the operators.\n        :rtype: List[float]\n        \"\"\"\n        # Developer note: This code is for backwards compatibility. It can't be replaced with\n        # ForestConnection._expectation because we've turned off the ability to set\n        # `needs_compilation` (that usually indicates the user is doing something iffy like\n        # using a noise model with this function)\n\n        if isinstance(operator_programs, Program):\n            warnings.warn(\n                \"You have provided a Program rather than a list of Programs. The results from expectation \"\n                \"will be line-wise expectation values of the operator_programs.\", SyntaxWarning)\n\n        payload = self._expectation_payload(prep_prog, operator_programs)\n        response = post_json(self.session, self.sync_endpoint + \"/qvm\", payload)\n        return response.json()"}, {"code": "def from_node(cls, work):\n        \"\"\"Initialize an instance from a :class:`Work` instance.\"\"\"\n        new = super().from_node(work)\n\n        # Will put all files found in outdir in GridFs\n        # Warning: assuming binary files.\n        d = {os.path.basename(f): f for f in work.outdir.list_filepaths()}\n        new.register_gridfs_files(**d)\n\n        return new", "code_tokens": ["def", "from_node", "(", "cls", ",", "work", ")", ":", "new", "=", "super", "(", ")", ".", "from_node", "(", "work", ")", "# Will put all files found in outdir in GridFs", "# Warning: assuming binary files.", "d", "=", "{", "os", ".", "path", ".", "basename", "(", "f", ")", ":", "f", "for", "f", "in", "work", ".", "outdir", ".", "list_filepaths", "(", ")", "}", "new", ".", "register_gridfs_files", "(", "*", "*", "d", ")", "return", "new"], "original_string": "def from_node(cls, work):\n        \"\"\"Initialize an instance from a :class:`Work` instance.\"\"\"\n        new = super().from_node(work)\n\n        # Will put all files found in outdir in GridFs\n        # Warning: assuming binary files.\n        d = {os.path.basename(f): f for f in work.outdir.list_filepaths()}\n        new.register_gridfs_files(**d)\n\n        return new"}, {"code": "def explain_weights(self, **kwargs):\n        \"\"\"\n        Call :func:`eli5.show_weights` for the locally-fit\n        classification pipeline. Keyword arguments are passed\n        to :func:`eli5.show_weights`.\n\n        :func:`fit` must be called before using this method.\n        \"\"\"\n        self._fix_target_names(kwargs)\n        return eli5.explain_weights(self.clf_, vec=self.vec_, **kwargs)", "code_tokens": ["def", "explain_weights", "(", "self", ",", "*", "*", "kwargs", ")", ":", "self", ".", "_fix_target_names", "(", "kwargs", ")", "return", "eli5", ".", "explain_weights", "(", "self", ".", "clf_", ",", "vec", "=", "self", ".", "vec_", ",", "*", "*", "kwargs", ")"], "original_string": "def explain_weights(self, **kwargs):\n        \"\"\"\n        Call :func:`eli5.show_weights` for the locally-fit\n        classification pipeline. Keyword arguments are passed\n        to :func:`eli5.show_weights`.\n\n        :func:`fit` must be called before using this method.\n        \"\"\"\n        self._fix_target_names(kwargs)\n        return eli5.explain_weights(self.clf_, vec=self.vec_, **kwargs)"}, {"code": "def validation_curve(model, X, y, param_name, param_range, ax=None, logx=False,\n                     groups=None, cv=None, scoring=None, n_jobs=1,\n                     pre_dispatch=\"all\", **kwargs):\n    \"\"\"\n    Displays a validation curve for the specified param and values, plotting\n    both the train and cross-validated test scores. The validation curve is a\n    visual, single-parameter grid search used to tune a model to find the best\n    balance between error due to bias and error due to variance.\n\n    This helper function is a wrapper to use the ValidationCurve in a fast,\n    visual analysis.\n\n    Parameters\n    ----------\n    model : a scikit-learn estimator\n        An object that implements ``fit`` and ``predict``, can be a\n        classifier, regressor, or clusterer so long as there is also a valid\n        associated scoring metric.\n\n        Note that the object is cloned for each validation.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    param_name : string\n        Name of the parameter that will be varied.\n\n    param_range : array-like, shape (n_values,)\n        The values of the parameter that will be evaluated.\n\n    ax : matplotlib.Axes object, optional\n        The axes object to plot the figure on.\n\n    logx : boolean, optional\n        If True, plots the x-axis with a logarithmic scale.\n\n    groups : array-like, with shape (n_samples,)\n        Optional group labels for the samples used while splitting the dataset\n        into train/test sets.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train/test splits.\n\n        see the scikit-learn\n        `cross-validation guide <http://scikit-learn.org/stable/modules/cross_validation.html>`_\n        for more information on the possible strategies that can be used here.\n\n    scoring : string, callable or None, optional, default: None\n        A string or scorer callable object / function with signature\n        ``scorer(estimator, X, y)``. See scikit-learn model evaluation\n        documentation for names of possible metrics.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n\n    pre_dispatch : integer or string, optional\n        Number of predispatched jobs for parallel execution (default is\n        all). The option can reduce the allocated memory. The string can\n        be an expression like '2*n_jobs'.\n\n    kwargs : dict\n        Keyword arguments that are passed to the base class and may influence\n        the visualization as defined in other Visualizers. These arguments are\n        also passed to the `poof()` method, e.g. can pass a path to save the\n        figure to.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes object that the validation curves were drawn on.\n    \"\"\"\n\n    # Initialize the visualizer\n    oz = ValidationCurve(\n        model, param_name, param_range, ax=ax, logx=logx, groups=groups,\n        cv=cv, scoring=scoring, n_jobs=n_jobs, pre_dispatch=pre_dispatch\n    )\n\n    # Fit and poof the visualizer\n    oz.fit(X, y)\n    oz.poof(**kwargs)\n    return oz.ax", "code_tokens": ["def", "validation_curve", "(", "model", ",", "X", ",", "y", ",", "param_name", ",", "param_range", ",", "ax", "=", "None", ",", "logx", "=", "False", ",", "groups", "=", "None", ",", "cv", "=", "None", ",", "scoring", "=", "None", ",", "n_jobs", "=", "1", ",", "pre_dispatch", "=", "\"all\"", ",", "*", "*", "kwargs", ")", ":", "# Initialize the visualizer", "oz", "=", "ValidationCurve", "(", "model", ",", "param_name", ",", "param_range", ",", "ax", "=", "ax", ",", "logx", "=", "logx", ",", "groups", "=", "groups", ",", "cv", "=", "cv", ",", "scoring", "=", "scoring", ",", "n_jobs", "=", "n_jobs", ",", "pre_dispatch", "=", "pre_dispatch", ")", "# Fit and poof the visualizer", "oz", ".", "fit", "(", "X", ",", "y", ")", "oz", ".", "poof", "(", "*", "*", "kwargs", ")", "return", "oz", ".", "ax"], "original_string": "def validation_curve(model, X, y, param_name, param_range, ax=None, logx=False,\n                     groups=None, cv=None, scoring=None, n_jobs=1,\n                     pre_dispatch=\"all\", **kwargs):\n    \"\"\"\n    Displays a validation curve for the specified param and values, plotting\n    both the train and cross-validated test scores. The validation curve is a\n    visual, single-parameter grid search used to tune a model to find the best\n    balance between error due to bias and error due to variance.\n\n    This helper function is a wrapper to use the ValidationCurve in a fast,\n    visual analysis.\n\n    Parameters\n    ----------\n    model : a scikit-learn estimator\n        An object that implements ``fit`` and ``predict``, can be a\n        classifier, regressor, or clusterer so long as there is also a valid\n        associated scoring metric.\n\n        Note that the object is cloned for each validation.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    param_name : string\n        Name of the parameter that will be varied.\n\n    param_range : array-like, shape (n_values,)\n        The values of the parameter that will be evaluated.\n\n    ax : matplotlib.Axes object, optional\n        The axes object to plot the figure on.\n\n    logx : boolean, optional\n        If True, plots the x-axis with a logarithmic scale.\n\n    groups : array-like, with shape (n_samples,)\n        Optional group labels for the samples used while splitting the dataset\n        into train/test sets.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train/test splits.\n\n        see the scikit-learn\n        `cross-validation guide <http://scikit-learn.org/stable/modules/cross_validation.html>`_\n        for more information on the possible strategies that can be used here.\n\n    scoring : string, callable or None, optional, default: None\n        A string or scorer callable object / function with signature\n        ``scorer(estimator, X, y)``. See scikit-learn model evaluation\n        documentation for names of possible metrics.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n\n    pre_dispatch : integer or string, optional\n        Number of predispatched jobs for parallel execution (default is\n        all). The option can reduce the allocated memory. The string can\n        be an expression like '2*n_jobs'.\n\n    kwargs : dict\n        Keyword arguments that are passed to the base class and may influence\n        the visualization as defined in other Visualizers. These arguments are\n        also passed to the `poof()` method, e.g. can pass a path to save the\n        figure to.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes object that the validation curves were drawn on.\n    \"\"\"\n\n    # Initialize the visualizer\n    oz = ValidationCurve(\n        model, param_name, param_range, ax=ax, logx=logx, groups=groups,\n        cv=cv, scoring=scoring, n_jobs=n_jobs, pre_dispatch=pre_dispatch\n    )\n\n    # Fit and poof the visualizer\n    oz.fit(X, y)\n    oz.poof(**kwargs)\n    return oz.ax"}, {"code": "def _retry_on_connection_error(func: Callable) -> Callable:\n    \"\"\"Decorator to retry the function max_connection_attemps number of times.\n\n    Herewith-decorated functions need an ``_attempt`` keyword argument.\n\n    This is to decorate functions that do network requests that may fail. Note that\n    :meth:`.get_json`, :meth:`.get_iphone_json`, :meth:`.graphql_query` and :meth:`.graphql_node_list` already have\n    their own logic for retrying, hence functions that only use these for network access must not be decorated with this\n    decorator.\"\"\"\n    @wraps(func)\n    def call(instaloader, *args, **kwargs):\n        try:\n            return func(instaloader, *args, **kwargs)\n        except (urllib3.exceptions.HTTPError, requests.exceptions.RequestException, ConnectionException) as err:\n            error_string = \"{}({}): {}\".format(func.__name__, ', '.join([repr(arg) for arg in args]), err)\n            if (kwargs.get('_attempt') or 1) == instaloader.context.max_connection_attempts:\n                raise ConnectionException(error_string) from None\n            instaloader.context.error(error_string + \" [retrying; skip with ^C]\", repeat_at_end=False)\n            try:\n                if kwargs.get('_attempt'):\n                    kwargs['_attempt'] += 1\n                else:\n                    kwargs['_attempt'] = 2\n                instaloader.context.do_sleep()\n                return call(instaloader, *args, **kwargs)\n            except KeyboardInterrupt:\n                instaloader.context.error(\"[skipped by user]\", repeat_at_end=False)\n                raise ConnectionException(error_string) from None\n    return call", "code_tokens": ["def", "_retry_on_connection_error", "(", "func", ":", "Callable", ")", "->", "Callable", ":", "@", "wraps", "(", "func", ")", "def", "call", "(", "instaloader", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "try", ":", "return", "func", "(", "instaloader", ",", "*", "args", ",", "*", "*", "kwargs", ")", "except", "(", "urllib3", ".", "exceptions", ".", "HTTPError", ",", "requests", ".", "exceptions", ".", "RequestException", ",", "ConnectionException", ")", "as", "err", ":", "error_string", "=", "\"{}({}): {}\"", ".", "format", "(", "func", ".", "__name__", ",", "', '", ".", "join", "(", "[", "repr", "(", "arg", ")", "for", "arg", "in", "args", "]", ")", ",", "err", ")", "if", "(", "kwargs", ".", "get", "(", "'_attempt'", ")", "or", "1", ")", "==", "instaloader", ".", "context", ".", "max_connection_attempts", ":", "raise", "ConnectionException", "(", "error_string", ")", "from", "None", "instaloader", ".", "context", ".", "error", "(", "error_string", "+", "\" [retrying; skip with ^C]\"", ",", "repeat_at_end", "=", "False", ")", "try", ":", "if", "kwargs", ".", "get", "(", "'_attempt'", ")", ":", "kwargs", "[", "'_attempt'", "]", "+=", "1", "else", ":", "kwargs", "[", "'_attempt'", "]", "=", "2", "instaloader", ".", "context", ".", "do_sleep", "(", ")", "return", "call", "(", "instaloader", ",", "*", "args", ",", "*", "*", "kwargs", ")", "except", "KeyboardInterrupt", ":", "instaloader", ".", "context", ".", "error", "(", "\"[skipped by user]\"", ",", "repeat_at_end", "=", "False", ")", "raise", "ConnectionException", "(", "error_string", ")", "from", "None", "return", "call"], "original_string": "def _retry_on_connection_error(func: Callable) -> Callable:\n    \"\"\"Decorator to retry the function max_connection_attemps number of times.\n\n    Herewith-decorated functions need an ``_attempt`` keyword argument.\n\n    This is to decorate functions that do network requests that may fail. Note that\n    :meth:`.get_json`, :meth:`.get_iphone_json`, :meth:`.graphql_query` and :meth:`.graphql_node_list` already have\n    their own logic for retrying, hence functions that only use these for network access must not be decorated with this\n    decorator.\"\"\"\n    @wraps(func)\n    def call(instaloader, *args, **kwargs):\n        try:\n            return func(instaloader, *args, **kwargs)\n        except (urllib3.exceptions.HTTPError, requests.exceptions.RequestException, ConnectionException) as err:\n            error_string = \"{}({}): {}\".format(func.__name__, ', '.join([repr(arg) for arg in args]), err)\n            if (kwargs.get('_attempt') or 1) == instaloader.context.max_connection_attempts:\n                raise ConnectionException(error_string) from None\n            instaloader.context.error(error_string + \" [retrying; skip with ^C]\", repeat_at_end=False)\n            try:\n                if kwargs.get('_attempt'):\n                    kwargs['_attempt'] += 1\n                else:\n                    kwargs['_attempt'] = 2\n                instaloader.context.do_sleep()\n                return call(instaloader, *args, **kwargs)\n            except KeyboardInterrupt:\n                instaloader.context.error(\"[skipped by user]\", repeat_at_end=False)\n                raise ConnectionException(error_string) from None\n    return call"}, {"code": "def addfield(self, pkt, s, val):\n        \"\"\"\n        Reconstruct the header because the TLS type may have been updated.\n        Then, append the content.\n        \"\"\"\n        res = b\"\"\n        for p in val:\n            res += self.i2m(pkt, p)\n        if (isinstance(pkt, _GenericTLSSessionInheritance) and\n            _tls_version_check(pkt.tls_session.tls_version, 0x0304) and\n                not isinstance(pkt, TLS13ServerHello)):\n            return s + res\n        if not pkt.type:\n            pkt.type = 0\n        hdr = struct.pack(\"!B\", pkt.type) + s[1:5]\n        return hdr + res", "code_tokens": ["def", "addfield", "(", "self", ",", "pkt", ",", "s", ",", "val", ")", ":", "res", "=", "b\"\"", "for", "p", "in", "val", ":", "res", "+=", "self", ".", "i2m", "(", "pkt", ",", "p", ")", "if", "(", "isinstance", "(", "pkt", ",", "_GenericTLSSessionInheritance", ")", "and", "_tls_version_check", "(", "pkt", ".", "tls_session", ".", "tls_version", ",", "0x0304", ")", "and", "not", "isinstance", "(", "pkt", ",", "TLS13ServerHello", ")", ")", ":", "return", "s", "+", "res", "if", "not", "pkt", ".", "type", ":", "pkt", ".", "type", "=", "0", "hdr", "=", "struct", ".", "pack", "(", "\"!B\"", ",", "pkt", ".", "type", ")", "+", "s", "[", "1", ":", "5", "]", "return", "hdr", "+", "res"], "original_string": "def addfield(self, pkt, s, val):\n        \"\"\"\n        Reconstruct the header because the TLS type may have been updated.\n        Then, append the content.\n        \"\"\"\n        res = b\"\"\n        for p in val:\n            res += self.i2m(pkt, p)\n        if (isinstance(pkt, _GenericTLSSessionInheritance) and\n            _tls_version_check(pkt.tls_session.tls_version, 0x0304) and\n                not isinstance(pkt, TLS13ServerHello)):\n            return s + res\n        if not pkt.type:\n            pkt.type = 0\n        hdr = struct.pack(\"!B\", pkt.type) + s[1:5]\n        return hdr + res"}, {"code": "def add_subscriber(self, connection_id, subscriptions,\n                       last_known_block_id):\n        \"\"\"Register the subscriber for the given event subscriptions.\n\n        Raises:\n            InvalidFilterError\n                One of the filters in the subscriptions is invalid.\n        \"\"\"\n        with self._subscribers_cv:\n            self._subscribers[connection_id] = \\\n                EventSubscriber(\n                    connection_id, subscriptions, last_known_block_id)\n\n        LOGGER.debug(\n            'Added Subscriber %s for %s', connection_id, subscriptions)", "code_tokens": ["def", "add_subscriber", "(", "self", ",", "connection_id", ",", "subscriptions", ",", "last_known_block_id", ")", ":", "with", "self", ".", "_subscribers_cv", ":", "self", ".", "_subscribers", "[", "connection_id", "]", "=", "EventSubscriber", "(", "connection_id", ",", "subscriptions", ",", "last_known_block_id", ")", "LOGGER", ".", "debug", "(", "'Added Subscriber %s for %s'", ",", "connection_id", ",", "subscriptions", ")"], "original_string": "def add_subscriber(self, connection_id, subscriptions,\n                       last_known_block_id):\n        \"\"\"Register the subscriber for the given event subscriptions.\n\n        Raises:\n            InvalidFilterError\n                One of the filters in the subscriptions is invalid.\n        \"\"\"\n        with self._subscribers_cv:\n            self._subscribers[connection_id] = \\\n                EventSubscriber(\n                    connection_id, subscriptions, last_known_block_id)\n\n        LOGGER.debug(\n            'Added Subscriber %s for %s', connection_id, subscriptions)"}, {"code": "def get_commands_from_file(self, mission_file, role):\n        \"\"\"Get commands from xml file as a list of (command_type:int, turnbased:boolean, command:string)\"\"\"\n        doc = etree.parse(mission_file)\n        mission = doc.getroot()\n        return self.get_commands_from_xml(mission, role)", "code_tokens": ["def", "get_commands_from_file", "(", "self", ",", "mission_file", ",", "role", ")", ":", "doc", "=", "etree", ".", "parse", "(", "mission_file", ")", "mission", "=", "doc", ".", "getroot", "(", ")", "return", "self", ".", "get_commands_from_xml", "(", "mission", ",", "role", ")"], "original_string": "def get_commands_from_file(self, mission_file, role):\n        \"\"\"Get commands from xml file as a list of (command_type:int, turnbased:boolean, command:string)\"\"\"\n        doc = etree.parse(mission_file)\n        mission = doc.getroot()\n        return self.get_commands_from_xml(mission, role)"}, {"code": "def get_summed_cohp_by_label_list(self, label_list, divisor=1):\n        \"\"\"\n        Returns a COHP object that includes a summed COHP divided by divisor\n\n        Args:\n            label_list: list of labels for the COHP that should be included in the summed cohp\n            divisor: float/int, the summed cohp will be divided by this divisor\n        Returns:\n            Returns a COHP object including a summed COHP\n        \"\"\"\n        # check if cohps are spinpolarized or not\n        first_cohpobject = self.get_cohp_by_label(label_list[0])\n        summed_cohp = first_cohpobject.cohp.copy()\n        summed_icohp = first_cohpobject.icohp.copy()\n        for label in label_list[1:]:\n            cohp_here = self.get_cohp_by_label(label)\n            summed_cohp[Spin.up] = np.sum([summed_cohp[Spin.up], cohp_here.cohp[Spin.up]], axis=0)\n            if Spin.down in summed_cohp:\n                summed_cohp[Spin.down] = np.sum([summed_cohp[Spin.down], cohp_here.cohp[Spin.down]], axis=0)\n            summed_icohp[Spin.up] = np.sum([summed_icohp[Spin.up], cohp_here.icohp[Spin.up]], axis=0)\n            if Spin.down in summed_icohp:\n                summed_icohp[Spin.down] = np.sum([summed_icohp[Spin.down], cohp_here.icohp[Spin.down]], axis=0)\n\n        divided_cohp = {}\n        divided_icohp = {}\n        divided_cohp[Spin.up] = np.divide(summed_cohp[Spin.up], divisor)\n        divided_icohp[Spin.up] = np.divide(summed_icohp[Spin.up], divisor)\n        if Spin.down in summed_cohp:\n            divided_cohp[Spin.down] = np.divide(summed_cohp[Spin.down], divisor)\n            divided_icohp[Spin.down] = np.divide(summed_icohp[Spin.down], divisor)\n\n        return Cohp(efermi=first_cohpobject.efermi, energies=first_cohpobject.energies, cohp=divided_cohp,\n                    are_coops=first_cohpobject.are_coops,\n                    icohp=divided_icohp)", "code_tokens": ["def", "get_summed_cohp_by_label_list", "(", "self", ",", "label_list", ",", "divisor", "=", "1", ")", ":", "# check if cohps are spinpolarized or not", "first_cohpobject", "=", "self", ".", "get_cohp_by_label", "(", "label_list", "[", "0", "]", ")", "summed_cohp", "=", "first_cohpobject", ".", "cohp", ".", "copy", "(", ")", "summed_icohp", "=", "first_cohpobject", ".", "icohp", ".", "copy", "(", ")", "for", "label", "in", "label_list", "[", "1", ":", "]", ":", "cohp_here", "=", "self", ".", "get_cohp_by_label", "(", "label", ")", "summed_cohp", "[", "Spin", ".", "up", "]", "=", "np", ".", "sum", "(", "[", "summed_cohp", "[", "Spin", ".", "up", "]", ",", "cohp_here", ".", "cohp", "[", "Spin", ".", "up", "]", "]", ",", "axis", "=", "0", ")", "if", "Spin", ".", "down", "in", "summed_cohp", ":", "summed_cohp", "[", "Spin", ".", "down", "]", "=", "np", ".", "sum", "(", "[", "summed_cohp", "[", "Spin", ".", "down", "]", ",", "cohp_here", ".", "cohp", "[", "Spin", ".", "down", "]", "]", ",", "axis", "=", "0", ")", "summed_icohp", "[", "Spin", ".", "up", "]", "=", "np", ".", "sum", "(", "[", "summed_icohp", "[", "Spin", ".", "up", "]", ",", "cohp_here", ".", "icohp", "[", "Spin", ".", "up", "]", "]", ",", "axis", "=", "0", ")", "if", "Spin", ".", "down", "in", "summed_icohp", ":", "summed_icohp", "[", "Spin", ".", "down", "]", "=", "np", ".", "sum", "(", "[", "summed_icohp", "[", "Spin", ".", "down", "]", ",", "cohp_here", ".", "icohp", "[", "Spin", ".", "down", "]", "]", ",", "axis", "=", "0", ")", "divided_cohp", "=", "{", "}", "divided_icohp", "=", "{", "}", "divided_cohp", "[", "Spin", ".", "up", "]", "=", "np", ".", "divide", "(", "summed_cohp", "[", "Spin", ".", "up", "]", ",", "divisor", ")", "divided_icohp", "[", "Spin", ".", "up", "]", "=", "np", ".", "divide", "(", "summed_icohp", "[", "Spin", ".", "up", "]", ",", "divisor", ")", "if", "Spin", ".", "down", "in", "summed_cohp", ":", "divided_cohp", "[", "Spin", ".", "down", "]", "=", "np", ".", "divide", "(", "summed_cohp", "[", "Spin", ".", "down", "]", ",", "divisor", ")", "divided_icohp", "[", "Spin", ".", "down", "]", "=", "np", ".", "divide", "(", "summed_icohp", "[", "Spin", ".", "down", "]", ",", "divisor", ")", "return", "Cohp", "(", "efermi", "=", "first_cohpobject", ".", "efermi", ",", "energies", "=", "first_cohpobject", ".", "energies", ",", "cohp", "=", "divided_cohp", ",", "are_coops", "=", "first_cohpobject", ".", "are_coops", ",", "icohp", "=", "divided_icohp", ")"], "original_string": "def get_summed_cohp_by_label_list(self, label_list, divisor=1):\n        \"\"\"\n        Returns a COHP object that includes a summed COHP divided by divisor\n\n        Args:\n            label_list: list of labels for the COHP that should be included in the summed cohp\n            divisor: float/int, the summed cohp will be divided by this divisor\n        Returns:\n            Returns a COHP object including a summed COHP\n        \"\"\"\n        # check if cohps are spinpolarized or not\n        first_cohpobject = self.get_cohp_by_label(label_list[0])\n        summed_cohp = first_cohpobject.cohp.copy()\n        summed_icohp = first_cohpobject.icohp.copy()\n        for label in label_list[1:]:\n            cohp_here = self.get_cohp_by_label(label)\n            summed_cohp[Spin.up] = np.sum([summed_cohp[Spin.up], cohp_here.cohp[Spin.up]], axis=0)\n            if Spin.down in summed_cohp:\n                summed_cohp[Spin.down] = np.sum([summed_cohp[Spin.down], cohp_here.cohp[Spin.down]], axis=0)\n            summed_icohp[Spin.up] = np.sum([summed_icohp[Spin.up], cohp_here.icohp[Spin.up]], axis=0)\n            if Spin.down in summed_icohp:\n                summed_icohp[Spin.down] = np.sum([summed_icohp[Spin.down], cohp_here.icohp[Spin.down]], axis=0)\n\n        divided_cohp = {}\n        divided_icohp = {}\n        divided_cohp[Spin.up] = np.divide(summed_cohp[Spin.up], divisor)\n        divided_icohp[Spin.up] = np.divide(summed_icohp[Spin.up], divisor)\n        if Spin.down in summed_cohp:\n            divided_cohp[Spin.down] = np.divide(summed_cohp[Spin.down], divisor)\n            divided_icohp[Spin.down] = np.divide(summed_icohp[Spin.down], divisor)\n\n        return Cohp(efermi=first_cohpobject.efermi, energies=first_cohpobject.energies, cohp=divided_cohp,\n                    are_coops=first_cohpobject.are_coops,\n                    icohp=divided_icohp)"}, {"code": "def coerce_pandas_values(objects):\n    \"\"\"Convert pandas values found in a list of labeled objects.\n\n    Parameters\n    ----------\n    objects : list of Dataset or mappings\n        The mappings may contain any sort of objects coercible to\n        xarray.Variables as keys, including pandas objects.\n\n    Returns\n    -------\n    List of Dataset or OrderedDict objects. Any inputs or values in the inputs\n    that were pandas objects have been converted into native xarray objects.\n    \"\"\"\n    from .dataset import Dataset\n    from .dataarray import DataArray\n\n    out = []\n    for obj in objects:\n        if isinstance(obj, Dataset):\n            variables = obj\n        else:\n            variables = OrderedDict()\n            if isinstance(obj, PANDAS_TYPES):\n                obj = OrderedDict(obj.iteritems())\n            for k, v in obj.items():\n                if isinstance(v, PANDAS_TYPES):\n                    v = DataArray(v)\n                variables[k] = v\n        out.append(variables)\n    return out", "code_tokens": ["def", "coerce_pandas_values", "(", "objects", ")", ":", "from", ".", "dataset", "import", "Dataset", "from", ".", "dataarray", "import", "DataArray", "out", "=", "[", "]", "for", "obj", "in", "objects", ":", "if", "isinstance", "(", "obj", ",", "Dataset", ")", ":", "variables", "=", "obj", "else", ":", "variables", "=", "OrderedDict", "(", ")", "if", "isinstance", "(", "obj", ",", "PANDAS_TYPES", ")", ":", "obj", "=", "OrderedDict", "(", "obj", ".", "iteritems", "(", ")", ")", "for", "k", ",", "v", "in", "obj", ".", "items", "(", ")", ":", "if", "isinstance", "(", "v", ",", "PANDAS_TYPES", ")", ":", "v", "=", "DataArray", "(", "v", ")", "variables", "[", "k", "]", "=", "v", "out", ".", "append", "(", "variables", ")", "return", "out"], "original_string": "def coerce_pandas_values(objects):\n    \"\"\"Convert pandas values found in a list of labeled objects.\n\n    Parameters\n    ----------\n    objects : list of Dataset or mappings\n        The mappings may contain any sort of objects coercible to\n        xarray.Variables as keys, including pandas objects.\n\n    Returns\n    -------\n    List of Dataset or OrderedDict objects. Any inputs or values in the inputs\n    that were pandas objects have been converted into native xarray objects.\n    \"\"\"\n    from .dataset import Dataset\n    from .dataarray import DataArray\n\n    out = []\n    for obj in objects:\n        if isinstance(obj, Dataset):\n            variables = obj\n        else:\n            variables = OrderedDict()\n            if isinstance(obj, PANDAS_TYPES):\n                obj = OrderedDict(obj.iteritems())\n            for k, v in obj.items():\n                if isinstance(v, PANDAS_TYPES):\n                    v = DataArray(v)\n                variables[k] = v\n        out.append(variables)\n    return out"}, {"code": "def _replay_index(replay_dir):\n  \"\"\"Output information for a directory of replays.\"\"\"\n  run_config = run_configs.get()\n  replay_dir = run_config.abs_replay_path(replay_dir)\n  print(\"Checking: \", replay_dir)\n\n  with run_config.start(want_rgb=False) as controller:\n    print(\"-\" * 60)\n    print(\",\".join((\n        \"filename\",\n        \"build\",\n        \"map_name\",\n        \"game_duration_loops\",\n        \"players\",\n        \"P1-outcome\",\n        \"P1-race\",\n        \"P1-apm\",\n        \"P2-race\",\n        \"P2-apm\",\n    )))\n\n    try:\n      bad_replays = []\n      for file_path in run_config.replay_paths(replay_dir):\n        file_name = os.path.basename(file_path)\n        try:\n          info = controller.replay_info(run_config.replay_data(file_path))\n        except remote_controller.RequestError as e:\n          bad_replays.append(\"%s: %s\" % (file_name, e))\n          continue\n        if info.HasField(\"error\"):\n          print(\"failed:\", file_name, info.error, info.error_details)\n          bad_replays.append(file_name)\n        else:\n          out = [\n              file_name,\n              info.base_build,\n              info.map_name,\n              info.game_duration_loops,\n              len(info.player_info),\n              sc_pb.Result.Name(info.player_info[0].player_result.result),\n              sc_common.Race.Name(info.player_info[0].player_info.race_actual),\n              info.player_info[0].player_apm,\n          ]\n          if len(info.player_info) >= 2:\n            out += [\n                sc_common.Race.Name(\n                    info.player_info[1].player_info.race_actual),\n                info.player_info[1].player_apm,\n            ]\n          print(u\",\".join(str(s) for s in out))\n    except KeyboardInterrupt:\n      pass\n    finally:\n      if bad_replays:\n        print(\"\\n\")\n        print(\"Replays with errors:\")\n        print(\"\\n\".join(bad_replays))", "code_tokens": ["def", "_replay_index", "(", "replay_dir", ")", ":", "run_config", "=", "run_configs", ".", "get", "(", ")", "replay_dir", "=", "run_config", ".", "abs_replay_path", "(", "replay_dir", ")", "print", "(", "\"Checking: \"", ",", "replay_dir", ")", "with", "run_config", ".", "start", "(", "want_rgb", "=", "False", ")", "as", "controller", ":", "print", "(", "\"-\"", "*", "60", ")", "print", "(", "\",\"", ".", "join", "(", "(", "\"filename\"", ",", "\"build\"", ",", "\"map_name\"", ",", "\"game_duration_loops\"", ",", "\"players\"", ",", "\"P1-outcome\"", ",", "\"P1-race\"", ",", "\"P1-apm\"", ",", "\"P2-race\"", ",", "\"P2-apm\"", ",", ")", ")", ")", "try", ":", "bad_replays", "=", "[", "]", "for", "file_path", "in", "run_config", ".", "replay_paths", "(", "replay_dir", ")", ":", "file_name", "=", "os", ".", "path", ".", "basename", "(", "file_path", ")", "try", ":", "info", "=", "controller", ".", "replay_info", "(", "run_config", ".", "replay_data", "(", "file_path", ")", ")", "except", "remote_controller", ".", "RequestError", "as", "e", ":", "bad_replays", ".", "append", "(", "\"%s: %s\"", "%", "(", "file_name", ",", "e", ")", ")", "continue", "if", "info", ".", "HasField", "(", "\"error\"", ")", ":", "print", "(", "\"failed:\"", ",", "file_name", ",", "info", ".", "error", ",", "info", ".", "error_details", ")", "bad_replays", ".", "append", "(", "file_name", ")", "else", ":", "out", "=", "[", "file_name", ",", "info", ".", "base_build", ",", "info", ".", "map_name", ",", "info", ".", "game_duration_loops", ",", "len", "(", "info", ".", "player_info", ")", ",", "sc_pb", ".", "Result", ".", "Name", "(", "info", ".", "player_info", "[", "0", "]", ".", "player_result", ".", "result", ")", ",", "sc_common", ".", "Race", ".", "Name", "(", "info", ".", "player_info", "[", "0", "]", ".", "player_info", ".", "race_actual", ")", ",", "info", ".", "player_info", "[", "0", "]", ".", "player_apm", ",", "]", "if", "len", "(", "info", ".", "player_info", ")", ">=", "2", ":", "out", "+=", "[", "sc_common", ".", "Race", ".", "Name", "(", "info", ".", "player_info", "[", "1", "]", ".", "player_info", ".", "race_actual", ")", ",", "info", ".", "player_info", "[", "1", "]", ".", "player_apm", ",", "]", "print", "(", "u\",\"", ".", "join", "(", "str", "(", "s", ")", "for", "s", "in", "out", ")", ")", "except", "KeyboardInterrupt", ":", "pass", "finally", ":", "if", "bad_replays", ":", "print", "(", "\"\\n\"", ")", "print", "(", "\"Replays with errors:\"", ")", "print", "(", "\"\\n\"", ".", "join", "(", "bad_replays", ")", ")"], "original_string": "def _replay_index(replay_dir):\n  \"\"\"Output information for a directory of replays.\"\"\"\n  run_config = run_configs.get()\n  replay_dir = run_config.abs_replay_path(replay_dir)\n  print(\"Checking: \", replay_dir)\n\n  with run_config.start(want_rgb=False) as controller:\n    print(\"-\" * 60)\n    print(\",\".join((\n        \"filename\",\n        \"build\",\n        \"map_name\",\n        \"game_duration_loops\",\n        \"players\",\n        \"P1-outcome\",\n        \"P1-race\",\n        \"P1-apm\",\n        \"P2-race\",\n        \"P2-apm\",\n    )))\n\n    try:\n      bad_replays = []\n      for file_path in run_config.replay_paths(replay_dir):\n        file_name = os.path.basename(file_path)\n        try:\n          info = controller.replay_info(run_config.replay_data(file_path))\n        except remote_controller.RequestError as e:\n          bad_replays.append(\"%s: %s\" % (file_name, e))\n          continue\n        if info.HasField(\"error\"):\n          print(\"failed:\", file_name, info.error, info.error_details)\n          bad_replays.append(file_name)\n        else:\n          out = [\n              file_name,\n              info.base_build,\n              info.map_name,\n              info.game_duration_loops,\n              len(info.player_info),\n              sc_pb.Result.Name(info.player_info[0].player_result.result),\n              sc_common.Race.Name(info.player_info[0].player_info.race_actual),\n              info.player_info[0].player_apm,\n          ]\n          if len(info.player_info) >= 2:\n            out += [\n                sc_common.Race.Name(\n                    info.player_info[1].player_info.race_actual),\n                info.player_info[1].player_apm,\n            ]\n          print(u\",\".join(str(s) for s in out))\n    except KeyboardInterrupt:\n      pass\n    finally:\n      if bad_replays:\n        print(\"\\n\")\n        print(\"Replays with errors:\")\n        print(\"\\n\".join(bad_replays))"}, {"code": "def version():\n    '''\n    Return the system version for this minion\n\n    .. versionchanged:: 2016.11.4\n        Added support for AIX\n\n    .. versionchanged:: 2018.3.0\n        Added support for OpenBSD\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' status.version\n    '''\n    def linux_version():\n        '''\n        linux specific implementation of version\n        '''\n        try:\n            with salt.utils.files.fopen('/proc/version', 'r') as fp_:\n                return salt.utils.stringutils.to_unicode(fp_.read()).strip()\n        except IOError:\n            return {}\n\n    def bsd_version():\n        '''\n        bsd specific implementation of version\n        '''\n        return __salt__['cmd.run']('sysctl -n kern.version')\n\n    # dict that returns a function that does the right thing per platform\n    get_version = {\n        'Linux': linux_version,\n        'FreeBSD': bsd_version,\n        'OpenBSD': bsd_version,\n        'AIX': lambda: __salt__['cmd.run']('oslevel -s'),\n    }\n\n    errmsg = 'This method is unsupported on the current operating system!'\n    return get_version.get(__grains__['kernel'], lambda: errmsg)()", "code_tokens": ["def", "version", "(", ")", ":", "def", "linux_version", "(", ")", ":", "'''\n        linux specific implementation of version\n        '''", "try", ":", "with", "salt", ".", "utils", ".", "files", ".", "fopen", "(", "'/proc/version'", ",", "'r'", ")", "as", "fp_", ":", "return", "salt", ".", "utils", ".", "stringutils", ".", "to_unicode", "(", "fp_", ".", "read", "(", ")", ")", ".", "strip", "(", ")", "except", "IOError", ":", "return", "{", "}", "def", "bsd_version", "(", ")", ":", "'''\n        bsd specific implementation of version\n        '''", "return", "__salt__", "[", "'cmd.run'", "]", "(", "'sysctl -n kern.version'", ")", "# dict that returns a function that does the right thing per platform", "get_version", "=", "{", "'Linux'", ":", "linux_version", ",", "'FreeBSD'", ":", "bsd_version", ",", "'OpenBSD'", ":", "bsd_version", ",", "'AIX'", ":", "lambda", ":", "__salt__", "[", "'cmd.run'", "]", "(", "'oslevel -s'", ")", ",", "}", "errmsg", "=", "'This method is unsupported on the current operating system!'", "return", "get_version", ".", "get", "(", "__grains__", "[", "'kernel'", "]", ",", "lambda", ":", "errmsg", ")", "(", ")"], "original_string": "def version():\n    '''\n    Return the system version for this minion\n\n    .. versionchanged:: 2016.11.4\n        Added support for AIX\n\n    .. versionchanged:: 2018.3.0\n        Added support for OpenBSD\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' status.version\n    '''\n    def linux_version():\n        '''\n        linux specific implementation of version\n        '''\n        try:\n            with salt.utils.files.fopen('/proc/version', 'r') as fp_:\n                return salt.utils.stringutils.to_unicode(fp_.read()).strip()\n        except IOError:\n            return {}\n\n    def bsd_version():\n        '''\n        bsd specific implementation of version\n        '''\n        return __salt__['cmd.run']('sysctl -n kern.version')\n\n    # dict that returns a function that does the right thing per platform\n    get_version = {\n        'Linux': linux_version,\n        'FreeBSD': bsd_version,\n        'OpenBSD': bsd_version,\n        'AIX': lambda: __salt__['cmd.run']('oslevel -s'),\n    }\n\n    errmsg = 'This method is unsupported on the current operating system!'\n    return get_version.get(__grains__['kernel'], lambda: errmsg)()"}, {"code": "def obj_python_attrs(msg_):\n    \"\"\"iterate object attributes for stringify purposes\n    \"\"\"\n\n    # a special case for namedtuple which seems widely used in\n    # ofp parser implementations.\n    if hasattr(msg_, '_fields'):\n        for k in msg_._fields:\n            yield(k, getattr(msg_, k))\n        return\n    base = getattr(msg_, '_base_attributes', [])\n    opt = getattr(msg_, '_opt_attributes', [])\n    for k, v in inspect.getmembers(msg_):\n        if k in opt:\n            pass\n        elif k.startswith('_'):\n            continue\n        elif callable(v):\n            continue\n        elif k in base:\n            continue\n        elif hasattr(msg_.__class__, k):\n            continue\n        yield (k, v)", "code_tokens": ["def", "obj_python_attrs", "(", "msg_", ")", ":", "# a special case for namedtuple which seems widely used in", "# ofp parser implementations.", "if", "hasattr", "(", "msg_", ",", "'_fields'", ")", ":", "for", "k", "in", "msg_", ".", "_fields", ":", "yield", "(", "k", ",", "getattr", "(", "msg_", ",", "k", ")", ")", "return", "base", "=", "getattr", "(", "msg_", ",", "'_base_attributes'", ",", "[", "]", ")", "opt", "=", "getattr", "(", "msg_", ",", "'_opt_attributes'", ",", "[", "]", ")", "for", "k", ",", "v", "in", "inspect", ".", "getmembers", "(", "msg_", ")", ":", "if", "k", "in", "opt", ":", "pass", "elif", "k", ".", "startswith", "(", "'_'", ")", ":", "continue", "elif", "callable", "(", "v", ")", ":", "continue", "elif", "k", "in", "base", ":", "continue", "elif", "hasattr", "(", "msg_", ".", "__class__", ",", "k", ")", ":", "continue", "yield", "(", "k", ",", "v", ")"], "original_string": "def obj_python_attrs(msg_):\n    \"\"\"iterate object attributes for stringify purposes\n    \"\"\"\n\n    # a special case for namedtuple which seems widely used in\n    # ofp parser implementations.\n    if hasattr(msg_, '_fields'):\n        for k in msg_._fields:\n            yield(k, getattr(msg_, k))\n        return\n    base = getattr(msg_, '_base_attributes', [])\n    opt = getattr(msg_, '_opt_attributes', [])\n    for k, v in inspect.getmembers(msg_):\n        if k in opt:\n            pass\n        elif k.startswith('_'):\n            continue\n        elif callable(v):\n            continue\n        elif k in base:\n            continue\n        elif hasattr(msg_.__class__, k):\n            continue\n        yield (k, v)"}]